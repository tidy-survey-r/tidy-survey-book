# Recommendations for Successful Survey Data Analysis {#c12-pitfalls}

The previous chapters in this book aimed to provide the technical skills and knowledge required to run survey analyses. This chapter builds upon the best practices previously mentioned to present a curated set of recommendations aimed at running a *successful* survey analysis. We hope this list equips you with practical insights that assist in producing meaningful and reliable results.

```{r}
#| include: false
library(dplyr)
library(tidyr)
library(ggplot2)

anscombe_tidy <- anscombe %>%
  mutate(observation = row_number()) %>%
  pivot_longer(-observation, names_to = "key", values_to = "value") %>%
  separate(key, c("variable", "set"), 1, convert = TRUE) %>%
  mutate(set = c("I", "II", "III", "IV")[set]) %>%
  pivot_wider(names_from = variable, values_from = value)

example_srvy <- tibble::tribble(
  ~id, ~region, ~q_d1, ~q_d2_1, ~weight,
   1L,   1L,    1L,      0L,    1740,
   2L,   1L,    1L,      0L,    1428,
   3L,   2L,    1L,      2L,     496,
   4L,   2L,    1L,      2L,     550,
   5L,   3L,    1L,      1L,    1762,
   6L,   4L,    1L,      0L,    1004,
   7L,   4L,    1L,      0L,     522,
   8L,   3L,    2L,      0L,    1099,
   9L,   4L,    2L,      2L,    1295
  )
```

## Grasp the survey design

Understanding complex design factors such as clustering, stratification, and weighting is the foundation of complex survey analysis. Each of these techniques impact standard errors and variance, and we cannot treat complex surveys as unweighted simple random samples if we want to produce accurate estimates. 

Let's 

## Begin your analysis with descriptive data analysis

When receiving a fresh batch of data, it's tempting to jump right into running models to find significant results. However, a successful data analyst begins by exploring the dataset. This involves running descriptive analysis on the dataset as a whole, as well as individual variables and combinations of variables. As described in Chapter \@ref(c05-descriptive-analysis), descriptive analyses should always precede statistical analysis to avoid avoidable (and potentially embarrassing) mistakes.

Even before applying weights, consider running cross-tabulations on the raw data. Do any results jump out?

Let’s go back to our earlier example. We run `svy_des %>% group_by(group) %>% summarize(p = mean())` and see the data shows that males make up 10% of the sample.

```r
## # A tibble: 2 × 2
##   group    	p
##   <fct> 	<dbl>
## 1 female  	0.9
## 2 male    	0.1
```

We would generally assume around a 50/50 split between male and female respondents in a population. The large female proportion could indicate either a unique sample or a potential error in the data. If we review the survey documentation and see this was an intentional part of the design, we can continue our analysis using the appropriate methods. If this was not an intentional choice by the researchers, the results alert us that something may be incorrect in the data or our code, and we can verify if there's an issue by comparing the results with the weighted means.

Tables provide a quick check of our assumptions, but there is no substitute for graphs and plots to visualize the distribution of data. We might miss outliers or nuances if we scan only summary statistics. Anscombe's Quartet demonstrates the importance of visualization in analysis. Let's say we have a dataset with x- and y- variables. Let's take a look at how the dataset is structured:

```{r}
head(anscombe_tidy)
```

We can begin by checking one set of variables. For Set I, the x-variables have an average of 9 with a standard deviation of 3.3; for y, we have an average of 7.5 with a standard deviation of 2.03. The two variables have a correlation of 0.81.

```{r}
anscombe_tidy %>% 
  filter(set == "I") %>% 
  summarize(
    x_mean = mean(x),
    x_sd = sd(x),
    y_mean = mean(y),
    y_sd = sd(y),
    correlation = cor(x, y)
  )
```

These are useful statistics. We can note that the data doesn't have high variability and the two variables are strongly correlated.

Now, let's check all of our variables. Notice anything interesting?

```{r}
anscombe_tidy %>% 
  group_by(set) %>%
  summarize(
    x_mean = mean(x),
    x_sd = sd(x, na.rm = TRUE),
    y_mean = mean(y),
    y_sd = sd(y, na.rm = TRUE),
    correlation = cor(x, y)
  )
```

The summary results for these four variables are nearly identical! We might assume that the distribution for each of them is similar. A data visualization can help confirm our assumptions.

```{r}
ggplot(anscombe_tidy, aes(x, y)) +
  geom_point() +
  facet_wrap( ~ set) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```

When reviewing the plots, it becomes apparent that the distributions are very dissimilar. Each set of points results in different shapes and distributions. Imagine sharing each individual plot with a shareholder and how you would describe the data, and how different the interpretations will be.

With survey data, we may not always have continuous data that we can plot like Anscombe's Quartet. However, if the dataset does contain continuous data, or other types of data which would benefit from a visual representation, we recommend taking the time to graph distributions and correlations.

## Use the appropriate variable types

When we pull the data from surveys into R, the data may be listed as character, factor, numeric, or logical/Boolean. For example, here we take a `glimpse()` of ANES data:

```{r}
data(anes_2020)

anes_2020 %>%
  select(-matches("^V\\d")) %>%
  glimpse()
```

While the output shows that `CampaignInterest` is a factor, R will not clearly indicate whether it is an ordinal variable. When working with survey data, analysts need to properly use the questionnaire and codebook along with the data (see Chapter \@ref(c03-understanding-survey-data-documentation)) to understand what the values for each variable represent.

Here is another example. We have a dataset `example_srvy` that contains information about the respondents' region in the column `region`. Taking a `glimpse()` of the data:

```{r}
example_srvy %>%
  glimpse()
```

The categorical variables (e.g., the North, South, East, and West regions of the United States) are represented using numbers (e.g., 1, 2, 3, and 4). When importing the file, R will automatically read the column as numeric values. Without carefully reviewing the data frame, we may calculate the mean across all numeric variables:

```{r}
example_srvy %>%
  summarize(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)))
```

R will calculate the mean even if it is not appropriate, leading to the common mistake of applying an average to categorical values instead of a proportion function. If the variable name is difficult to interpret, we might accidentally report an average region of 2.67 to our stakeholders. Checking that your variables are of the appropriate type will avoid this pitfall and ensure the measures and models are appropriate for the type of variable.

## Improve your debugging skills

Whether `NA` is a level or a value impacts whether `is.na()` works
If the variable is a factor and has an `NA` as a level, and you want to remove those `NA`s, you might try using dplyr's `filter()`:

```{r}
svy_dat %>% 
  filter(!is.na(variable))
```

## Draw significance conclusions appropriately

When we say something is "statistically significant", we mean that our result can be attributed to an effect, a relationship between variables, or a difference between groups, rather than purely to chance. As mentioned in Chapter \@ref(c02-overview-survey), determining the study design is a lengthy and intensive process. Careful consideration is taken to reduce the sampling error, in hopes that our results are not solely due to how the sample was chosen.

For instance, 

```{r}
anova_out <- recs_des %>%
  svyglm(design = ., formula = SummerTempNight ~ Region, na.action = na.omit)

tidy(anova_out)
```





