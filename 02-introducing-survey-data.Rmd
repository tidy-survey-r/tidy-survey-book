# Overview of Surveys {#c02}

Creating and fielding surveys to provide estimates of a population are much more complex than simply adding a few questions to an online platform. Survey researchers can spend months or even years developing the study design, questions, and other methods for a single survey to ensure high quality data is collected. While this book focuses on the analysis methods of surveys, understanding the entire survey process can provide a better understanding of what types of analyses should be conducted on the data. There are many pieces of existing literature that cover designing and implementing surveys that we recommend reviewing in detail (e.g., {cite: Dillman book, Groves book, education book?, others?}). The information provided in this chapter is a brief introduction into the survey process aiming to provide a foundation of the complexities of survey data.

The survey life cycle starts with a research topic or question of interest. For example, a researcher may be interested in the impact childhood trauma has on political affiliation later in life. Before choosing to conduct a survey, researchers typically review existing data sources to determine if data are already available that can answer this question. If existing data cannot answer the nuances of the research question, a survey can be used to capture the exact data that the researcher needs.

<!--For some studies, this may be a cycle where after the first wave of the study, the next one begins shortly after and the process begins again.  Other studies may require translation as part of questionnaire design and data collection materials preparation, or include pretesting such as cognitive and usability testing prior to data collection.  Each study is slightly different how it is developed, but this process provides a general flow for how surveys are developed.-->

<!--Need something else here as a transition-->

## Error Sources

Some types of error can be addressed by adjusting the data using weights, and others need to be addressed during the survey development process. Generally, survey researchers consider there to be 7 main types of error: 

* Measurement
  * **Validity**: A mismatch between the topic of interest and the question used to collect that information
  * **Measurement Error**: A mismatch between what the researcher asked and how the respondent answered
  * **Processing Error**: Edits by the researcher to responses provided by the respondent (e.g., adjusts data based on illogical responses)
* Representation
  * **Coverage Error**: A mismatch between the target population and the sample frame available
  * **Sampling Error**: Error produced when selecting a sample from the sample frame (none if conducting a census)
  * **Nonresponse Error**: Differences between those who responded and did not respond to the survey
  * **Adjustment Error**: Error introduced during post-survey statistical adjustments





## Survey Preparation

Having a robust study design is crucial for success of the survey both implementation and analysis. Knowing who and how to survey individuals depends on both the goals of the study and the feasibility of implementation. Who we want to survey is known as the *population* of interest in the study. This could be broad, such as, "all adults age 18+ living in the U.S.", or it could be a very specific population of interest based on a specific characteristic or location. For example, we may want to know about "adults age 18-24 who live in North Carolina" or "eligible voters living in Illinois".

While it would be great to survey every person that meet these requirements (i.e., conduct a census), the ability to implement a questionnaire at that scale is something few can do. Instead, researchers choose to *sample* individuals and use weights to estimate numbers in the population. There are a variety of different sampling methods that can be used, and more information on these can be found in Chapter \@ref(c05).

Once a population of interest is identified, researchers need to consider how to survey these individuals. There are four main modes that researchers can consider when conducting a survey:

-   Computer Assisted Personal Interview (CAPI; also known as face-to-face or in-person interviewing)
-   Computer Assisted Telephone Interview (CATI; also known as phone or telephone interviewing)
-   Web
-   Paper and Pencil

Researchers can use a single mode to collect data, or multiple modes (also called mixed modes). Using mixed modes can allow for broader reach and can increase response rates depending on the population of interest ({CITE DeLeeuw mixed mode article}). However, mode effects (where responses differ based on the mode of response) can be present in the data and may need to be considered during analysis.

When selecting which mode, or modes, to use, understanding the unique aspects of the chosen population will provide insight into how they can best be reached and engaged. For example, if we plan to survey adults age 18-24 who live in North Carolina, asking them to complete a survey using CATI (i.e., over the phone) would most likely not be as successful as other modes like web.

### Sampling Design

### Questionnaire Design

After determining the study design to use for the survey, researchers will begin developing the questions of the survey. As with writing papers, an outline of the topics to be asked is a good place to begin. When starting out, adding in the "why" each question or topic is important to the research question(s) can help researchers better tailor their questionnaire and potentially reduce the number of questions (and thus burden on the respondent) if topics are deemed irrelevant to the research question. When making these decisions, researchers should also consider questions needed for weighting purposes. While we would love to have everyone sampled answer our survey, this is rarely the case. Thus, including questions about demographics in the survey can assist with weighting for nonresponse.

Additionally, when crafting questions for surveys, researchers should consider the mode the survey will be administered in and adjust language appropriately. In self-administered surveys (e.g., web or mail), respondents can see all of the questions and response options, so formatting of the questions and ordering of questions is important. In interviewer-administered surveys (e.g., CATI), respondents only hear the interviewer asking the questions and it becomes more of a conversation between parties. There are multiple resources out there to help researchers draft questions for different modes (e.g., {CITE Dillman, fowler, ed book?}).

How questions are worded have large impacts on what results can be obtained from the data. For example, Let's say we have the following question in our survey:

<!-- Put this in as an image instead of as a block quote -->
> What animal do you prefer to have as a pet?
>
<ul class="ro">
  <li class="ro">Dogs</li>
  <li class="ro">Cats</li>
</ul>

If we had 100 respondents who answered the question and 50 selected dogs, then the results of this question cannot be 

> 50% of the population perfers to have a dog as a pet

as only two response options were provided. If a respondent taking our survey prefers turtles, they could either be forced to choose a response between these two (i.e., interpret the question as "between dogs and cats which do you prefer?"), or they may not answer the question (i.e., nonresponse).  As researchers we cannot determine, how these respondents would have answered.  Instead, the interpretation of this question should be

> When given the choice between dogs and cats, 50% of respodents perferred to have a dog as a pet.

However, when researchers are creating questions they should consider these possibilities and adjust the question accordingly.  One simple way could be to add an "other" response option to give respondents a chance to provide a different response.  The "other" response option could then include a way for respondents to write in what their other preference is.  For example, this question could be rewritten as

<!-- Put this in as an image instead of as a block quote, or need to add in an open-ended box for write in after the last option. -->
> What animal do you prefer to have as a pet?
>
<ul class="ro">
  <li class="ro">Dogs</li>
  <li class="ro">Cats</li>
  <li class="ro">Other, please specify:</li>
</ul>

Researchers can then code the responses from the open-ended box and get a better understanding of the respondent's choice of preferred pet.  Interpreting this question becomes easier as researchers no longer need to qualify the results with the choices provided.

This is a very simple example of how the question presentation and options can impact the findings.  More complex topics and questions will need researchers to thoroughly consider how to mitigate any impacts from the presentation, formatting, wording, and other aspects.  As survey analysts, reviewing not only the data but also the wording of the questions is crucial to ensure the results are presented in a manner consistent with the question asked.

<!--Consider adding a note about qbanks-->

### Data Collection Planning

Much of data collection relies on what is available on the sample frame (see \@ref(c05)).  For example, if our sampling frame includes email address, we could send email to our selected sample members to convince them to complete a web survey. While in this example the contact mode and the survey mode are similar (email and web), they do not have to be.  For example, we could mail respondents a letter with a link to access a web survey. However, it is important to make access to the survey as easy as possible for sample members to reduce burden and increase response rates.

Understanding how sample members were contacted is important for determining the correct population that the data references.  One way researchers can contact respondents is using an address-based sampling (ABS) frame.  This frame is a list of addresses, so by nature, the survey is a household survey and will not incorporate respondents from those who do not have an address (e.g., homeless).  Additionally, using an address based approach will most often result in a population of _households_.  Surveys can combat this by asking the households to select an individual from the household to complete the survey.  This is called within household selection, however, there is no guarantee that the person completing the survey is the one that _should_ have completed the survey giving the within household selection criteria.

This is just one example of how the data collection mode can impact the interpretation of survey results.  Other modes also have their own considerations, and survey analysts should read the survey documentation carefully (see \@ref(c03)) to have a better understanding of who was sampled and what the target population should be.

## Data Collection??

## Post Survey Processing

### Data Cleaning and Imputation

### Weighting
Weighting can typically be used to address some of these error sources and many published surveys will include an "analysis weight" variable that should be used in analysis to account for this. The construction of weights is outside the scope of this book and researchers should reference other materials if interested constructing their own (e.g., {CITE Jill's Book}). Instead, this book assumes the survey has been completed, weights are constructed, and data is made available for users. We will walk users through how to read documentation and work with the data and analysis weights provided to analyze and interpret survey results correctly. 

### Disclosure

## This book
After all of these steps are taken, the data is ready for use by analysts.  The rest of this book works from this point. If you're interested in learning more about the steps talked about in this chapter, we recommend you look into the references cited in this chapter.s

