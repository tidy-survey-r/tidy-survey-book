# Overview of Surveys {#c02}

Creating and fielding surveys to provide estimates of a population are much more complex than simply adding a few questions to an online platform. Survey researchers can spend months, or even years, developing the study design, questions, and other methods for a single survey to ensure high quality data is collected. While this book focuses on the analysis methods of surveys, understanding the entire survey process can provide a better insight into what types of analyses should be conducted on the data. To gain a deeper understanding of survey design and implementation, there are many pieces of existing literature that we recommend reviewing in detail (e.g., {cite: Dillman book, Groves book, education book?, Tourangeau book, Asking Questions book?, others?}).

The survey life cycle starts with a research topic or question of interest (e.g., what impact does childhood trauma have on health outcomes later in life). Researchers typically review existing data sources to determine if data are already available that can answer this question, as this can result in reduced burden on respondents, cheaper research costs, and faster research outcomes. However, if existing data cannot answer the nuances of the research question, a survey can be used to capture the exact data that the researcher needs.

When starting a survey, there are multiple things to consider including the study population, how the sample is selected, and the way the question is worded. However, each step and decision can impact how the results can be interpreted, specifically related to the types of error that can be introduced into the data. This chapter provides a brief introduction to the types of error to consider when conducting surveys and how these error sources can inform the interpretations of the data.

## Error Sources {#overview-error}

Generally, survey researchers consider there to be 7 main sources of error ({Cite Groves book}):

-  Representation
  -  **Coverage Error**: A mismatch between the target population and the sample frame available
  -  **Sampling Error**: Error produced when selecting a sample from the sample frame (none if conducting a census)
  -  **Nonresponse Error**: Differences between those who responded and did not respond to the survey (unit nonresponse) or to a given question (item nonresponse)
  -  **Adjustment Error**: Error introduced during post-survey statistical adjustments
-  Measurement
  -  **Validity**: A mismatch between the topic of interest and the question used to collect that information
  -  **Measurement Error**: A mismatch between what the researcher asked and how the respondent answered
  -  **Processing Error**: Edits by the researcher to responses provided by the respondent (e.g., adjusts data based on illogical responses)

Almost every survey conducted will have some of each of these types of error, and researchers attempt to conduct a survey that reduces this *total survey error*. One of the most common ways to reduce total survey error is through weighting. However, weighting may result in increased error on its own (*adjustment error*). Therefore when designing surveys, researchers should carefully think about ways to reduce each of these error sources. Additionally when analyzing survey data, understanding decisions that researchers took to minimize these error sources can impact how results are interpreted. The remainder of this chapter dives into considerations for survey development and places where each of these sources of error can be considered.

## Study Design {#overview-design}

Study design is often used to encompass multiple parts of the survey planning process including decisions on population, survey mode, and timeline. Multiple decisions made prior to the launch of the survey can assist researchers in reducing different error sources, and are fundamental in understanding how to interpret the results of the data. Knowing who and how to survey individuals depends on both the goals of the study and the feasibility of implementation.

### Sampling Design {#overview-design-sampdesign}

Who we want to survey is known as the *population* of interest in the study. This could be broad, such as, "all adults age 18+ living in the U.S.", or it could be a very specific population of interest based on a specific characteristic or location. For example, we may want to know about "adults age 18-24 who live in North Carolina" or "eligible voters living in Illinois". However, to survey individuals in these populations, a *sampling frame* is needed with contact information. If researchers are looking at eligible voters, the sampling frame would be the voting registry for a given state or area. For more broad populations like all adults living in the U.S., the sampling frame will most likely be imperfect. In these cases, researchers may choose to use a sample frame of mailing addresses and send the survey to households, or they may choose to use random digit dialing (RDD) and call random phone numbers. These imperfect sampling frames can result in *coverage error* where there is a mismatch between the population of interest and the list of individuals researchers can select. For example, if a researcher is looking to obtain estimates for "all adults age 18+ living in the U.S.", using a sample frame from mailing addresses will be missing specific types of individuals such as the homeless or transient populations. Additionally, many households have more than one adult living there, so researchers would need to consider how to get a specific individual to fill out the survey (called within household selection), or adjust the population to report on "U.S. households" instead of "individuals".

Once the sample frame is selected, the next step is figuring out who to survey. In rare cases researchers may which to conduct a census and survey everyone on the sampling frame. However, the ability to implement a questionnaire at that scale is something few can do. Instead, researchers choose to *sample* individuals and use weights to estimate numbers in the population. There are a variety of different sampling methods that can be used, and more information on these can be found in Chapter \@ref(c05). This decision of which sampling method to use, impacts *sampling_error* and can be adjusted for in weighting.

#### Example: Number of Pets in a Household {.unnumbered #overview-design-sampdesign-ex}

Let's use a simple example where a researcher is interested in knowing the average number of pets in a household. Our researcher will need to consider what the population of interest is for this. Specifically, are they interested in all of the U.S., a different country, or a more local area (e.g., city or state). Let's assume our researcher is interested in the number of pets in a U.S. household where there is at least one adult living (18 years old or older). In this case, using a sampling frame of mailing addresses would provide the least amount of coverage error. The United States Postal Service (USPS) creates a file of mailing addresses in the U.S. that covers about 95% of U.S. households ({cite Iannichone<!--Check the 95% number when adding citation, not sure if coverage has increased since then-->}). This file is called the Delivery Sequence File (DSF) and is used by many researchers and government agencies to reach the U.S. population. To sample these individuals, for simplicity, we will use a stratified simple random sample design, where we stratify by state. 

Throughout this chapter we will build on this example research question to plan a survey. 

### Data Collection Planning {#overview-design-dcplanning}

With the sampling design decided, researchers can then make decisions on how to survey these individuals. Specifically, the *mode* used for contacting and surveying the sample, how frequently to send reminders and follow-ups, and the overall timeline of the study are three of the major data collection determinations. Traditionally, researchers have considered four main modes^[Other modes such as using mobile apps or text messaging can also be considered, but have typically smaller reach or are better for longitudinal studies (i.e., surveying the same individuals over many waves of a single study). For the purposes of this overview we will focus on these four main modes.] for conducting surveys:

-  Computer Assisted Personal Interview (CAPI; also known as face-to-face or in-person interviewing)
-  Computer Assisted Telephone Interview (CATI; also known as phone or telephone interviewing)
-  Web
-  Paper and Pencil

Researchers can use a single mode to collect data, or multiple modes (also called mixed modes). Using mixed modes can allow for broader reach and can increase response rates depending on the population of interest ({CITE DeLeeuw mixed mode article}). However, mode effects (where responses differ based on the mode of response) can be present in the data and may need to be considered during analysis.

When selecting which mode, or modes, to use, understanding the unique aspects of the chosen population and sampling frame will provide insight into how they can best be reached and engaged. For example, if we plan to survey adults age 18-24 who live in North Carolina, asking them to complete a survey using CATI (i.e., over the phone) would most likely not be as successful as other modes like web. Additionally, the mode for contacting respondents relies on what information is available on the sample frame. For example, if our sampling frame includes email address, we could send email to our selected sample members to convince them to complete a survey. Or if the sampling frame is a list of mailing addresses, researchers would have to contact sample members with a letter. 

It is important to note that there can be a difference between the contact mode and the survey mode. For example, if we have a sample frame with addresses, we can send a letter to our sample members and provide information on how to complete a web survey. Or we could use mixed-mode surveys and send sample members a paper and pencil survey with our letter and also ask them to complete the survey online. Combining different contact modes and different survey modes can be useful in reducing *unit nonresponse error*, as different sample members may respond better to different contact and survey modes. However, when considering which modes to use, it is important to make access to the survey as easy as possible for sample members to reduce burden and unit nonresponse.

Another way to reduce unit nonresponse error is though varying the language of the contact materials ({cite Dillman Book}). People are motivated by different things, so constantly repeating the same messaging may not be helpful. Instead, mixing up the messaging and the type of contact material the sample member receives can increase response rates and reduce unit nonresponse error. For example, instead of only sending standard letters, researchers could consider sending mailings that invoke "urgent" or "important" thoughts by sending priority letters, or using other delivery services like FedEx, UPS, or DHL.

Determining the number and types of contacts may also be determined by a study timeline. If the timeline is long, then there is a lot of time for follow-ups and varying the message in contact materials. If the timeline is short, than fewer follow-ups can be implemented. Many studies will start with the tailored design method put forth by {Dillman et al. (2014)} and implement 5 contacts: (1) prenotice letting sample members know the survey is coming, (2) invitation to complete the survey, (3) reminder postcard that also thanks respondents that may have already completed the survey, (4) reminder letter (with a replacement paper survey if needed), and (5) final reminder postcard. This method is easily adaptable based on the study timeline and needs, but provides a good basis for most studies.


#### Example: Number of Pets in a Household {- #overview-design-dcplanning-ex}

Let's return to our example of a researcher who wants to know the average number of pets in a household. We are using a sample frame of mailing addresses, so we recommend starting our data collection with letters mailed to households, but later in data collection we want to send interviewers to the house to conduct an in-person (or CAPI) interview in an effort to decrease unit nonresponse error. This means we will have two contact modes (paper and in-person). As we mentioned above, the survey mode does not have to be the same as the contact mode, so we recommend a mixed-mode study with both Web and CAPI modes. Let's assume we have 6 months for data collection, so we may want to recommend the following protocol:

Table: Protocol Example for 6-month Web and CAPI Data Collection

Week  | Contact Mode          | Contact Message         | Survey Mode Offered
:----: | ------------------------------ | ------------------------------- | -------------------
   1 | Mail: Letter          | Prenotice            | ---
   2 | Mail: Letter          | Invitation           | Web
   3 | Mail: Postcard         | Thank You/Reminder       | Web
   6 | Mail: Letter in large envelope | Animal Welfare Discussion    | Web
  10 | Mail: Postcard         | Inform Upcoming In-Person Visit | Web
  14 | In-Person Visit        | ---               | CAPI
  16 | Mail: Letter          | Reminder of In-Person Visit   | Web, but includes number to call to schedule CAPI
  20 | In-Person Visit        | ---               | CAPI
  25 | Mail: Letter in large envelope | Survey Closing Notice      | Web, but includes number to call to schedule CAPI

This is just one possible protocol that could be used that starts respondents with web (typically done to reduce costs). However, researchers may want to start in-person data collection earlier during the data collection period, or ask their interviewers to attempt more than 2 visits with a household.

### Questionnaire Design {#overview-design-questionnaire}

When developing the questionnaire, it can be helpful to first outline the topics to be asked and include the "why" each question or topic is important to the research question(s). This can help researchers better tailor the questionnaire and potentially reduce the number of questions (and thus burden on the respondent) if topics are deemed irrelevant to the research question. When making these decisions, researchers should also consider questions needed for weighting purposes. While we would love to have everyone sampled answer our survey, this is rarely the case. Thus, including questions about demographics in the survey can assist with weighting for *nonresponse error* (both unit and item nonresponse). Knowing details of the sampling plan and what may impact *coverage error* and *sampling error* can help guide researchers in determining what types of demographics to include.

Researchers can benefit from the work of others by using questions from other surveys. This is common with demographic questions such as race and ethnicity that use questions from a government census or other official surveys. Other survey questions can be found using question banks which are a compilation of questions that have been asked across a variety of surveys such as the [Inter-university Consortium for Political and Social Research (ICPSR) variable search] (https://www.icpsr.umich.edu/web/pages/ICPSR/ssvd/).

If a question does not already exist in a question bank, then researchers can craft their own. When creating their own questions, researchers should start with the research question or topic and attempt to write questions that match the concept. The closer the question asked is to the overall concept the better *validity* there is. For example, if the researcher wants to know how people consume tv series and movies, but only ask a question about how many TVs are in the house, then they would be missing other ways that people watch tv series and movies such as on other devices or at places outside of the home.

Additionally, when designing questions, researchers should consider the mode the survey will be administered in and adjust language appropriately. In self-administered surveys (e.g., web or mail), respondents can see all of the questions and response options, but that is not the case in interviewer-administered surveys (e.g., CATI or CAPI). With interviewer-administered surveys, the response options need to be read out loud to the respondents, so the question may need to be adjusted to allow a better flow to the interview. Additionally, with self-administered surveys, because the respondents are viewing the questionnaire, the formatting of the questions is even more important to ensure accurate measurement. Incorrectly formatting or wording questions can result in *measurement error*, so following best practices or using existing validated questions can reduce error. There are multiple resources out there to help researchers draft questions for different modes (e.g., {CITE Dillman, fowler, ed book?, asking questions, tourangeau formatting article?}).


#### Example: Number of Pets in a Household {- #overview-design-questionnaire-ex}

As part of our survey on the average number of pets in a household, researchers may want to know what animal the majority of people prefer to have as a pet. Let's say we have the following question in our survey:

<!-- Put this in as an image instead of as a block quote -->

> What animal do you prefer to have as a pet?
>
> <ul class="ro">
>
> <li class="ro">
>
> Dogs
>
> </li>
>
> <li class="ro">
>
> Cats
>
> </li>
>
> </ul>

This question may have validity issues as it is only providing the options of "dogs" and "cats" to respondents and interpretation of the data could be incorrect. For example, if we had 100 respondents who answered the question and 50 selected dogs, then the results of this question cannot be: `50% of the population perfers to have a dog as a pet` as only two response options were provided. If a respondent taking our survey prefers turtles, they could either be forced to choose a response between these two (i.e., interpret the question as "between dogs and cats which do you prefer?" and result in *measurement error*), or they may not answer the question (which results in *item nonresponse error*). Based on this, the interpretation of this question should be `When given the choice between dogs and cats, 50% of respondents preferred to have a dog as a pet`.

To avoid this issue, researchers should consider these possibilities and adjust the question accordingly. One simple way could be to add an "other" response option to give respondents a chance to provide a different response. The "other" response option could then include a way for respondents to write in what their other preference is. For example, this question could be rewritten as

<!-- Put this in as an image instead of as a block quote, or need to add in an open-ended box for write in after the last option. -->

> What animal do you prefer to have as a pet?
>
> <ul class="ro">
>
> <li class="ro">
>
> Dogs
>
> </li>
>
> <li class="ro">
>
> Cats
>
> </li>
>
> <li class="ro">
>
> Other, please specify:
>
> </li>
>
> </ul>

Researchers can then code the responses from the open-ended box and get a better understanding of the respondent's choice of preferred pet. Interpreting this question becomes easier as researchers no longer need to qualify the results with the choices provided.

This is a very simple example of how the question presentation and options can impact the findings. More complex topics and questions will need researchers to thoroughly consider how to mitigate any impacts from the presentation, formatting, wording, and other aspects. As survey analysts, reviewing not only the data but also the wording of the questions is crucial to ensure the results are presented in a manner consistent with the question asked.


## Data Collection {#overview-datacollection}

Once the data collection starts, researchers try to stick to the data collection protocol designed during pre survey planning. However, a good researcher will adjust their plans and adapt as needed to the current progress of data collection ({cite Peytchev book on adaptive survey design}). Some extreme examples could be natural disasters that could prevent mail or interviewers from getting to the sample members. Others could be smaller in that something news worthy occurs that is connected to the survey, so researchers could choose to play this up in communication materials. In addition to these external factors, there could be factors unique to the survey such as the response rates are lower for a specific sub-group, so the data collection protocol may need to find ways to improve response rates for a specific group.


## Post Survey Processing {#overview-post}

After data collection, there are a variety of activities that need to be conducted before the survey can be analyzed. Multiple decisions made during this post survey phase can assist researchers in reducing different error sources such as through weighting to account for the sample selection. Knowing the decisions researchers made in creating the final analytic data can impact how analysts use the data and interpret the results.


### Data Cleaning and Imputation {#overview-post-cleaning}

Post survey cleaning and imputation is one of the first steps researchers will do to get the survey responses into a dataset for use by analysts. Data cleaning can comprise of cleaning inconsistent data (e.g., with skip pattern errors or multiple questions throughout the survey being consistent with each other), editing numeric entry or open-ended responses for grammar and consistency, or recoding open-ended questions into categories for analysis. Each project should create their own rules for how to handle different cleaning situations, and there are no specific rules that must be followed. Instead, researchers should use their best judgement in ensuring data integrity remains and all decisions should be documented and available to those using the data in analysis. Each decision a researcher makes has an impact on *processing error*, so often researchers often will have multiple people review these rules or recode open-ended data and adjudicate any differences in an attempt to reduce this error.

Another crucial step in post survey processing is imputation. Often times there is item nonresponse where respondents do not answer specific questions. If the questions are crucial to analysis efforts or to the research question, researchers will implement imputation in an effort to reduce *item nonresponse error*. However, imputation itself can introduce *processing error* as well, so researchers should consider the overall implications of imputing data compared to having item nonresponse. There are multiple ways that imputation can be conducted, we recommend reviewing other resources like {cite Dever}<!--Stephanie is this the right book or is there a better one on imputation?--> for more information.

#### Example: Number of Pets in a Household {- #overview-post-cleaning-ex}

Let's return to the question we created to ask about [animal preference](#overview-design-questionnaire-ex). The "other specify" invites respondents to specify the type of animal they prefer to have as a pet. If respondents entered answers such as "puppy", "turtle", "rabit", "rabbit", "bunny", "ant farm", "snake", "Mr. Purr", then researchers may wish to categorize these write-in responses to help with analysis. In this example, "puppy" could be assumed to be a reference to a `Dog`, and could be recoded there. The misspelling of "rabit" could be coded along with "rabbit" and "bunny" into a single category of `Bunny or Rabbit`. These are relatively standard decisions that a researcher could make. The remaining write-in responses could be categorized into a few different ways. "Mr. Purr", which may be someone's reference to their own cat, could be recoded to `Cat`, or it could remain as `Other` or some category that is `Unknown`. Depending on the number of responses related to each of others, they could all be combined into a single `Other` category, or maybe categories such as `Reptiles` or `Insects` could be created. Each of these decisions may impact the interpetation of the data, so our researcher should make sure to document the types of responses that fall into each of the new categories.



### Weighting {#overview-post-weighting}

Weighting can typically be used to address some of the error sources identified in the previous sections.  For example, weights may be used to address coverage, sampling, and nonresponse errors and many published surveys will include an "analysis weight" variable that combines these adjustments. However, weighting itself can also introduce *adjustment error*, so researchers need to balance which types of errors should be corrected with weighting. The construction of weights is outside the scope of this book and researchers should reference other materials if interested constructing their own (e.g., {CITE Jill's Book}). Instead, this book assumes the survey has been completed, weights are constructed, and data is made available for users. We will walk users through how to read documentation and work with the data and analysis weights provided to analyze and interpret survey results correctly.

#### Example: Number of Pets in a Household {- #overview-post-weighting-ex}

In the simple example of our survey, we decided to use a stratified sample by state to select our sample members.  Knowing this sampling design our researcher can include selection weights for analysis that account for how the sample members were selected into the survey.  Additionally, the sampling frame may have the type of building associated with each address, so we could include the building type as a potential nonresponse weighting variable, along with some interviewer observations that may be related to our research topic of average number of pets in a household.  Combining these weights we could create an analytic weight that researchers can use when analyzing the data.


### Disclosure {#overview-post-disclosure}

Before data is allowed to be made publicly available, researchers will need to ensure that individual respondents can not be identified by the data.  There are a variety of different methods that can be used, but one common method is swapping records.  Researchers may swap specific data values across different respondents such that it does not impact insights that come from the data, but ensures that specific individuals cannot be identified.  Other methods may include aggregating various categories of responses or location information to avoid having only a few people in a given group that could be identified.  There is as much art as there is science to the methods used for disclosure, and in documentation researchers should only provide a high level comments that disclosure was conducted and not specific details to ensure nobody can reverse the disclosure and thus identify individuals.  For more information on different disclosure methods please see {CITE??--Stephanie?}.


## This book

After all of these steps are taken, the data is ready for use by analysts. The rest of this book works from this point. If you're interested in learning more about the steps talked about here, we recommend you look into the references cited throughout this chapter.
