\mainmatter

# Introduction {#c01-intro}

Surveys are used to gather information about a population. They are frequently used by researchers, governments, and businesses to better understand public opinion and behavior. For example, a non-profit group might be interested in public opinion on a given topic, government agencies may be interested in behaviors to inform policy, or companies may survey potential consumers about what they want from their products. Developing and fielding a survey is a method to gather information about topics that interest us.

<!--Consider pulling out definitions into call out boxes or a glossary-->
This book focuses on how to **analyze** the data collected from a survey. We assume that you have conducted a survey or obtained a microdata file.  Microdata, also known as respondent-level or row-level data, contains individual survey responses, analysis weights, and design variables (as opposed to opposed to summarized data in tables). For the purposes of this book, you need the weights and design variables for your survey data. These are required to accurately calculate unbiased estimates^[If you do not already have weights created for the survey data you are using, we recommend reviewing other resources focused on weight creation such as @Valliant2018weights]. Understanding the concepts and techniques discussed in this book will help you to extract meaningful insights from your survey data.

To account for the weights and study design, researchers rely on statistical software such as SAS, Stata, SUDAAN, and R. In this book, we will use R to provide an overview to survey analysis. Our goal is to provide a comprehensive guide for individuals new to survey analysis but have some statistics and R programming background. We will use a combination of both the {survey} and {srvyr} packages and present the code following best practices from the tidyverse. 

In 2003, the {survey} package was released on CRAN and has been continuously developed over time^[https://cran.r-project.org/src/contrib/Archive/survey/]. This package, primarily developed by Thomas Lumley, is extensive and includes the following features:

- Estimates of point estimates and their associated variances, including means, totals, ratios, quantiles, and proportions
- Estimation of regression models, including generalized linear models, log-linear models, and survival curves
- Variances by Taylor linearization or by replicate weights (balance repeated replication, jackknife, bootstrap, multistage bootstrap, or user-supplied)
- Hypothesis testing for means, proportions, and more

The {srvyr} package in R builds on the {survey} package. It provides wrappers for functions that align with the tidyverse philosophy, which is our motivation for using and recommending this package. We find that the {srvyr} package is user-friendly for those familiar with tidyverse packages in R. For example, while many functions in the {survey} package use variables as formulas, the {srvyr} package uses tidy selection to pass variable names^[https://dplyr.tidyverse.org/reference/dplyr_tidy_select.html] (a common feature in the tidyverse). Users of the tidyverse are most likely familiar with the magittr pipe (`%>%`), which seamlessly works with functions from the {srvyr} package. Moreover, several common functions from {dplyr}, such as including `filter()`, `mutate()`, and `summarize()`, can be applied to survey objects. Users can streamline their analysis workflow and capitalize on the benefits of both the {srvyr} and tidyverse packages.

There is one limitation to the {srvyr} package: it doesn't fully incorporate the modeling capabilities of the {survey} package into its tidy versions. This book will use the {survey} package when discussing modeling and hypothesis testing. However, we will guide you on how to apply the pipe to these functions to ensure clarity in your analyses.

## What to expect {#what-to-expect}

This book will cover many aspects of survey design and analysis, from understanding how to create design effects to conducting descriptive analysis, statistical tests, and models.  Additionally, we emphasize best practices in coding and presenting results. Throughout this book, we use real-world data and present practical examples to help you gain proficiency in survey analysis. While we provide a brief overview of survey methodology and statistical theory, this book is not intended to be the sole resource for these topics.  We reference other materials throughout the book and encourage readers to seek those out for more information. Below is a summary of each chapter:

- **Chapter \@ref(c02-overview-surveys)**: An overview of surveys and the process of designing surveys. This is only an overview, and we include many references for more in-depth knowledge.
- **Chapter \@ref(c03-specifying-sample-designs)**: Specifying sampling designs. Descriptions of common sampling designs, when they are used, the math behind the mean and standard error estimates, how to specify the designs in R, and examples using real data.
- **Chapter \@ref(c04-understanding-survey-data-documentation)**: Understanding survey documentation. How to read the various components of survey documentation, working with missing data, and finding the documentation.
- **Chapter \@ref(c05-descriptive-analysis)**: Descriptive analyses. Calculating point estimates along with their standard errors, confidence intervals, and design effects.
- **Chapter \@ref(c06-statistical-testing)**: Statistical testing. Testing for differences between groups, including comparisons of means and proportions as well as goodness of fit tests, tests of independence, and tests of homogeneity.
- **Chapter \@ref(c07-modeling)**: Modeling. Linear regression, ANOVA, and logistic regression.
- **Chapter \@ref(c08-communicating-results)**: Communicating results. Describing results, reproducibility, making publishable tables and graphs, and helpful functions.
- **Chapter \@ref(c09-ncvs-vignette)**: National Crime Victimization Survey Vignette. A vignette on how to analyze data from the NCVS, a survey in the U.S. that collects information on crimes and their characteristics. This illustrates an analysis that requires multiple files to calculate victimization rates.
- **Chapter \@ref(c10-ambarom-vignette)**: AmericasBarometer Vignette. A vignette on how to analyze data from the AmericasBarometer, a survey of attitudes, evaluations, experiences, and behavior in countries in the Western Hemisphere. This includes how to make choropleth maps with survey estimates.

In most chapters, you'll find code that you can follow. Each of these chapters starts with a "set-up" section. This section will include the code needed to load the necessary packages and datasets in the chapter.  We then provide the main idea of the chapter and examples on how to use the functions.  Most chapters end with exercises to work through.  Solutions to the exercises can be found in the Appendix.

## Datasets used in this book {#book-datasets}

We work with two key datasets throughout the book: the Residential Energy Consumption Survey [RECS -- @recs-2020-tech] and the American National Election Studies [ANES -- @debell].  To ensure that all readers can follow the examples, we have provided analytic datasets available on OSF^[https://osf.io/gzbkn/?view_only=8ca80573293b4e12b7f934a0f742b957]. 

If a chapter contains data that is not part of existing packages, we have created a helper function, `read_osf()`,  for you to load it easily. We recommend saving the script below in a folder called "helper-fun" and calling the file `helper-function.R` if you would like to follow along with the prerequisites listed in the chapters that contain code. 

```r
read_osf <- function(filename){
  #' Downloads file from OSF project
  #' Reads in file
  #' Deletes file from computer
  
  osf_dl_del_later <- !dir.exists("osf_dl")
  
  if (osf_dl_del_later) {
    osf_dl_del_later <- TRUE
    dir.create("osf_dl")
  }
  
  dat_det <-
    osf_retrieve_node("https://osf.io/gzbkn/?view_only=8ca80573293b4e12b7f934a0f742b957") %>%
    osf_ls_files() %>%
    dplyr::filter(name == filename) %>%
    osf_download(conflicts = "overwrite", path = "osf_dl")
  
  out <- dat_det %>%
    dplyr::pull(local_path) %>%
    readr::read_rds()
  
  if (osf_dl_del_later) {
    unlink("osf_dl", recursive = TRUE)
  } else{
    unlink(dplyr::pull(dat_det, local_path))
  }
  
  return(out)
}
```

Here's how to use the function to read in the RECS and ANES datasets:

```{r}
#| label: intro-setup
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(survey)
library(srvyr)
library(osfr)
source("helper-fun/helper-function.R")
```

```{r}
#| label: intro-setup-readin
#| error: FALSE
#| warning: FALSE
#| message: FALSE
#| cache: TRUE
recs_in <- read_osf("recs_2020.rds")
anes_in <- read_osf("anes_2020.rds")
```

RECS is a study that provides energy consumption and expenditures data in American households. The Energy Information Administration funds RECS and has been fielded 15 times between 1950 and 2020. The survey has two components - the household survey and the energy supplier survey. In 2020, the household survey was collected by web and paper questionnaires. The household survey includes questions about appliances, electronics, heating, air conditioning (A/C), temperatures, water heating, lighting, respondent demographics, and energy assistance. The energy supplier survey consists of components relating to energy consumption and energy expenditure. Below is an overview of the `recs_in` data:

```{r}
#| label: intro-recs
recs_in %>% select(-starts_with("NWEIGHT"))
recs_in %>% select(starts_with("NWEIGHT"))
```

From this output, we can see that there are `r nrow(recs_in) %>% formatC(big.mark = ",")` rows and `r ncol(recs_in) %>% formatC(big.mark = ",")` variables.  We can see that there are variables containing an ID (`DOEID`), geographic information (e.g., `Region`, `state_postal`, `Urbanicity`), along with information about the house, including the type of house (`HousingUnitType`) and when the house was built (`YearMade`). Additionally, there is a long list of weighting variables that we will use in the analysis (e.g., `NWEIGHT`, `NWEIGHT1`, ..., `NWEIGHT60`). We will discuss using these weighting variables in Chapter \@ref(c03-specifying-sample-designs). For a more detailed codebook, see Appendix \@ref(recs-cb).

The ANES is a series study that has collected data from election surveys since 1948. These surveys contain data on public opinion and voting behavior in U.S. presidential elections. The 2020 survey (the data we will be using) was fielded to individuals over the web, through live video interviewing, or over with computer-assisted telephone interviewing (CATI). The survey includes questions on party affiliation, voting choice, and level of trust with the government. Here is an overview of the `anes_in` data. First, we show the variables starting with "V" followed by a number; these are the original variables. Then, we show you the remaining variables that we created based on the original data:

```{r}
#| label: intro-anes
anes_in %>% select(matches("^V\\d"))
anes_in %>% select(-matches("^V\\d"))
```

From this output we can see that there are `r nrow(anes_in) %>% formatC(big.mark = ",")` rows and `r ncol(anes_in) %>% formatC(big.mark = ",")` variables.  Most of the variables start with V20, so referencing the documentation for survey will be crucial to not get lost (see Chapter \@ref(c04-understanding-survey-data-documentation)).  We have created some more descriptive variables for you to use throughout this book, such as the age (`Age`) and gender (`Gender`) of the respondent, along with variables that represent their party affiliation (`PartyID`). Additionally, we need the variables  `Weight` and `Stratum` to analyze this data accurately.  We will discuss how to use these weighting variables in Chapters \@ref(c03-specifying-sample-designs) and \@ref(c04-understanding-survey-data-documentation). For a more detailed codebook, see Appendix \@ref(anes-cb).
