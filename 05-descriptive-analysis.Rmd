# Descriptive analyses in srvyr {#c05-descriptive-analysis}


```{r}
#| label: desc-summary-tab
#| echo: FALSE
tribble(
  ~c1, ~c2,
  "**Topic**", "Descriptive analysis of survey data",
  "**Purpose**", "purpose-blah",
  "**Learning Goals**", "learning-goals-blah"
) %>%
  knitr::kable(format="pandoc", col.names=NULL, caption="Summary of Chapter 5")
```


```{r}
#| label: desc-tidy
#| include: FALSE
knitr::opts_chunk$set(tidy = TRUE)
```

## Similarities between {dplyr} and {srvyr} functions

```{r}
#| label: desc-dstrat
#| include: false
library(srvyr)
library(survey)
data(api)

dstrata <- apistrat %>%
  as_survey_design(strata = stype, weights = pw)
```

One of the major advantages of using {srvyr} is that it applies {dplyr}-like syntax to the {survey} package. We can use pipes to specify a tbl_svy object, apply a function, and then feed that output into the next function's first argument. Functions follow the 'tidy' convention of snake_case functions names. In the example below, the mean and median are calculated for the variable `mpg` on the `mtcars` dataset.

```{r}
#| label: desc-dplyr-examp
mtcars %>%
  summarize(mpg_mean = mean(mpg),
            mpg_median = median(mpg))
```

Similarly, in the next example, the variance and standard deviation of the variable `api00` are calculated for the tbl_svy object `dstrata`. Note how similar the syntax is. When we dig into the functions later, we will show that the results output are similar in that one row is output for each group (if there are groups) but there will be more columns output. Specifically, by default, the standard error of the statistic is calculated in addition to the statistic.

```{r}
#| label: desc-srvyr-examp
dstrata %>% 
  summarize(api00_mean = survey_mean(api00),
            api00_med = survey_median(api00))
```

The functions in {srvyr} also play nicely with other tidyverse functions. If we wanted to select columns that have something in common, we can use {tidyselect} functions such as `starts_with()`, `num_range()`, etc.. In the examples below, a combination of `across()` and `starts_with()` to calculate the mean of variables starting with "Sepal" in the `iris` dataframe and then starting with `api` in the `dstrata` survey object.

```{r}
#| label: desc-dplyr-select
iris %>%
  summarize(
    across(starts_with("Sepal"), mean)
  )
```

```{r}
#| label: desc-srvyr-select
dstrata %>%
  summarize(
    across(starts_with("api"), survey_mean)
  )
```

We can use {dplyr} verbs such as `mutate()`, `filter()`, etc., on our survey object.

```{r}
#| label: desc-srvyr-mutate
dstrata_mod <- dstrata %>%
  mutate(api_diff = api00 - api99) %>%
  filter(stype=="E") %>%
  select(stype, api99, api00, api_diff, api_students=api.stu)
dstrata_mod
dstrata
```

Instead of data frames or tibbles, {srvyr} functions are meant for `tbl_svy` objects. Attempting to run data manipulation on non-`tbl_svy` objects will result in an error as shown in the example below while using the mtcars data frame which is not `tbl_svy` object.

```{r}
#| label: desc-nsobj-error
#| error: true
mtcars %>%
  summarize(mpg_mean = survey_mean(mpg))
```

A few functions in {srvyr} parallel functions in {dplyr}, such as `srvyr::summarize()` and `srvyr::group_by()`. Unlike {srvyr}-specific verbs, the package recognizes these parallel functions on a non-survey object. It will not error and instead give the equivalent output from {dplyr}:

```{r}
#| label: desc-nsobj-noerr
mtcars %>% 
  srvyr::summarize(mpg_mean = mean(mpg))
```

Because this book focuses on survey analysis, most of our pipes will stem from a survey object. We will not include the namespace for each function (e.g., `srvyr::summarize()`). Several functions in {srvyr} must be called within `srvyr::summarize()` with the exception of `srvyr::survey_count()` and `srvyr::survey_tally()` much like `dplyr::count()` and `dplyr::tally()` are not called within `dplyr::summarize()`.

These verbs can be used in conjunction with `group_by()` or `by/.by`, applying the functions on a group-by-group basis to create grouped summaries.

```{r}
#| label: desc-dplyr-groupby
mtcars %>%
  group_by(cyl) %>%
  dplyr::summarize(mpg_mean = mean(mpg))
```

We use a similar setup to summarize data in {srvyr}.

```{r}
#| label: desc-srvyr-groupby
dstrata %>%
  group_by(stype) %>%
  summarize(api00_mean = survey_mean(api00),
            api00_median = survey_median(api00))
```

## Chapter set up

Recall from Chapter \@ref(c03-specifying-sample-designs) the general process for estimation with the {srvyr} package:  

1. Create a `tbl_svy` object using `srvyr::as_survey_design()` or `srvyr::as_survey_rep()`.
2. Subset the data for subpopulations using `srvyr::filter()`, if needed.
3. Specify domains of analysis using `srvyr::group_by()`, if needed.
4. Within `srvyr::summarize()`, specify variables to calculate means, totals, proportions, quantiles, and more.

Filtering should be done after creating the `tbl_svy` object (using `as_survey_design()` or `as_survey_rep()`) because survey objects incorporate the survey design information into the resulting object. 

<!-- TODO: edit this depending on how much we've talked about it in earlier chapters-->
The Residential Energy Consumption Survey (RECS) provides energy consumption and expenditures data. It is funded by Energy Information Administration and collects information through energy suppliers through in-person, phone, and web interviews. It has been fielded 14 times between 1950 and 2020. Topics include appliances, electronics, heating, air conditioning (A/C), temperatures, water heating, lighting, energy bills, respondent demographics, and energy assistance.

The survey targets primarily occupied housing units in the US. RECS uses Balanced Repeated Replication (BRR) to estimate the variances. The full sample information is available on the [EIA website](https://www.eia.gov/consumption/residential/index.php). 

To begin analyzing RECS, we create a `tbl_svy` object using `srvyr::as_survey_design()`:

```{r}
#| label: recs_des
#| error: FALSE
#| warning: FALSE
#| message: FALSE
#| eval: FALSE
library(survey) # for survey analysis
library(srvyr) # for tidy survey analysis
library(readr)
library(osfr)
source("helper-fun/helper-functions.R")

recs_in <-
  read_osf("recs_2015.rds")

recs_des <- recs_in %>%
  as_survey_rep(
    weights = NWEIGHT,
    repweights = starts_with("BRRWT"),
    type = "Fay",
    rho = 0.5,
    mse = TRUE
  )
```


## Deciding on descriptive analyses

We select measures based on the type of variable we are analyzing. Variables are classified as categorical/nominal, ordinal, and interval/ratio.

* Categorical/nominal data: variables with levels or descriptions that cannot be ordered, such as the region of the country (North, South, East, and West)
* Ordinal data: variables that can be ordered, such as those from a Likert scale (strongly disagree, disagree, agree, and strongly agree)
* Interval/ratio: variables that are counted or measured, such as the total cost of electricity
  * Within interval/ratio data are *discrete* variables, whose values are whole numbers, such as a count of children, and *continuous* variables, whose values can lie anywhere on an interval, such as weight.
  
If our variable is categorical, such as gender or occupation, we might use frequency counts or percentages. In contrast, if the variable is continuous, such as income or age, we might use mean, median, or standard deviation.

Choosing appropriate measures is important to reach valid conclusions. Different variable types have distinct properties and levels of measurement, and we cannot apply all measures to all variables.

Our survey data may represent categorical variables using numeric codes. For example, the North, South, East, and West regions of the United States might be coded as 1, 2, 3, and 4, respectively. Though this is a categorical variable, this variable might be automatically read as numeric values when we import our data. This can lead to the common mistake of applying `survey_mean()` to all numeric columns in the dataset, including categorical values. 

This practice can lead to incorrect inferences because categorical variables lack a natural zero point or linear ordering, making measures like mean inappropriate. Instead, it is crucial to inspect the codebook, understand the variable type, and choose appropriate measures, such as frequency counts or percentages, to describe the data across regions.

Descriptive analysis can be categorized into univariate and multivariate analysis, depending on the number of variables we are analyzing. Below, we describe the descriptive analysis for single or multiple variables and the {srvyr} functions associated with the measures.

## Measures of distribution

Measures of distribution describe how often an event or response occurs. These measures include counts, proportions, and totals. 

* Examples: the proportion of students in California who did or did not receive an award based on their Academic Performance score; the estimated number of U.S. citizens who voted in the last election; the total amount of money residential households spend on electricity in a year

The {srvyr} package includes several functions for determining measures of distribution and all must be called within the `summarize()` function except `survey_count()` and `survey_tally()`. 

* The `survey_count()` and `survey_tally()` functions calculate weighted observations by group
* The `survey_total()` function calculates totals 
* The `survey_prop()` function calculates proportions 
* The `survey_quantile()` function calculates quantiles

### Counts and cross-tabulations

With `survey_count()`, we can calculate the estimated population counts for a given variable or combination of variables. Sometimes, these are referred to as cross-tabulations or crosstabs, for short. These summaries should be applied to categorical data and is used to get estimated counts of the population size of groups from the survey.


#### Syntax {-}
The syntax is very similar to the `dplyr::count()` syntax; however, as noted above, it can only be called on `tbl_svy` objects. Let's explore the syntax: 

```r
survey_count(
  x,
  ...,
  wt = NULL,
  sort = FALSE,
  name = "n",
  .drop = dplyr::group_by_drop_default(x),
  vartype = c("se", "ci", "var", "cv")
  )
  
survey_tally(
  x,
  wt,
  sort = FALSE,
  name = "n",
  vartype = c("se", "ci", "var", "cv")
)
```
The arguments are:

* `x`: a `tbl_svy` object created by `as_survey`
* `...`: variables to group by, passed to `group_by`
* `wt`: a variable to weight on in addition to the survey weights, defaults to `NULL`
* `sort`: how to sort the variables, defaults to `FALSE`
* `name`: the name of the count variable, defaults to `n`
* `.drop`: whether to drop empty groups
* `vartype`: type(s) of variation estimate to calculate, defaults to `se` (standard error)

We will discuss `vartype` in Section \@ref(Var-types) as this option occurs in all functions.

#### Examples {-}

If we do not specify any variables in `survey_count()`, the function will output the estimated population count (n) and standard error (n_se).  For example, in the RECS data we can obtain the estimated number of households in the U.S. (the target population) by running the following code: 

```{r}
#| label: desc-count-overall
recs_des %>%
  survey_count() 
```

```{r}
#| label: desc-count-oa-save
#| echo: FALSE
.est_pop <- recs_des %>%
  survey_count() %>%
  pull(n) %>%
  prettyNum(big.mark=",", digits=20)
```


Thus, the estimated number of households in the U.S. is `r .est_pop`.

To calculate the estimated number of observations for subgroups, such as Region and Division, we can add the variables of interest into the function. In the example below, the estimated number of housing units by region and division is calculated. Additionally, the name of the count variable is changed to "N" from the default ("n").

```{r}
#| label: desc-count-group
recs_des %>%
  survey_count(Region, Division, name = "N") 
```

```{r}
#| label: desc-count-group-save
#| echo: FALSE

.est_pop_div <- recs_des %>%
  survey_count(Region, Division, name = "N")  %>%
  mutate(N=prettyNum(N, big.mark=",", digits=20))
```

When we run the crosstab, we see  there are an estimated `r .est_pop_div %>% filter(Division=="New England") %>% pull(N)` housing units in the New England Division.

The `survey_tally()` function is similar to the `survey_count()`, but the `survey_tally()` function lacks the following arguments `...` (used to specify the by groups) and `.drop`. To get the estimated overall population, `survey_count()` and `survey_total()` will be identical and the example below yields the same results as using `survey_count()` previously.

```{r}
#| label: desc-tally-oa
recs_des %>%
  survey_tally() 
```

However, if we wanted the estimated population total by region and division, we will get an error if we try to use the same syntax:

```{r}
#| label: desc-tally-group-bad
#| error: TRUE
recs_des %>%
  survey_tally(Region, Division, name = "N") 
```

Instead, use a the `group_by()` function prior to using `survey_tally()` as is illustrated below:

```{r}
#| label: desc-tally-group-good
recs_des %>%
  group_by(Region, Division) %>%
  survey_tally(name = "N") 
```



### Totals and sums

The `survey_total()` function is analogous to `sum`. This can be used to find the estimated aggregate sum of an outcome and should be applied to continuous variables to obtain the estimated total quantity in a population. All the functions introduced henceforth in this chapter must be called from within `summarize()`. 

#### Syntax {-}

Let's explore the syntax:

```r
survey_total(
  x,
  na.rm = FALSE,
  vartype = c("se", "ci", "var", "cv"),
  level = 0.95,
  deff = FALSE,
  df = NULL
)
```

The arguments are:

* `x`: a variable, expression, or empty
* `na.rm`: an indicator of whether missing values should be dropped, defaults to `FALSE`
* `vartype`: type(s) of variation estimate to calculate, defaults to `se` (standard error)
* `level`: a number or a vector indicating the confidence level, defaults to 0.95
* `deff`: a logical value stating whether the design effect should be returned, defaults to FALSE
* `df`: (for `vartype = 'ci'`), a numeric value indicating degrees of freedom for the t-distribution


#### Examples {-}

To calculate a population count estimate with `survey_total()`, the argument `x` can be left empty as shown in the example below::

```{r}
#| label: desc-tot-nox
recs_des %>%
  summarize(survey_total())  
```

Note that the result from `recs_des %>% summarize(survey_total())` is equivalent to the `survey_count()` call. However, the `survey_total()` function is called within `summarize`, where as `survey_count()`, like `dplyr::count()`, is not. 

The difference between `survey_total()` and `survey_count()` is more evident when specifying continuous variables to sum. Let's compute the total cost of electricity in whole dollars from variable `DOLLAREL`^[RECS has two components: a household survey and an energy supplier survey. For each household that responds, their energy provider(s) are contacted to obtain their energy consumption and expenditure. This value reflects the dollars spent on electricity in 2015 according to the energy supplier. See https://www.eia.gov/consumption/residential/reports/2015/methodology/pdf/2015C&EMethodology.pdf for more details.]. We also calculate an unweighted estimate using `unweighted()`. The `unweighted()` function calculates unweighted summaries from `tbl_svy` object which reflects the summary among the **respondents** and does not extrapolate to a population estimate.

```{r}
#| label: desc-tot-dollarel
recs_des %>%
  summarize(
    elec_bill = survey_total(DOLLAREL),
    elec_unweight = unweighted(sum(DOLLAREL))
  )
```

```{r}
#| label: desc-tot-dollarel-save
#| echo: FALSE
.elbill <- recs_des %>%
  summarize(
    elec_bill = survey_total(DOLLAREL),
    elec_unweight = unweighted(sum(DOLLAREL))
  ) %>%
  mutate(across(starts_with("elec"), \(x) str_c("$", prettyNum(x, big.mark=",", digits=20))))
```

It is estimated that American residential households spent a total of `r .elbill %>% pull(elec_bill)` on electricity in 2015 and the estimate has a standard error of `r .elbill %>% pull(elec_bill_se)`. The unweighted function calculates unweighted counts and illustrates the total amount of money the respondents spent on electricity in 2015 which was `r .elbill %>% pull(elec_unweight)`.

Since we are using the {srvyr} package, we can use `group_by()` to calculate the cost of electricity by different groups. Let's see how much the cost of electricity in whole dollars differed between regions: 

```{r}
#| label: desc-tot-group
recs_des %>%
  group_by(Region) %>%
  summarize(elec_bill = survey_total(DOLLAREL))   
```

```{r}
#| label: desc-tot-group-save
#| echo: FALSE
.elbil_reg <- recs_des %>%
  group_by(Region) %>%
  summarize(elec_bill = survey_total(DOLLAREL)) %>%
  mutate(across(starts_with("elec"), \(x) str_c("$", prettyNum(round(x, 0), big.mark=",", digits=20))))
```

It's estimated that households in the Northeast spent `r .elbil_reg %>% filter(Region=="Northeast") %>% pull(elec_bill)` on electricity in 2015 while households in the South spent an estimated `r .elbil_reg %>% filter(Region=="South") %>% pull(elec_bill)`.

### Proportions

To find estimated proportions in a population, the `survey_prop()` function should be used. This should be applied to a categorical variable. 

#### Syntax {-}

```r
survey_prop(
  vartype = c("se", "ci", "var", "cv"),
  level = 0.95,
  proportion = TRUE,
  prop_method = c("logit", "likelihood", "asin", "beta", "mean", "xlogit"),
  deff = FALSE,
  df = NULL
)
```

The arguments are:

* `na.rm`: an indicator of whether missing values should be dropped, defaults to `FALSE`
* `vartype`: type(s) of variation estimate to calculate, defaults to `se` (standard error)
* `level`: a number or a vector indicating the confidence level, defaults to 0.95
* `proportion`: an indicator of whether the estimate is a proportion. Defaults to `TRUE`. Only impacts confidence intervals
* `prop_method`: Method to calculate confidence interval for confidence intervals. More details in \@ref(Var-types)
* `deff`: a logical value stating whether the design effect should be returned, defaults to FALSE
* `df`: (for `vartype = 'ci'`), a numeric value indicating degrees of freedom for the t-distribution

Prior to usage, use the `group_by()` function to specify the categories of interest. 

#### Examples {-}

```{r}
#| label: desc-p-ex1
recs_des %>%
  group_by(Region) %>%
  summarize(p=survey_prop()) 
```

```{r}
#| label: desc-p-ex1-save
#| echo: FALSE
.preg <- recs_des %>%
  group_by(Region) %>%
  summarize(p=survey_prop()) %>%
  mutate(p=p*100 %>% signif(3))
```

`r .preg %>% filter(Region=="Northeast") %>% pull(p)`% of the households are in the Northeast, `r .preg %>% filter(Region=="Midwest") %>% pull(p)`% in the Midwest, and so on.

Note: `survey_prop()` is essentially the same as using `survey_mean()` (discussed later) with a categorical variable and without specifying a numeric variable in the `x` argument. The following code will give us the same results as above:

```{r}
#| label: desc-p-ex2
recs_des %>%
  group_by(Region) %>%
  summarize(p=survey_mean())
```

Getting proportions by more than one variable is possible. In the next example, we look at the proportion of housing units by Region and whether air-conditioning is used (`ACUsed`).^[Question text: Is any air condition equipment used in your home?] 

```{r}
#| label: desc-pmulti-ex1
recs_des %>%
  group_by(Region, ACUsed) %>%
  summarize(p=survey_mean())
```

When specifying multiple variables, the proportions are conditional. In the results above, notice that the proportions sum to 1 within each region. This can be interpreted as the proportion of housing units that have air conditioning WITHIN each region. If we want the joint proportion instead, the interact function is necessary. In the example below, the interact function is used on `Region` and `ACUsed`:

```{r}
#| label: desc-pmulti-ex2
recs_des %>%
  group_by(interact(Region, ACUsed)) %>%
  summarize(p=survey_mean())
```


### Quantiles

The `survey_quantile()` function can be used to find the estimated quantiles of a continuous outcome. For example, we might want estimates of the quartiles of income in a population to understand how the income is distributed. 

#### Syntax {-}

Let's explore the syntax:

```r
survey_quantile(
  x,
  quantiles,
  na.rm = FALSE,
  vartype = c("se", "ci", "var", "cv"),
  level = 0.95,
  interval_type = c("mean", "beta", "xlogit", "asin", "score", "quantile"),
  qrule = c("math", "school", "shahvaish", "hf1", "hf2", "hf3", "hf4", "hf5", "hf6",
    "hf7", "hf8", "hf9"),
  df = NULL
)
```

The arguments are:

* `x`: a variable, expression, or empty
* `quantiles`: A vector of quantiles to calculate
* `na.rm`: an indicator of whether missing values should be dropped, defaults to `FALSE`
* `vartype`: type(s) of variation estimate to calculate, defaults to `se` (standard error)
* `level`: a number or a vector indicating the confidence level, defaults to 0.95
* `interval_type`: method for calculating confidence interval. More details in \@ref(Var-types).
* `qrule`: rule for defining quantiles. The default is the lower end of the quantile interval ("math"). The midpoint of the quantile interval is the "school" rule. "hf1" to "hf9" are weighted analogues to type=1 to 9 in `quantile()`. "shahvaish" corresponds to a rule proposed by @shahvaish. See `vignette("qrule", package="survey")` for more information.
* `df`: (for `vartype = 'ci'`), a numeric value indicating degrees of freedom for the t-distribution

#### Examples {-}

Quantiles are useful in learning about the distribution of an outcome. Let's look into the quartiles, specifically, the first quartile (p=0.25), the median (p=0.5) and the third quartile (p=0.75) of electric bills.

```{r}
#| label: desc-quantile-oa
recs_des %>%
  summarize(elec_bill = survey_quantile(DOLLAREL, quantiles=c(0.25, .5, 0.75)))
```

```{r}
#| label: desc-quantile-oa-save
#| echo: FALSE
.elbill_quant <- recs_des %>%
  summarize(elec_bill = survey_quantile(DOLLAREL, quantiles=c(0.25, .5, 0.75))) %>%
  mutate(
    across(c(starts_with("elec_bill")), \(x) str_c("$", prettyNum(round(x, 0), big.mark=",")))
  )
```

In the output above, we see the 3 quartiles and their respective standard errors. We can also estimate the quantiles of electric bills by region as shown below:

```{r}
#| label: desc-quantile-reg
recs_des %>%
  group_by(Region) %>%
  summarize(
    elec_bill = survey_quantile(DOLLAREL, quantiles=c(0.25, .5, 0.75)))
```

While we can specify quantiles of 0 and 1 which represent the minimum and maximum, this is not recommended. It only returns the minimum and maximum of the respondents and cannot be extrapolated to the population as there is no valid definition of standard error.

```{r}
#| label: desc-quantile-minmax
recs_des %>%
  summarize(
    elec_bill = survey_quantile(DOLLAREL, quantiles=c(0, 1)))
```


## Measures of central tendency

Measures of central tendency find the central (or average) responses. These include the mean, median, and mode.

* Examples: Average 2000 Academic Performance Index in California, the median house price in Canada 

The {srvyr} package includes functions for estimating the mean and median, and they must be called within the `summarize()` function.

* The `survey_mean()` function calculates means
* The `survey_median()` function calculates medians

### Means

The `survey_mean()` function calculate the estimated means of continuous variables of survey data. 

#### Syntax {-}
Let's explore the syntax:

```r
survey_mean(
  x,
  na.rm = FALSE,
  vartype = c("se", "ci", "var", "cv"),
  level = 0.95,
  proportion = FALSE,
  prop_method = c("logit", "likelihood", "asin", "beta", "mean"),
  deff = FALSE,
  df = NULL
)

```

The arguments are:

* `x`: a variable, expression, or empty
* `na.rm`: an indicator of whether missing values should be dropped, defaults to `FALSE`
* `vartype`: type(s) of variation estimate to calculate, defaults to `se` (standard error)
* `level`: a number or a vector indicating the confidence level, defaults to 0.95
* `proportion`: an indicator of whether the estimate is a proportion. Defaults to `FALSE`
* `prop_method`: Method to calculate confidence interval for confidence intervals. More details in \@ref(Var-types)
* `deff`: a logical value stating whether the design effect should be returned, defaults to FALSE
* `df`: (for `vartype = 'ci'`), a numeric value indicating degrees of freedom for the t-distribution

#### Examples {-}

We can calculate the estimated average cost of electricity in the U.S. and then for each region in the U.S.: 

```{r}
#| label: desc-mn-oa
recs_des %>%
  summarize(elec_bill = survey_mean(DOLLAREL))
```

```{r}
#| label: desc-mn-oa-save
#| echo: FALSE
.elbill_mn <- recs_des %>%
  summarize(elec_bill = survey_mean(DOLLAREL)) %>%
  mutate(across(starts_with("elec"), \(x) str_c("$", prettyNum(round(x, 0), big.mark=",", digits=6))))
```

```{r}
#| label: desc-mn-group
recs_des %>%
  group_by(Region) %>%
  summarize(elec_bill = survey_mean(DOLLAREL))
```

```{r}
#| label: desc-mn-group-save
#| echo: FALSE
.elbill_mn_reg <- recs_des %>%
  group_by(Region) %>%
  summarize(elec_bill = survey_mean(DOLLAREL)) %>%
  mutate(across(starts_with("elec"), \(x) str_c("$", prettyNum(round(x, 0), big.mark=",", digits=6))))
```

Nationally, the average household spent `r pull(.elbill_mn, elec_bill)` in 2015 with some variability by region. 
Households from the West spent `r .elbill_mn_reg %>% filter(Region=="West") %>% pull(elec_bill)` on electricity and in the South, they spent an average of `r .elbill_mn_reg %>% filter(Region=="South") %>% pull(elec_bill)`.

### Medians

The median is another measure of central tendency which provides an estimate of the midpoint of a continuous distribution. Medians are less subject to outliers than means. 

#### Syntax {-}

The syntax is nearly identical to `survey_quantile()` as the median is a special quantile with p=0.5. 

```r
survey_median(
  x,
  na.rm = FALSE,
  vartype = c("se", "ci", "var", "cv"),
  level = 0.95,
  interval_type = c("mean", "beta", "xlogit", "asin", "score", "quantile"),
  qrule = c("math", "school", "shahvaish", "hf1", "hf2", "hf3", "hf4", "hf5", "hf6",
    "hf7", "hf8", "hf9"),
  df = NULL
)
```

The arguments are:

* `x`: a variable, expression, or empty
* `na.rm`: an indicator of whether missing values should be dropped, defaults to `FALSE`
* `vartype`: type(s) of variation estimate to calculate, defaults to `se` (standard error)
* `level`: a number or a vector indicating the confidence level, defaults to 0.95
* `interval_type`: method for calculating confidence interval. More details in \@ref(Var-types).
* `qrule`: rule for defining quantiles. The default is the lower end of the quantile interval ("math"). The midpoint of the quantile interval is the "school" rule. "hf1" to "hf9" are weighted analogues to type=1 to 9 in `quantile()`. "shahvaish" corresponds to a rule proposed by @shahvaish. See `vignette("qrule", package="survey")` for more information.
* `df`: (for `vartype = 'ci'`), a numeric value indicating degrees of freedom for the t-distribution

#### Examples {-}

We can calculate the estimated median cost of electricity in the U.S. and then for each region in the U.S.: 

```{r}
#| label: desc-med-oa
recs_des %>%
  summarize(elec_bill = survey_median(DOLLAREL))
```

```{r}
#| label: desc-med-oa-save
#| echo: FALSE
.elbill_med <- recs_des %>%
  summarize(elec_bill = survey_median(DOLLAREL)) %>%
  mutate(across(starts_with("elec"), \(x) str_c("$", prettyNum(round(x, 0), big.mark=",", digits=6))))
```

```{r}
#| label: desc-med-group
recs_des %>%
  group_by(Region) %>%
  summarize(elec_bill = survey_median(DOLLAREL))
```

```{r}
#| label: desc-med-group-save
#| echo: FALSE
.elbill_med_reg <- recs_des %>%
  group_by(Region) %>%
  summarize(elec_bill = survey_median(DOLLAREL)) %>%
  mutate(across(starts_with("elec"), \(x) str_c("$", prettyNum(round(x, 0), big.mark=",", digits=6))))
```

Nationally, the median household spent `r pull(.elbill_med, elec_bill)` in 2015 with some variability by region. 
Households from the West spent `r .elbill_med_reg %>% filter(Region=="West") %>% pull(elec_bill)` on electricity and in the South, they spent an average of `r .elbill_med_reg %>% filter(Region=="South") %>% pull(elec_bill)`.


Note that the 50^th^ percentile and the median are the same, as expected. The average electric bill for households was `r pull(.elbill_mn, elec_bill)` but the estimated median electric bill is `r pull(.elbill_med, elec_bill)` indicating the distribution is likely right skewed.


## Measures of dispersion

Measures of dispersion describe how data spreads around the central tendency for continuous variables. These measures include the standard deviation, variance, and range. 

* Examples: The standard deviation of the 2000 Academic Performance Index in California, the variance of electricity expenditure in Ohio

* The `survey_var()` function calculates variances
* The `survey_sd()` function calculates standard deviations

It should be noted these are estimates of the population variance and population standard deviation. These are not standard errors of another estimate. In our experience, these are sometimes used when designing a future study as understanding the variability in the population can help inform the precision of a future sampling design.

### Standard deviation and variance

The standard deviation estimate is simply the square root of the variance estimate and thus the functions have the same arguments except the standard deviation does not allow the usage of `vartype`. 

#### Syntax {-}

The syntax is as follows: 


```r
survey_var(
  x,
  na.rm = FALSE,
  vartype = c("se", "ci", "var"),
  level = 0.95,
  df = NULL
)

survey_sd(
  x, 
  na.rm = FALSE
)
```

The arguments are:

* `x`: A variable or expression, or empty
* `na.rm`: A logical value to indicate whether missing values should be dropped
* `vartype`: Report variability as one or more of: standard error ("se", default) or variance ("var") (confidence intervals and coefficient of variation not available).
* `level`: (For vartype = "ci" only) A single number or vector of numbers indicating the confidence level.
* `df`: (For vartype = "ci" only) A numeric value indicating the degrees of freedom for t-distribution

#### Examples {-}

Returning to electricity bills, we look at the amount of variability in electricity expenditure.

```{r}
#| label: desc-sdvar-ex1

recs_des %>%
  summarize(
    var_elbill = survey_var(DOLLAREL),
    sd_elbill = survey_sd(DOLLAREL)
  )
```

A warning message may be displayed if using a replicate design. Your results are still valid. The results above give an estimate of the population variance of electricity bills (`var_elbill`), the standard error of that variance (`var_elbill_se`), and the estimated population standard deviation of electricity bills. Note that there is no standard error associated with the standard deviation - this is the only estimate that does not include a standard error. 

Like other estimates, we can calculate the variance by region. This would be useful to learn if the variability is similar across regions:

```{r}
#| label: desc-sdvar-ex2

recs_des %>%
  group_by(Region) %>%
  summarize(
    var_elbill = survey_var(DOLLAREL),
    sd_elbill = survey_sd(DOLLAREL)
  )
```


## Measures of relationship

Measures of relationship describe how variables relate to each other. In this section, we present two measures - the Pearson's correlation (simply referred to correlation henceforth) and the ratio. 

* Examples: Correlation between house square footage and electricity expenditure. Ratio of number of diseased teeth to total teeth.

* The `survey_corr()` function calculates the Pearson's correlation between two variables
* The `survey_ratio()` function calculates ratio between two variables

### Correlations

The correlation is a measure of linear relationship between two continuous variables which ranges between -1 and 1. A sample correlation for a simple random sample is calculated as:

$$\frac{\sum (x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum (x_i-\bar{x})^2} \sqrt{\sum(y_i-\bar{y})^2}} $$
When using `survey_corr()`, for designs other than a simple random sample, the weights are applied when estimating the correlation. 

#### Syntax {-}

The syntax for `survey_corr()` is as follows:

```r
survey_corr(
  x,
  y,
  na.rm = FALSE,
  vartype = c("se", "ci", "var", "cv"),
  level = 0.95,
  df = NULL
)
```

The arguments are:

* `x`: A variable or expression
* `y`: A variable or expression
* `na.rm`: A logical value to indicate whether missing values should be dropped
* `vartype`: NULL to report no variability. Otherwise one or more of: standard error ("se", the default), confidence interval ("ci"), variance ("var") or coefficient of variation ("cv").
* `level`: (For vartype = "ci" only) A single number or vector of numbers indicating the confidence level
* `df`: (For vartype = "ci" only) A numeric value indicating the degrees of freedom for t-distribution

#### Examples {-}

We can calculate the correlation between total square footage (`TOTSQFT_EN`)^[Question text: What is the square footage of your home?] and electricity consumption (`BTUEL`)^[BTUEL is derived from the supplier side component of the survey where BTUEL represents the electricity consumption in British thermal units (Btus) converted from kilowatt hours (kWh) in a year].

```{r}
#| label: desc-corr-1

recs_des %>%
  summarize(
    SQFT_Elec_Corr=survey_corr(TOTSQFT_EN, BTUEL)
  )
```

Like with other statistics, we can do this by another variable. For example, we can examine the correlation by whether air-conditioning is used (`ACUsed`).

```{r}
#| label: desc-corr-2

recs_des %>%
  group_by(ACUsed) %>%
  summarize(
    SQFT_Elec_Corr=survey_corr(TOTSQFT_EN, DOLLAREL)
  )
```


### Ratios

The ratio estimate is one many are not as familiar with. The ratio is a measure of the ratio of the sum of two variables, specifically in the form of:

$$ \frac{\sum x_i}{\sum y_i}.$$
The ratio is not the same as calculating the following:

$$ \frac{1}{N} \sum \frac{x_i}{y_i} $$ 
which could be calculated with `survey_mean()` by creating a derived variable $z=x/y$ and then calculating the mean of $z$. 

Consider a survey of police agencies in the United States. We might want to estimate the ratio of female police officers to total police officers. This could be done with `survey_ratio(Female_Officers, Total_Officers)`. If instead, we used `survey_means(Female_Officers/Total_Officers)`, we would be estimating the average percentage of female officers across agencies which is a different quantity. 

#### Syntax {-}

The syntax for `survey_corr()` is as follows:

```r
survey_ratio(
  numerator,
  denominator,
  na.rm = FALSE,
  vartype = c("se", "ci", "var", "cv"),
  level = 0.95,
  deff = FALSE,
  df = NULL
)
```

The arguments are:

* `numerator`: The numerator of the ratio
* `denominator`: The denominator of the ratio
* `na.rm`: A logical value to indicate whether missing values should be dropped
* `vartype`: Report variability as one or more of: standard error ("se", default), confidence interval ("ci"), variance ("var") or coefficient of variation ("cv").
* `level`: A single number or vector of numbers indicating the confidence level
* `deff`: A logical value to indicate whether the design effect should be returned
* `df`: (For vartype = "ci" only) A numeric value indicating the degrees of freedom for t-distribution

#### Examples {-}

Suppose we wanted to find the ratio of dollars spent on liquid propane per unit (in British thermal unit [Btu]) nationally. If we wanted to find the average cost to a household, we could use `survey_mean()` but to find the national unit rate, we can use ratio. In the following example, we will show both methods and discuss the interpretation of each:

```{r}
#| label: desc-ratio-1

recs_des %>%
  summarize(
    DOLLARLP_Tot=survey_total(DOLLARLP),
    BTULP_Tot=survey_total(BTULP),
    DOL_BTU_Rat=survey_ratio(DOLLARLP, BTULP),
    DOL_BTU_Avg=survey_mean(DOLLARLP/BTULP, na.rm=TRUE),
  )
```

```{r}
#| label: desc-ratio-1-save
#| echo: FALSE
.rat_out <- recs_des %>%
  summarize(
    DOLLARLP_Tot=survey_total(DOLLARLP),
    BTULP_Tot=survey_total(BTULP),
    DOL_BTU_Rat=survey_ratio(DOLLARLP, BTULP),
    DOL_BTU_Avg=survey_mean(DOLLARLP/BTULP, na.rm=TRUE),
  )

num <- pull(.rat_out, DOLLARLP_Tot) %>% formatC(big.mark=",", digits=0, format="f")
den <- pull(.rat_out, BTULP_Tot) %>% formatC(big.mark=",", digits=0, format="f")
rat <- pull(.rat_out, DOL_BTU_Rat) %>% signif(3)
avg <- pull(.rat_out, DOL_BTU_Rat) %>% signif(3)
```

In the output above, the ratio of the total spent on liquid propane to the total consumption was `r rat` but the average rate was `r avg`. With a little calculation, it can be shown that the ratio is the ratio of the totals DOLLARLP_Tot/BTULP_Tot=`r num`/`r den`=`r rat`. While the ratio could be calculated manually in this manner, the standard error requires usage of the `survey_ratio()` function. The average can be interpreted as the average rate a household pays. 

As previously done, we can use `group_by()` to examine whether this rate varies by region.

```{r}
#| label: desc-ratio-2

recs_des %>%
  group_by(Region) %>%
  summarize(
    DOL_BTU_Rat=survey_ratio(DOLLARLP, BTULP),
  )
```

Though not a statistical test, it does appear the cost rates in the Midwest for liquid propane are the lowest.

## Additional topics

### Subpopulation analysis

Briefly, we mentioned using `filter()` to subset a survey object for analysis. This operation should be done after creating the design object. In rare circumstances, subsetting data before creating the object can lead to incorrect variability estimates. This can occur if subsetting removes an entire PSU.

Suppose, we wanted estimates of the average amount spent on natural gas among housing units that use natural gas. This could be obtained by first filtering records to only include records where `BTUNG>0` and then finding the average amount of money spent.

```{r}
#| label: desc-subpop

recs_des %>%
  filter(BTUNG>0) %>%
  summarize(
    NG_mean=survey_mean(DOLLARNG, vartype = c("se", "ci"))
  )
```

Note that this yields a higher mean than when not applying the filter. When including housing units that do not use natural gas, many $0 amounts are included in the mean calculation.

```{r}
#| label: desc-subpop-2

recs_des %>%
  summarize(
    NG_mean=survey_mean(DOLLARNG, vartype = c("se", "ci"))
  )
```


### Variability types {#Var-types}

In all of the functions discussed above (except `survey_sd()`), measures of variability can be calculated. The default variability measure is the standard error (`se`). There are 4 types of variability measures that can be calculated which are:

* `se`: standard error
    * The estimated standard deviation of the estimate
    * Output has a column with the variable name specified in `summarize()` with a suffix of "_se"
* `ci`: confidence interval
    * The lower and upper limits of a confidence interval
    * Output has a column with the variable name specified in `summarize()` with a suffix of "_low" and "_upp"
    * By default, this is a 95% confidence interval but can be changed by using the argument level and specifying a number between 0 and 1. For example, `level=0.8` would produce a 80% confidence interval
* `var`: variance
    * The estimated variance of the estimate
    * Output has a column with the variable name specified in `summarize()` with a suffix of "_var"
* `cv`: coefficient of variation
    * A ratio of the standard error and the estimate
    * Output has a column with the variable name specified in `summarize()` with a suffix of "_cv"
    * Not an option available for `survey_var()`

We will return to the example of calculating the average electricity bill but include all the variability type options. Multiple options can be used at once.

```{r}
#| label: desc-vartypes-1

recs_des %>%
  summarize(elec_bill = survey_mean(DOLLAREL, vartype=c("se", "ci", "var", "cv")))
```

It is also possible to not return any variability estimate by specifying `vartype` as `NULL`.

```{r}
#| label: desc-vartypes-2

recs_des %>%
  summarize(elec_bill = survey_mean(DOLLAREL, vartype=NULL))
```

In the example below, an 80% confidence interval rather than the deafult 95% confidence interval is calculated

```{r}
#| label: desc-vartypes-3

recs_des %>%
  summarize(elec_bill = survey_mean(DOLLAREL, vartype="ci", level=0.8))
```

For means, counts, totals, ratios, correlations, and variances, the confidence intervals are always calculated using a symmetric t-distribution based confidence interval as follows:

$$ \text{estimate} \pm t^*_{df}\times SE$$
where $t^*_{df}$ is the critical value from a t-distribution based on the confidence level and the degrees of freedom. By default the degrees of freedom are calculated based off of the design or number of replicates but they can be specified using the argument `df`. For survey design objects, the degrees of freedom are calculated as the number of PSUs minus the number of strata. For replicate based objects, the degrees of freedom are calculated as one less than the rank of the matrix of replicate weight where the number of replicates is typically the rank. Note that specifying `df=Inf` is equivalent to using a normal (z-based) confidence interval. There are more options for calculating confidence intervals for proportions and quantiles (including medians) which are further described.

#### Confidence intervals for proportions {-}

The interval above is referred to as a Wald-type interval. While a Wald-type interval using a symmetric t-based confidence interval is an option, this does not generally have the correct coverage rate when sample sizes are small and/or the proportion is "near" 0 or 1. Thus, other methods have been developed to calculate confidence intervals and can be specified using the `prop_method` option in `survey_prop()`. The options include:

* `logit`: fits a logistic regression model and computes a Wald-type interval on the log-odds scale which is then transformed to the probability scale. This is the default method.
* `likelihood`: uses the (Rao-Scott) scaled chi-squared distribution for the log-likelihood from a binomial distribution.
* `asin`: uses the variance-stabilizing transformation for the binomial distribution, the arcsine square root, and then back-transforms the interval to the probability scale
* `beta`: uses the incomplete beta function with an effective sample size based on the estimated variance of the proportion.
* `mean`: the Wald-type interval
* `xlogit`: uses a logit transformation of the proportion, calculates a Wald-type interval, and then back-transforms to the probability scale. This method is implemented in SUDAAN and SPSS.

In the example below, a logical derived variable is created for whether someone voted for a candidate that was neither Trump nor Biden in the 2020 presidential election. Then, a confidence interval is calculated for that proportion using the various methods. 

```{r}
#| label: desc-prop-cis
anes_des_sel <- anes_des  %>%
  mutate(VoteOther=(VotedPres2020_selection=="Other")) %>%
  group_by(VoteOther)

ci_p_vototh <- 
  anes_des_sel %>% summarize(
  p_logit=survey_prop(vartype="ci", prop_method="logit"),
  p_like=survey_prop(vartype="ci", prop_method="likelihood"),
  p_asin=survey_prop(vartype="ci", prop_method="asin"),
  p_beta=survey_prop(vartype="ci", prop_method="beta"),
  p_mean=survey_prop(vartype="ci", prop_method="mean"),
  p_xlogit=survey_prop(vartype="ci", prop_method="xlogit")
) %>% filter(VoteOther) 

ci_p_vototh %>%
  select(ends_with("low"))

ci_p_vototh %>%
  select(ends_with("upp"))
```

#### Confidence intervals for quantiles {-}

Like confidence intervals for proportions, there are several methods for calculating confidence intervals of quantiles which includes medians. The methods for interval types are many of the same as those for proportions (asin, beta, mean, and xlogit) with the addition of two more methods including:

* `score`: the Francisco & Fuller confidence interval based on inverting a score test (only available for design-based survey objects and not replicate based objects)
* `quantile`: based on the replicates of the quantile. Not valid for jackknife-type replicates but available for bootstrap and BRR replicates.

In the example below the confidence interval for the 95^th^ percentile of electricity expenditure is calculated using the various methods. 

```{r}
#| label: desc-prop-quant

ci_elbill_p90 <- recs_des %>% summarize(
  p90_asin=survey_quantile(DOLLAREL, .9, vartype="ci", interval_type = "asin"),
  p90_beta=survey_quantile(DOLLAREL, .9, vartype="ci", interval_type = "beta"),
  p90_mean=survey_quantile(DOLLAREL, .9, vartype="ci", interval_type = "mean"),
  p90_xlog=survey_quantile(DOLLAREL, .9, vartype="ci", interval_type = "xlogit"),
  p90_quant=survey_quantile(DOLLAREL, .9, vartype="ci", interval_type = "quantile")
)
ci_elbill_p90 %>%
  select(ends_with("low"))
ci_elbill_p90 %>%
  select(ends_with("upp"))
  
```

In the example below, the mean and score type confidence interval are calculated for the median age of the voting age population.

```{r}
#| label: desc-prop-quant-2

anes_des %>% summarize(
  am=survey_median(Age, vartype="ci", interval_type="mean", na.rm=TRUE),
  as=survey_median(Age, vartype="ci", interval_type="score", na.rm=TRUE),
  )
```

Note that in instances when there are many ties in the data, the score method can produce confidence intervals that do not contain the estimate as is shown above. The documentation in the {survey} package indicates this method has lower performance than the beta and logit intervals. This is the method implemented in SUDAAN though SUDAAN now adds noise to the values to prevent the issue with the ties.

### Design effects

The design effect measures how the precision of an estimate is impacted by the sampling design. A design effect is calculated as the ratio of the variance of an estimate under the design at hand to the variance of the estimate under a simple random sample without replacement (SRS). A design effect less than 1 indicates that the design is *more* statisticially efficient than a SRS design. This is rare but possible in a stratified sampling design where the outcome is correlated with the stratification variable(s). A design effect greater than 1 indicates that the design is *less* statistically effecient than a SRS design. From a design effect, we can calculate the effective sample size as follows:

$$n_{eff}=\frac{n}{D_{eff}} $$

where $n$ is the nominal sample size (number of survey responses) and $D_{eff}$ is the estimated design effect. The effective sample size has an interesting interpretation that a survey using a SRS design would need a sample size of $n_{eff}$ to obtain the same precision as the design at hand which is where the efficiency interpretation comes in. Design effects are outcome specific - outcomes that are less clustered in the population have smaller design effects than outcomes which are clustered.

In the {srvyr} package, design effects can be calculated for totals, proportions, means, and ratio estimates by setting the `deff` argument to TRUE in the corresponding functions. 

#### Example {-}

In the example below, the design effect is calculated for the means the consumption of electricity (BTUEL), natural gas (BTUNG), liquid propane (BTULP), fuel oil (BTUFO), wood (BTUWOOD), and wood pellets (BTUPELLET).

```{r}
#| label: desc-deff

recs_des %>%
  summarize(
    across(c(BTUEL, BTUNG, BTULP, BTUFO, BTUWOOD, BTUPELLET),
           ~survey_mean(.x, deff=TRUE, vartype=NULL))
  ) %>%
  select(ends_with("deff"))
```


### Creating summary rows 

When using `group_by()` in analysis, results are returned with a row for each group or group combination. Often, we want a summary row for the estimate for the entire population. For example, we may want the average electricity consumption by region AND nationally. The {srvyr} package has a function `cascade()` which adds summary rows for the total of a group. It is used in place of summarize and has some additional features. 

#### Syntax {-}

The syntax is as follows:

```
cascade(
  .data, 
  ..., 
  .fill = NA, 
  .fill_level_top = FALSE, 
  .groupings = NULL
)
```

where the arguments are:

* `.data`: A `tbl_svy` object
* `...`: Name-value pairs of summary functions
* `.fill`: Value to fill in for group summaries (defaults to `NA`)
* `.fill_level_top`: When filling factor variables, whether to put the value '.fill' in the first position (defaults to FALSE, placing it in the bottom).
* `.groupings`: (Experimental) A list of lists of quosures to manually specify the groupings to use, rather than the default.

#### Examples {-}

First, let's take a look at a simple example and then build on it to examine the features of the function. In the first example, all default values are used.

```{r}
#| desc-casc-ex1

recs_des %>%
  group_by(Region) %>%
  cascade(DOLLAREL_mn=survey_mean(DOLLAREL))
```

The last row where `Region=NA` is the national average electricity bill. We might wish to have a better name for it and can do that using the `.fill` argument.

```{r}
#| desc-casc-ex2

recs_des %>%
  group_by(Region) %>%
  cascade(DOLLAREL_mn=survey_mean(DOLLAREL),
          .fill="National")
```

We can also have more than one grouping variable as follows:

```{r}
#| desc-casc-ex3

recs_des %>%
  group_by(Region, Urbanicity) %>%
  cascade(DOLLAREL_mn=survey_mean(DOLLAREL), .fill="Total") %>%
  ungroup()
```

We can move the summary row to being the first row:

```{r}
#| desc-casc-ex4

recs_des %>%
  group_by(Region) %>%
  cascade(DOLLAREL_mn=survey_mean(DOLLAREL), .fill="National", .fill_level_top = TRUE) %>%
  ungroup()
```

### Calculating estimates for many outcomes

Often, we are interested in a summary statistic across many variables. Two useful tools in doing this are the `across()` function in {dplyr} which has been shown a few times above and the `map()` function in {purrr}. 

The `across()` function allows you to apply the same function to several columns within `summarize()`. This works well for usage with all functions shown above except `survey_prop()`. In a later example, we will tackle several proportions. 

#### across() Example 1 {-}
Suppose, we want to calculate the total consumption for each fuel type and the average consumption for each fuel type with coefficients of variation. These include the consumption of electricity (BTUEL), natural gas (BTUNG), liquid propane (BTULP), fuel oil (BTUFO), wood (BTUWOOD), and wood pellets (BTUPELLET) as illustrated in the discussion on design effects. These are the only variables that start with "BTU" so we can use that to our advantage. 

```{r}
#| label: desc-multi-1

consumption_ests <- recs_des %>%
  summarize(
    across(starts_with("BTU"), 
           list(tot=~survey_total(.x, vartype="cv"), mn=~survey_mean(.x, vartype="cv")))
  )

consumption_ests
```

In the example above, this results in a very wide table. We may instead want a row for each fuel type. Using the `pivot_longer()`, `separate_wider_delim()`, and `pivot_wider()` functions from {tidyr} can help us get there. We will first make the data much longer. First we make the table longer:

```{r}
#| label: desc-multi-2
consumption_ests_long <- consumption_ests %>%
  pivot_longer(everything())

consumption_ests_long
```

Then we separate out the name column to separate columns for naming later:


```{r}
#| label: desc-multi-3
consumption_ests_sep <- consumption_ests_long %>%
  separate_wider_delim(name, 
                       names=c("FuelType", "EstType", "EstType2"), 
                       delim="_", 
                       too_few = "align_start")

consumption_ests_sep
```

Then we create what will become column names and pivot wider to create a table that is almost ready for a publication. A bit more on that will be covered in Chapter \@ref(c08-communicating-results).

```{r}
#| label: desc-multi-4

consumption_ests_sep %>%
  mutate(ColName=case_when(
    EstType=="tot"&is.na(EstType2)~"Total",
    EstType=="tot"&!is.na(EstType2)~"Total (CV)",
    EstType=="mn"&is.na(EstType2)~"Mean",
    EstType=="mn"&!is.na(EstType2)~"Mean (CV)",
  )) %>%
  pivot_wider(id_cols=FuelType, names_from=ColName, values_from = value) %>%
  mutate(FuelType=str_remove(FuelType, "BTU"))
```

#### across() Example 2 {-}

As mentioned earlier, proportions will not work as well directly with the across method. If you wanted the proportion of houses with air conditioning and the proportion of houses with heating, you might imagine needing two `group_by()` statements as follows:

```{r}
#| label: desc-multip-1

recs_des %>%
  group_by(ACUsed) %>%
  summarise(p=survey_prop())

recs_des %>%
  group_by(SpaceHeatingUsed) %>%
  summarise(p=survey_prop())
```

If you are *only* interested in the `TRUE` outcomes, that is the proportion that do have air conditioning and the proportion that have heating, you can use the fact that `survey_mean()` applied to a logical variable is the same as using `survey_prop()` as is shown below:


```{r}
#| label: desc-multip-2

cool_heat_tab <- recs_des %>%
  summarise(across(c(ACUsed, SpaceHeatingUsed), ~survey_mean(.x)))

cool_heat_tab
```

Note the estimates are the same as when using the separate `group_by()` statements. Similar to previously done, we can use a combination of `pivot_longer()` and `pivot_wider()` to create a table in a format better suited for distribution.

```{r}
#| label: desc-multip-3

cool_heat_tab %>%
  pivot_longer(everything()) %>%
  separate_wider_delim(name, names=c("Comfort", "EstType"), delim="_", too_few = "align_start") %>%
  mutate(EstType=if_else(is.na(EstType), "p", EstType)) %>%
  pivot_wider(names_from=EstType, values_from=value)
```


#### map example {-}

If you want to calculate something again and again, loops are a common tool. The {purrr} package has the `map()` functions which like a loop allows you to do something in the same way many times. In our case, we want to calculate proportions from the same design multiple times. We find an easy way to do this is to think about how you would do it for one outcome, build a function from there, and then iterate.

Suppose, we want to create a table that shows the proportion of people that trust in their government (`TrustGovernment`)^[Question: How often can you trust the federal government in Washington to do what is right? (Always, most of the time, about half the time, some of the time, or never / Never, some of the time, about half the time, most of the time, or always)?] as well as those that trust in people (`TrustPeople`)^[Question: Generally speaking, how often can you trust other people? (Always, most of the time, about half the time, some of the time, or never / Never, some of the time, about half the time, most of the time, or always)? ].

In the example below, we create a table that has the variable name as a column, the answer as a column, and then the percentage and its standard error.

```{r}
#| label: desc-map-1
anes_des %>%
  drop_na(TrustGovernment) %>%
  group_by(TrustGovernment) %>%
  summarise(p=survey_prop()*100) %>%
  mutate(Variable="TrustGovernment") %>%
  rename(Answer=TrustGovernment) %>%
  select(Variable, everything())
```

To turn this into a function, we need to use a bit of tidy evaluation which is a more advanced skill. If you want to learn more, we recommend @wickham2019advanced. 

```{r}
#| label: desc-map-2

calcps <- function(var){
  anes_des %>%
    drop_na(!!sym(var)) %>%
    group_by(!!sym(var)) %>%
    summarise(p=survey_prop()*100) %>%
    mutate(Variable=var) %>%
    rename(Answer:=!!sym(var)) %>%
    select(Variable, everything())
}
calcps("TrustGovernment")
calcps("TrustPeople")
```

Now, we can use map to iterate over as many variables as we want. It will output a data.frame with the variable name in the column "Variable", the responses in "Answer", the percentage and then the standard error. Finally, we can now use map to do this iteratively. This example extends nicely if you have many variables for which you want the percentage estimate.

```{r}
#| label: desc-map-3
c("TrustGovernment", "TrustPeople") %>% map(calcps) %>% list_rbind()
```


## Exercises

The first set of exercises should use the design object `anes_des` created in the book earlier. 

1. How many females have a graduate degree? Hint: the variables `Gender` and `Education` will be useful.

2. What percentage of people identify as "Strong Democrat"? Hint: The variable `PartyID` indicates what party people identify with.

3. What percentage of people who voted in the 2020 election identify as "Strong Republican"? Hint: The variable `VotedPres2020` indicates whether someone voted in 2020.

4. What percentage of people voted in both the 2016 election and in the 2020 election?  Include the logit confidence interval. Hint: The variable `VotedPres2016` indicates whether someone voted in 2016.

5. What is the design effect for the proportion of people who voted early? Hint: The variable `EarlyVote2020` indicates whether someone voted early in 2020.

6. What is the average temperature that people set their thermostats to at night during the winter? Hint: The variable `WinterTempNight` indicates the temperature that people set their temperature in the winter at night.

7. People do not always set their temperature the same over different seasons and during the day. What are the median temperatures that people set their thermostat to in the summer and winter both during the day and during the night? Include confidence intervals. Hint: Use the variables `WinterTempDay`, `WinterTempNight`, `SummerTempDay`, and `SummerTempNight`. 

8. What is the correlation between the temperature that people set their temperature at during the night and during the day in the summer?

9. What is the 1st, 2nd, and 3rd quartile of the amount of money spent on energy by Building America (BA) climate zone? Hint: `TOTALDOL` indicates the total amount spent on electricity and `ClimateRegion_BA` indicates the BA climate zones