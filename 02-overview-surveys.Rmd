# Overview of Surveys {#c02-overview-surveys}

<!-- author review -->

Creating and fielding surveys to provide estimates of a population are much more complex than simply adding a few questions to an online platform. Survey researchers can spend months, or even years, developing the study design, questions, and other methods for a single survey to ensure high-quality data is collected. While this book focuses on the analysis methods of surveys, understanding the entire survey process can provide a better insight into what types of analyses should be conducted on the data. To gain a deeper understanding of survey design and implementation, there are many pieces of existing literature that we recommend reviewing in detail (e.g., @dillman2014mode, @groves2009survey, education book?, Tourangeau book, Asking Questions book?, @valliant2013practical, and @biemer2003survqual).

The survey life cycle starts with a research topic or question of interest (e.g., what impact does childhood trauma have on health outcomes later in life). Researchers typically review existing data sources to determine if data are already available that can answer this question, as drawing from available resources can result in a reduced burden on respondents, cheaper research costs, and faster research outcomes. However, if existing data cannot answer the nuances of the research question, a survey can be used to capture the exact data that the researcher needs.

When starting a survey, there are multiple things to consider, including the study population, how the sample is selected, and how the question is worded. However, each step and decision can impact how the results can be interpreted, specifically related to the types of error that can be introduced into the data. Generally, survey researchers consider there to be seven main sources of error that fall into two major categories of Representation and Measurement (@groves2009survey):

- Representation
  - **Coverage Error**: A mismatch between the target population (also known as the population of interest) and the sample frame available
  - **Sampling Error**: Error produced when selecting a sample from the sample frame (none if conducting a census)
  - **Nonresponse Error**: Differences between those who responded and did not respond to the survey (unit nonresponse) or a given question (item nonresponse)
  - **Adjustment Error**: Error introduced during post-survey statistical adjustments
- Measurement
  - **Validity**: A mismatch between the topic of interest and the question(s) used to collect that information
  - **Measurement Error**: A mismatch between what the researcher asked and how the respondent answered
  - **Processing Error**: Edits by the researcher to responses provided by the respondent (e.g., adjustments to data based on illogical responses)

Almost every survey will have some errors, and researchers attempt to conduct a survey that reduces this *total survey error*. Weighting is one of the most common ways to reduce total survey error. However, weighting may result in increased error on its own (*adjustment error*). Therefore when designing surveys, researchers should carefully consider ways to reduce each error source and total survey error as a whole. Additionally, when analyzing survey data, understanding decisions that researchers took to minimize these error sources can impact how results are interpreted. The remainder of this chapter dives into considerations for survey development. We explore how to consider each of these sources of error, and how these error sources can inform the interpretations of the data.

## Study Design {#overview-design}

Study design often encompasses multiple parts of the survey planning process, including decisions on the target population, survey mode, and timeline. Various decisions made before the launch of the survey can assist researchers in reducing different error sources and are fundamental in understanding how to interpret the results of the data. Knowing who and how to survey individuals depends on the study's goals and the feasibility of implementation.

### Sampling Design {#overview-design-sampdesign}

Who we want to survey is known as the target population in the study. The target population could be broad, such as “all adults age 18+ living in the U.S.” or a specific target population based on a particular characteristic or location. For example, we may want to know about "adults age 18-24 who live in North Carolina" or "eligible voters living in Illinois." However, to survey individuals in these target populations, a *sampling frame* is needed with contact information. If researchers are looking at eligible voters, the sampling frame could be the voting registry for a given state or area. The sampling frame will most likely be imperfect for more broad target populations like all adults in the United States. In these cases, researchers may choose to use a sample frame of mailing addresses and send the survey to households, or they may choose to use random digit dialing (RDD) and call random phone numbers (that may or may not be assigned, connected, and working). These imperfect sampling frames can result in *coverage error* where there is a mismatch between the target population and the list of individuals researchers can select. For example, if a researcher is looking to obtain estimates for "all adults age 18+ living in the U.S.", using a sample frame from mailing addresses will be missing specific types of individuals, such as the homeless, transient populations, and incarcerated individuals. Additionally, many households have more than one adult living there, so researchers would need to consider how to get a specific individual to fill out the survey (called within household selection) or adjust the target population to report on "U.S. households" instead of "individuals."

Once the researchers have selected the sample frame, the next step is determining how to select individuals for the survey. In rare cases, researchers may wish to conduct a census and survey everyone on the sampling frame. However, the ability to implement a questionnaire at that scale is something only some can do (e.g., government censuses). Instead, researchers choose to sample individuals and use weights to estimate numbers in the target population. There are a variety of different sampling methods that can be used, and more information on these can be found in Chapter \@ref(c03-specifying-sample-designs). This decision of which sampling method to use impacts *sampling error* and can be accounted for in weighting.

#### Example: Number of Pets in a Household {.unnumbered #overview-design-sampdesign-ex}

Let's use a simple example where a researcher is interested in the average number of pets in a household. Our researcher will need to consider the target population for this study. Specifically, are they interested in all households in a given country or household in a more local area (e.g., city or state)? Let's assume our researcher is interested in the number of pets in a U.S. household with at least one adult (18 years old or older). In this case, using a sampling frame of mailing addresses would provide the least coverage error as the frame would closely match our target population. Specifically, our researcher would most likely want to use the Computerized Delivery Sequence File (CDSF), which is a file of mailing addresses that the United States Postal Service (USPS) creates and covers nearly 100% of U.S. households (@harter2016address). To sample these households, for simplicity, we will use a stratified simple random sample design, where we randomly sample households within each state (i.e., we stratify by state). 

Throughout this chapter, we will build on this example research question to plan a survey. 

### Data Collection Planning {#overview-design-dcplanning}

With the sampling design decided, researchers can then decide on how to survey these individuals. Specifically, the *modes* used for contacting and surveying the sample, how frequently to send reminders and follow-ups, and the overall timeline of the study are four of the major data collection determinations. Traditionally, researchers have considered four main modes^[Other modes such as using mobile apps or text messaging can also be considered, but at the time of publication, have smaller reach or are better for longitudinal studies (i.e., surveying the same individuals over many time periods of a single study). For the purposes of this overview we will focus on these four main modes.] for conducting surveys:

- Computer Assisted Personal Interview (CAPI; also known as face-to-face or in-person interviewing)
- Computer Assisted Telephone Interview (CATI; also known as phone or telephone interviewing)
- Computer Assisted Web Interview (CAWI; also known as web or on-line interviewing)
- Paper and Pencil Interview (PAPI)

Researchers can use a single mode to collect data or multiple modes (also called mixed modes). Using mixed modes can allow for broader reach and increase response rates depending on the target population (@DeLeeuw_2018, @biemer_choiceplus). For example, researchers could both call households to conduct a CATI survey and send mail with a PAPI survey to the household. Using both of these modes, researchers could gain participation through the mail from individuals who do not pick up the phone to unknown numbers or through the phone from individuals who do not open all of their mail. However, mode effects (where responses differ based on the mode of response) can be present in the data and may need to be considered during analysis.

When selecting which mode, or modes, to use, understanding the unique aspects of the chosen target population and sampling frame will provide insight into how they can best be reached and engaged. For example, if we plan to survey adults aged 18-24 who live in North Carolina, asking them to complete a survey using CATI (i.e., over the phone) would most likely not be as successful as other modes like the web. This age group does not talk on the phone as much as other generations, and often do not answer their phones for unknown numbers. Additionally, the mode for contacting respondents relies on what information is available on the sample frame. For example, if our sampling frame includes an email address, we could email our selected sample members to convince them to complete a survey. Or if the sampling frame is a list of mailing addresses, researchers would have to contact sample members with a letter. 

It is important to note that there can be a difference between the contact and survey modes. For example, if we have a sample frame with addresses, we can send a letter to our sample members and provide information on how to complete a web survey. Or we could use mixed-mode surveys and send sample members a paper and pencil survey with our letter and also ask them to complete the survey online. Combining different contact modes and different survey modes can be useful in reducing *unit nonresponse error*--where the entire unit (e.g., a household) does not respond to the survey at all--as different sample members may respond better to different contact and survey modes. However, when considering which modes to use, it is important to make access to the survey as easy as possible for sample members to reduce burden and unit nonresponse.

Another way to reduce unit nonresponse error is through varying the language of the contact materials (@dillman2014mode). People are motivated by different things, so constantly repeating the same message may not be helpful. Instead, mixing up the messaging and the type of contact material the sample member receives can increase response rates and reduce the unit nonresponse error. For example, instead of only sending standard letters, researchers could consider sending mailings that invoke "urgent" or "important" thoughts by sending priority letters or using other delivery services like FedEx, UPS, or DHL.

A study timeline may also determine the number and types of contacts. If the timeline is long, then there is a lot of time for follow-ups and varying the message in contact materials. If the timeline is short, then fewer follow-ups can be implemented. Many studies will start with the tailored design method put forth by {Dillman et al. (2014)} and implement 5 contacts:

* Prenotice letting sample members know the survey is coming
* Invitation to complete the survey
* Reminder that also thanks respondents that may have already completed the survey
* Reminder (with a replacement paper survey if needed)
* Final reminder

This method is easily adaptable based on the study timeline and needs but provides a sound starting point for most studies.

#### Example: Number of Pets in a Household {.unnumbered #overview-design-dcplanning-ex}

Let's return to our example of a researcher who wants to know the average number of pets in a household. We are using a sample frame of mailing addresses, so we recommend starting our data collection with letters mailed to households, but later in data collection, we want to send interviewers to the house to conduct an in-person (or CAPI) interview to decrease unit nonresponse error. This means we will have two contact modes (paper and in-person). As mentioned above, the survey mode does not have to be the same as the contact mode, so we recommend a mixed-mode study with both Web and CAPI modes. Let's assume we have six months for data collection, so we may want to recommend the following protocol:

Table: Protocol Example for 6-month Web and CAPI Data Collection 

Week | Contact Mode     | Contact Message     | Survey Mode Offered
:----: | ------------------------------ | ------------------------------- | ------------------- 
  1 | Mail: Letter     | Prenotice      | ---
  2 | Mail: Letter     | Invitation      | Web
  3 | Mail: Postcard     | Thank You/Reminder    | Web
  6 | Mail: Letter in large envelope | Animal Welfare Discussion  | Web
 10 | Mail: Postcard     | Inform Upcoming In-Person Visit | Web
 14 | In-Person Visit    | ---        | CAPI
 16 | Mail: Letter     | Reminder of In-Person Visit  | Web, but includes a number to call to schedule CAPI
 20 | In-Person Visit    | ---        | CAPI
 25 | Mail: Letter in large envelope | Survey Closing Notice   | Web, but includes a number to call to schedule CAPI

This is just one possible protocol that we can use that starts respondents with web (typically done to reduce costs). However, researchers may want to begin in-person data collection earlier during the data collection period or ask their interviewers to attempt more than two visits with a household.

### Questionnaire Design {#overview-design-questionnaire}

When developing the questionnaire, it can be helpful to first outline the topics to be asked and include the "why" each question or topic is important to the research question(s). This can help researchers better tailor the questionnaire and potentially reduce the number of questions (and thus the burden on the respondent) if topics are deemed irrelevant to the research question. When making these decisions, researchers should also consider questions needed for weighting. While we would love to have everyone sampled answer our survey, this is rarely the case. Thus, including questions about demographics in the survey can assist with weighting for *nonresponse errors* (both unit and item nonresponse). Knowing the details of the sampling plan and what may impact *coverage error* and *sampling error* can help researchers determine what types of demographics to include.

Researchers can benefit from the work of others by using questions from other surveys. Demographic questions such as race, ethnicity, or education often use questions from a government census or other official surveys. Other survey questions can be found using question banks which are a compilation of questions that have been asked across various surveys such as the [Inter-university Consortium for Political and Social Research (ICPSR) variable search](https://www.icpsr.umich.edu/web/pages/ICPSR/ssvd/).

If a question does not exist in a question bank, researchers can craft their own. When creating their own questions, researchers should start with the research question or topic and attempt to write questions that match the concept. The closer the question asked is to the overall concept, the better *validity* there is. For example, if the researcher wants to know how people consume TV series and movies but only asks a question about how many TVs are in the house, then they would be missing other ways that people watch TV series and movies, such as on other devices or at places outside of the home.

Additionally, when designing questions, researchers should consider the mode for the survey and adjust language appropriately. In self-administered surveys (e.g., web or mail), respondents can see all the questions and response options, but that is not the case in interviewer-administered surveys (e.g., CATI or CAPI). With interviewer-administered surveys, the response options need to be read aloud to the respondents, so the question may need to be adjusted to allow a better flow to the interview. Additionally, with self-administered surveys, because the respondents are viewing the questionnaire, the formatting of the questions is even more important to ensure accurate measurement. Incorrect formatting or wording can result in *measurement error*, so following best practices or using existing validated questions can reduce error. There are multiple resources to help researchers draft questions for different modes (e.g., {CITE Dillman, fowler, ed book?, asking questions, tourangeau formatting article?}).


#### Example: Number of Pets in a Household {.unnumbered #overview-design-questionnaire-ex}

As part of our survey on the average number of pets in a household, researchers may want to know what animal most people prefer to have as a pet. Let's say we have the following question in our survey:

<!-- Put this in as an image instead of as a block quote -->

> What animal do you prefer to have as a pet?
>
> <ul class="ro">
>
> <li class="ro">
>
> Dogs
>
> </li>
>
> <li class="ro">
>
> Cats
>
> </li>
>
> </ul>

This question may have validity issues as it only provides the options of "dogs" and "cats" to respondents, and interpretation of the data could be incorrect. For example, if we had 100 respondents who answered the question and 50 selected dogs, then the results of this question cannot be `50% of the population prefers to have a dog as a pet` as only two response options were provided. If a respondent taking our survey prefers turtles, they could either be forced to choose a response between these two (i.e., interpret the question as "between dogs and cats, which do you prefer?" and result in *measurement error*), or they may not answer the question (which results in *item nonresponse error*). Based on this, the interpretation of this question should be `When given a choice between dogs and cats, 50% of respondents preferred to have a dog as a pet`. 

To avoid this issue, researchers should consider these possibilities and adjust the question accordingly. One simple way could be to add an "other" response option to give respondents a chance to provide a different response. The "other" response option could then include a way for respondents to write in what their other preference is. For example, this question could be rewritten as

<!-- Put this in as an image instead of as a block quote, or need to add in an open-ended box for write in after the last option. -->

> What animal do you prefer to have as a pet?
>
> <ul class="ro">
>
> <li class="ro">
>
> Dogs
>
> </li>
>
> <li class="ro">
>
> Cats
>
> </li>
>
> <li class="ro">
>
> Other, please specify:
>
> </li>
>
> </ul>

Researchers can then code the responses from the open-ended box and get a better understanding of the respondent's choice of preferred pet. Interpreting this question becomes easier as researchers no longer need to qualify the results with the choices provided.

This is a very simple example of how the presentation of the question and options can impact the findings. More complex topics and questions will need researchers to thoroughly consider how to mitigate any impacts from the presentation, formatting, wording, and other aspects. As survey analysts, reviewing not only the data but also the wording of the questions is crucial to ensure the results are presented in a manner consistent with the question asked. Chapter @\ref(c04-understanding-survey-data-documentation) provides further details on how to review existing survey documentation to inform our analyses.

## Data Collection {#overview-datacollection}

Once the data collection starts, researchers try to stick to the data collection protocol designed during pre-survey planning. However, a good researcher will adjust their plans and adapt as needed to the current progress of data collection ({cite Peytchev book on adaptive survey design}). Some extreme examples could be natural disasters that could prevent mail or interviewers from getting to the sample members. Others could be smaller in that something newsworthy occurs that is connected to the survey, so researchers could choose to play this up in communication materials. In addition to these external factors, there could be factors unique to the survey, such as lower response rates for a specific sub-group, so the data collection protocol may need to find ways to improve response rates for that specific group.

## Post-Survey Processing {#overview-post}

After data collection, a variety of activities need to be conducted before we can analyze the survey. Multiple decisions made during this post-survey phase can assist researchers in reducing different error sources, such as through weighting to account for the sample selection. Knowing the decisions researchers made in creating the final analytic data can impact how analysts use the data and interpret the results.

### Data Cleaning and Imputation {#overview-post-cleaning}

Post-survey cleaning and imputation is one of the first steps researchers will do to get the survey responses into a dataset for use by analysts. Data cleaning can comprise of cleaning inconsistent data (e.g., with skip pattern errors or multiple questions throughout the survey being consistent with each other), editing numeric entries or open-ended responses for grammar and consistency, or recoding open-ended questions into categories for analysis. Each project should create their own rules for handling different cleaning situations, and no specific rules must be followed. Instead, researchers should use their best judgment to ensure data integrity remains, and all decisions should be documented and available to those using the data in the analysis. Each decision a researcher makes impacts *processing error*, so often researchers will have multiple people review these rules or recode open-ended data and adjudicate any differences in an attempt to reduce this error. 

Another crucial step in post-survey processing is imputation. Often, there is item nonresponse where respondents do not answer specific questions. If the questions are crucial to analysis efforts or the research question, researchers may implement imputation in an effort to reduce *item nonresponse error*. However, as imputation is a way of assigning a value to missing data based on an algorithm or model, it can also introduce *processing error*, so researchers should consider the overall implications of imputing data compared to having item nonresponse. There are multiple ways imputation can be conducted. We recommend reviewing other resources like {cite Kim & Shao, Statistical Methods for Handling Incomplete Data} for more information.

#### Example: Number of Pets in a Household {.unnumbered #overview-post-cleaning-ex}

Let's return to the question we created to ask about [animal preference](#overview-design-questionnaire-ex). The "other specify" invites respondents to specify the type of animal they prefer to have as a pet. If respondents entered answers such as "puppy," "turtle," "rabit," "rabbit," "bunny," "ant farm," "snake," "Mr. Purr," then researchers may wish to categorize these write-in responses to help with analysis. In this example, "puppy" could be assumed to be a reference to a `Dog`, and could be recoded there. The misspelling of "rabit" could be coded along with "rabbit" and "bunny" into a single category of `Bunny or Rabbit`. These are relatively standard decisions that a researcher could make. The remaining write-in responses could be categorized in a few different ways. "Mr. Purr," which may be someone's reference to their own cat, could be recoded as `Cat`, or it could remain as `Other` or some category that is `Unknown`. Depending on the number of responses related to each of the others, they could all be combined into a single `Other` category, or maybe categories such as `Reptiles` or `Insects` could be created. Each of these decisions may impact the interpretation of the data, so our researcher should document the types of responses that fall into each of the new categories and any decisions made.

### Weighting {#overview-post-weighting}

Weighting can typically be used to address some of the error sources identified in the previous sections. For example, weights may be used to address coverage, sampling, and nonresponse errors. Many published surveys will include an "analysis weight" variable that combines these adjustments. However, weighting itself can also introduce *adjustment error*, so researchers need to balance which types of errors should be corrected with weighting. The construction of weights is outside the scope of this book, and researchers should reference other materials if interested in constructing their own (@valliant2013practical). Instead, this book assumes the survey has been completed, weights are constructed, and data is made available for users. We will walk users through how to read the documentation (Chapter \@ref(c04-understanding-survey-data-documentation)) and work with the data and analysis weights provided to analyze and interpret survey results correctly. 

#### Example: Number of Pets in a Household {.unnumbered #overview-post-weighting-ex}

In the simple example of our survey, we decided to use a stratified sample by state to select our sample members. Knowing this sampling design, our researcher can include selection weights for analysis that account for how the sample members were selected for the survey. Additionally, the sampling frame may have the type of building associated with each address, so we could include the building type as a potential nonresponse weighting variable, along with some interviewer observations that may be related to our research topic of the average number of pets in a household. Combining these weights, we can create an analytic weight that researchers will need to use when analyzing the data.

### Disclosure {#overview-post-disclosure}

Before data is made publicly available, researchers will need to ensure that individual respondents can not be identified by the data when confidentiality is required. There are a variety of different methods that can be used, including data swapping, top or bottom coding, coarsening, and perturbation. In data swapping, researchers may swap specific data values across different respondents so that it does not impact insights from the data but ensures that specific individuals cannot be identified. For extreme values, top and bottom coding is sometimes used. For example, researchers may top-code income values such that households with income greater than \$99,999,999 are coded into a single category of \$99,999,999 or more. Other disclosure methods may include aggregating response categories or location information to avoid having only a few respondents in a given group and thus be identified. For example, researchers may use coarsening to display income in categories instead of as a continuous variable. Data producers may also perturb the data by adding random noise. There is as much art as there is a science to the methods used for disclosure, and in documentation, researchers should only provide high-level comments that disclosure was conducted and not specific details to ensure nobody can reverse the disclosure and thus identify individuals. For more information on different disclosure methods, please see {CITE Skinner chapter in this book: https://www.sciencedirect.com/science/article/abs/pii/S0169716108000151
AAPOR checklist/standards: https://www-archive.aapor.org/Standards-Ethics/AAPOR-Code-of-Ethics/Survey-Disclosure-Checklist.aspx}.

## This book

After all of these steps are taken, the data is ready for use by analysts. The rest of this book works from this point. If you're interested in learning more about the steps talked about here, we recommend you look into the references cited throughout this chapter. 
