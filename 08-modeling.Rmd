# Modeling {#c08}

<!-- TODO: Add an intro of what modeling is (in the context of survey data) -->

Modeling survey data cannot be directly done in {srvyr} but can be done in the {survey}[@lumley2010complex; @R-survey] package as was true in Chapter \@ref(c07). In this chapter, we will first review concepts of modeling in R and how to specify formulas. Then, we will introduce linear modeling including ANOVA and linear regression, logistic regression, nonlinear models, and  survival analysis.

Accounting for the survey design and weights in modeling can generalize a model to the target population. When using the survey design objects, the statistical tests account for the fact that the observations may not be independent. There is some debate as to whether weights should be used in regression [@gelman2007weights; @bollen2016weightsreg].

In all models in R, you need to specify the structure of the model using a formula. The left side of the formula is the response/dependent variable and the right side of the formula has the predictor/independent variable(s). There are many symbols used in R to specify the formula. 

For example, a linear formula mathematically specified as 

$$Y_i=\beta_0+\beta_1 X_i+\epsilon_i$$ 
would be specified in R as `y~x` where the intercept is not explicitly included. To fit a model with no intercept, that is, 

$$Y_i=\beta_1 X_i+\epsilon_i$$ 

it can be specified as `y~x-1`. Formula notation details in R can be found in the help file for formula^[Use `help(formula)` in R or find the documentation online at https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html]. A quick overview of the common formula notation is in the following table:

Symbol | Example | Meaning
-|-|-
+|`+X`|include this variable
-|`-X`|delete this variable
:|`X:Z`|include the interaction between these variables
* |`X*Z`|include these variables and the interactions between them
^n |`(X+Z+Y)^3`|include these variables and all interactions up to n way
I |`I(X-Z)`| as-as: include a new variable which is the difference of these variables

There are often multiple ways to specify the same formula. For example, consider the following equation using the mtcars data

$$mpg_i=\beta_0+\beta_1cyl_{i}+\beta_2disp_{i}+\beta_3hp_{i}+\beta_4cyl_{i}disp_{i}+\beta_5cyl_{i}hp_{i}+\beta_6disp_{i}hp_{i}+\epsilon_i$$

This could be specified as any of the following:

- `mpg~(cyl+disp+hp)^2`
- `mpg~cyl+disp+hp+cyl:disp+cyl:hp+disp:hp` 
- `mpg~cyl*disp+cyl*hp+disp*hp`

Note that the following two specifications are not the same:

- `mpg~cyl:disp:hp` this only has the interactions and not the main effect
- `mpg~cyl*disp*hp` this also has the 3-way interaction in addition to the main effects and 2-way interactions


## Chapter Set-Up

For this chapter, we will be using the same data as we did in \@ref(c06): ANES and RECS. As a reminder, we will need to create survey design objects to work with. These design objects ensure that the variance estimation is calculated accurately, and thus we can accurately determine statistical significance.

First, make sure you have installed and loaded the following packages:

```{r c07_setup}
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(survey) # for survey analysis
library(srvyr) # for tidy survey analysis
library(here)
library(broom) # for tidy regression output
```

Second, we need to read in the data and create the design objects.

Here is how you create the design object for the ANES data, remember that we need to adjust the weight so it sums to the population instead of the sample. We do that by multiplying the weights (see the ANES methodology documentation for more information) as well as Chapter "understanding survey documentation" (TODO: add link).
```{r anes_des_c07}
anes <- read_rds(here("AnalysisData", "anes_2020.rds")) %>%
   mutate(Weight=Weight/sum(Weight)*231592693) 
anes_des <- anes %>%
   as_survey_design(weights = Weight,
                    strata = Stratum,
                    ids = VarUnit,
                    nest = TRUE)
```

Here is how you create the design object for the RECS data. More details for this are included in the RECS documentation and Chapter "unerstanding survey documentation" (TODO: add link)

```{r recs_des_c07}
recs <-read_rds(here("AnalysisData", "recs_2015.rds"))
recs_des <- recs %>%
  as_survey_rep(weights = NWEIGHT,
                repweights = starts_with("BRRWT"),
                type = "Fay",
                rho = 0.5,
                mse = TRUE)
```


## Linear modeling

In this section, we will discuss linear models. Many common models and tests are linear models including analysis of variance (ANOVA) and linear regression. Jonas Kristoffer LindelÃ¸v has an interesting [discussion](https://lindeloev.github.io/tests-as-linear/) of many statistical tests and models being equivalent to a linear model. As an example, a one-way ANOVA is a linear model with one categorical independent variable and a two-sample t-test is an ANOVA where the independent variable has exactly 2 levels.


### ANOVA

In ANOVA, we are testing whether the mean of an outcome is the same across two or more groups. Statistically, we set up this as:

- $H_0: \mu_1 = \mu_2= \dots = \mu_k$ where $\mu_i$ is the is the mean outcome for group $i$
- $H_A: \text{At least one mean is different}$

Some assumptions when using ANOVA on survey data include:

- Outcome variable is normally distributed within each group
- The variances of the outcome variable between each group are approximately equal
- We do NOT assume independence between the groups as with general ANOVA. The covariance is accounted for in the survey design

To perform this type of test in R, the general syntax is as follows:

```
des_obj %>%
  svyglm(design=.,
         outcomevar~groupvar,
         na.action=na.omit,
         df.resid=degf(.))
```

where `des_obj` is a design object, `outcomevar` is the outcome variable, `groupvar` is the group variable, and `na.action=na.omit` is set so that records with missing data in the outcome or group variable are removed for prediction ^[See `help(na.omit)` for more information on options to use for `na.action`]. The function `svyglm` does not have the design as the first argument so the `.` notation is used to pass it with a pipe.

Looking at an example will help us discuss the output and how to interpret the results. In RECS, respondents are asked what temperature they set their thermostat to during the day and evening when using the air-conditioning during the summer which is encoded in the variable `SummerTempNight`. A descriptive analysis is displayed below of the temperature set by region and the sample sizes. 

```{r}
#| label: anova-prep
recs_des %>%
  filter(ACUsed) %>%
  group_by(Region) %>%
  summarise(
    SMN=survey_mean(SummerTempNight, na.rm=TRUE),
    n=unweighted(n()),
    n_na=unweighted(sum(is.na(SummerTempNight)))
  )
```

In the following code, we test whether this temperature varies by region by first using `svyglm` to run the test and then using `broom::tidy` to display the output. Note that the temperature setting is set to NA when the household does not use air-conditioning and thus `na.action=na.omit` is specified to ignore these cases.

```{r}
#| label: anova-ex
anova_out <- recs_des %>%
   svyglm(design=.,
          formula=SummerTempNight~Region,
          na.action=na.omit)

tidy(anova_out)
```

In the output above, we can see the estimated coefficients (`estimate`), estimated standard errors of the coefficients (`std.error`), the t-statistic (`statistic`), and the p-value for each coefficient. In this output, the intercept represents the reference value which is the Northeast region. The other coefficients indicate the difference in temperature relative to the Northeast region. For example, in the Midwest, temperatures are set, on average, `r tidy(anova_out) %>% filter(term=="RegionMidwest") %>% pull(estimate) %>% round(3)` degrees higher than in the Northeast during summer nights.

### Linear regression

Linear regression is a more generalized method than ANOVA. In linear regression, we are fitting a model of a continuous outcome with any number of predictors which are categorical or continuous. We form these models as follows:

$$y_i=\beta_0 +\sum_{i=1}^p \beta_i x_i + \epsilon_i$$

where $y_i$ is the outcome, $\beta_0$ is an intercept, $x_1, \cdots, x_n$ are the predictors with $\beta_1, \cdots, \beta_p$ as the associated coefficients, and $\epsilon_i$ is the error. 

Assumptions in linear regression using survey data include:

- The residuals ($\epsilon_i$) are normally distributed but there is not an assumption of independence and the correlation structure is captured in the survey design object. 
- There is a linear relationship between the outcome variable and the independent variables
- The residuals are homoscedastic, that is the error term is the same across all values of independent variables

The syntax for linear regression is nearly identical to ANOVA as follows:

```
des_obj %>%
  svyglm(design=.,
         outcomevar~x1+x2+x3,
         na.action=na.omit,
         df.resid=degf(.))
```

As discussed in the beginning of the chapter, the formula on the right-hand side can be specified many ways whether interactions are desired or not, for example. On RECS, information on the square footage of homes and the electric bills are obtained. We assume that square footage is related to the amount of money spent on electricity and examine a model for this. Before any modeling, we first plot the data to examine whether it is reasonable to assume a linear relationship. In the plot below, each hexagon represents the weighted count of households in the bin.

```{r}
#| label: plot-sf-elbill
#| fig.cap: "Relationship between square footage and dollars spent on electricity, RECS 2015"
#| echo: FALSE
#| warning: FALSE

recs %>%
  ggplot(aes(x=TOTSQFT_EN, y=DOLLAREL, weight=NWEIGHT/1000000)) +
  geom_hex() + 
  scale_fill_gradientn(guide="colourbar",name="Housing Units\n(Millions)", labels=scales::comma, 
                       colours=c("#0B3954", "#087E8B", "#BFD7EA", "#FF8484", "#8D6B94")) +
  xlab("Total square footage") + ylab("Amount spent on electricity") +
  scale_y_continuous(labels=scales::dollar_format()) +
  scale_x_continuous(labels=scales::comma_format()) + 
  theme_bw() 
```

The model is fit below with electricity expenditure as the outcome. Some textbooks use the term simple linear regression when a linear model has one continuous independent variable.

```{r}
#| label: slr-examp
m_electric_sqft <- recs_des %>%
   svyglm(design=.,
          formula=DOLLAREL~TOTSQFT_EN,
          na.action=na.omit)
tidy(m_electric_sqft)
```

In the output above, we can see the estimated coefficients (`estimate`), estimated standard errors of the coefficients (`std.error`), the t-statistic (`statistic`), and the p-value for each coefficient. In these results, we can say that, on average, for every additional square foot of house size, the electricity bill increases by `r (tidy(m_electric_sqft) %>% filter(term=="TOTSQFT_EN") %>% pull(estimate) %>% round(3))*100` cents and that square footage is significantly associated with electricity expenditure. This is a very simple model and there are likely many more factors in electricity expenditure including the type of cooling, number of appliances, location, and more. 


In the following example, a model is fit predicting electricity expenditure including Census region (factor/categorical), urbanicity (factor/categorical), square footage (double/numeric), and whether air-conditioning is used (logical/categorical) with all two-way interactions also included.

```{r}
#| label: lmr-examp

m_electric_multi <- recs_des %>%
   svyglm(design=.,
          formula=DOLLAREL~(Region+Urbanicity+TOTSQFT_EN+ACUsed)^2-1,
          na.action=na.omit)
tidy(m_electric_multi)
```

To examine the predictions, residuals and more from the model, the function `augment` from {broom}. The `augment` function will return a tibble with the independent and dependent variables and other fit statistics. The fitted value and standard error are contained in one column and need to be split after using augment as illustrated below.

These results can then be used in a variety of ways including examining residual plots as is illustrated below:


```{r}
#| label: aug-examp
#| fig.cap: "Residual plot"

fitstats <-
  augment(m_electric_multi) %>%
  mutate(
    .se.fit=sqrt(attr(.fitted, "var")), # extract the variance of the fitted value
    .fitted=as.numeric(.fitted) 
    )

fitstats

fitstats %>%
  ggplot(aes(x=.fitted, .resid)) +
  geom_point() +
  geom_hline(yintercept =0, colour="red") +
  theme_bw()

```

## Logistic regression

Logistic regression is used to model a binary outcome and is a specific type of a generalized linear model (GLM). A GLM uses a link function to link the response variable to the linear model. In logistic regression, the link model is the logit function. Specifically, the model is specified as follows:

$$ y_i \sim \text{Bernoulli}(\pi_i)$$

\begin{equation}
\log \left(\frac{\pi_i}{1-\pi_i} \right)=\beta_0 +\sum_{i=1}^p \beta_i x_i
(\#eq:logoddlin)
\end{equation}

which can be re-expressed as

$$ \pi_i=\frac{\exp \left(\beta_0 +\sum_{i=1}^p \beta_i x_i \right)}{1+\exp \left(\beta_0 +\sum_{i=1}^p \beta_i x_i \right)}.$$
where $y_i$ is the outcome, $\beta_0$ is an intercept, and $x_1, \cdots, x_n$ are the predictors with $\beta_1, \cdots, \beta_n$ as the associated coefficients. 

Assumptions in logistic regression using survey data include:

- The outcome variable has 2 levels
- There is a linear relationship between the independent variables and the log odds \@ref(eq:logoddlin)
- The residuals are homoscedastic, that is the error term is the same across all values of independent variables

The syntax for logistic regression is as follows:

```
des_obj %>%
  svyglm(design=.,
         outcomevar~x1+x2+x3,
         na.action=na.omit,
         df.resid=degf(.),
         family= quasibinomial, # use this to avoid warning about non-integers
         )
```

Note this is the same function as used in both ANOVA and linear regression. However, we've added the link function quasibinomial. While you can use the binomial link function, it is recommended to use the quasibinomial as your weights may not be integers and the quasibinomial also allows for overdispersion. The quasibinomial family has a default logit link which is what is specified in the equations above. When specifying the outcome variable, it will likely be specified in one of two ways with survey data:

- A factor variable where not being the first level of the factor indicates a "success"
- A numeric variable which is 1 or 0 where 1 indicates a success
- A logical variable where TRUE indicates a success

In the following example, the ANES data is used and we are modeling whether someone usually has trust in the government^[Question: How often can you trust the federal government in Washington to do what is right?] by who someone voted in 2020. As a reminder, the main candidates were Biden and Trump though people could vote for someone else in neither the Democratic or Republican parties. Those votes are all grouped into an "Other" category. We first create a binary outcome for trusting in the government and plot the data. A scatter plot of the raw data is not useful as it is all 0 and 1 outcomes so instead a summary of the data is plotted.

```{r}
#| label: logisticexamp-plot
#| fig.cap: "Relationship between candidate selection and trust in government, ANES 2020"
anes_des_der <- anes_des %>%
  mutate(TrustGovernmentUsually=case_when(
    is.na(TrustGovernment)~NA,
    TRUE~TrustGovernment %in% c("Always", "Most of the time")))

anes_des_der %>%
  group_by(VotedPres2020_selection) %>%
  summarise(
    pct_trust=survey_mean(TrustGovernmentUsually, na.rm=TRUE, proportion=TRUE, vartype="ci"),
    .groups="drop"
  ) %>%
  filter(complete.cases(.)) %>%
  ggplot(aes(x=VotedPres2020_selection  , y=pct_trust)) +
  geom_bar(stat="identity") +
  geom_errorbar(aes(ymin=pct_trust_low, ymax=pct_trust_upp), width=.2) +
  xlab("Election choice (2022)") +
  ylab("Usually trust the government")+
  scale_y_continuous(labels=scales::percent)
```


Next, we fit the model. 

```{r}
#| label: logisticexamp-model
logistic_trust_vote <- anes_des_der %>%
  svyglm(design=.,
         formula=TrustGovernmentUsually~ VotedPres2020_selection ,
         family = quasibinomial) 

tidy(logistic_trust_vote)
tidy(logistic_trust_vote, exponentiate = TRUE) %>% select(term, estimate)

```


```{r}
#| label: logisticcalc
#| echo: false
or_trump <- tidy(logistic_trust_vote, exponentiate = TRUE) %>% filter(str_detect(term, "Trump")) %>% pull(estimate)
or_other <- tidy(logistic_trust_vote, exponentiate = TRUE) %>% filter(str_detect(term, "Other")) %>% pull(estimate)

```


In the output above, we can see the estimated coefficients (`estimate`), estimated standard errors of the coefficients (`std.error`), the t-statistic (`statistic`), and the p-value for each coefficient when the `tidy` function is run the first time. The second time the `tidy` function is used, the coefficients are exponentiated which illustrates the odds. In this example, we can interpret this as saying that the odds of trusting in government for someone who voted for Trump is `r signif(or_trump*100, 3)`% as likely to trust the government compared to a person who voted for Biden (the reference level) while a person who voted for neither Biden nor Trump is `r signif(or_other*100, 3)`% as likely to trust the government as someone who voted for Biden.

