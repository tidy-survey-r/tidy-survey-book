--- 
title: "Tidy Survey Book"
author: "Stephanie Zimmer, Rebecca J. Powell, and Isabella Vel√°squez"
date: "`r Sys.Date()`"
documentclass: krantz
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
colorlinks: true
lot: true
lof: true
site: bookdown::bookdown_site
description: "Bookdown for upcoming book on survey analysis the tidy way."
github-repo: tidy-survey-r/tidy-survey-book
graphics: yes
#cover-image: images/cover.jpg
header-includes:
   - \usepackage{draftwatermark}
---

\SetWatermarkText{DRAFT}


```{r setup}
#| include: false
options(
  htmltools.dir.version = FALSE, formatR.indent = 2, width = 55, digits = 4
)

library(tidyverse)
library(survey)
library(srvyr)
library(tidycensus)
library(DiagrammeR)
library(markdown)
library(webshot)
library(here)


```

# Preface {-}

Hi there, this is my great book.

## Why read this book {-}

It is very important...

## Structure of the book {-}

Chapters \@ref(c01) introduces a new topic, and ...

## Software information and conventions {-}

I used the **knitr**\index{knitr} package [@xie2015] and the **bookdown**\index{bookdown} package [@R-bookdown] to compile my book. My R session information is shown below:

```{r}
xfun::session_info()
```

Package names are in bold text (e.g., **rmarkdown**), and inline code and filenames are formatted in a typewriter font (e.g., `knitr::knit('foo.Rmd')`). Function names are followed by parentheses (e.g., `bookdown::render_book()`).

## Acknowledgments {-}

A lot of people helped me when I was writing the book.

```{block2, type='flushright', html.tag='p'}
Frida Gomam  
on the Mars
```


<!--chapter:end:index.Rmd-->

\mainmatter

# Introduction {#c01}

```{r}
#| label: globaloptions
#| include: FALSE
options(width=72)
```


<!--chapter:end:01-introduction.Rmd-->

# Overview of Surveys {#c02}

Creating and fielding surveys to provide estimates of a population are much more complex than simply adding a few questions to an online platform. Survey researchers can spend months, or even years, developing the study design, questions, and other methods for a single survey to ensure high quality data is collected. While this book focuses on the analysis methods of surveys, understanding the entire survey process can provide a better insight into what types of analyses should be conducted on the data. To gain a deeper understanding of survey design and implementation, there are many pieces of existing literature that we recommend reviewing in detail (e.g., {cite: Dillman book, Groves book, education book?, Tourangeau book, Asking Questions book?, Valliant, Dever, and Krauter book (for design)
Biemer and Lyberg: Introduction to Survey Quality}).

The survey life cycle starts with a research topic or question of interest (e.g., what impact does childhood trauma have on health outcomes later in life). Researchers typically review existing data sources to determine if data are already available that can answer this question, as this can result in reduced burden on respondents, cheaper research costs, and faster research outcomes. However, if existing data cannot answer the nuances of the research question, a survey can be used to capture the exact data that the researcher needs.

When starting a survey, there are multiple things to consider including the study population, how the sample is selected, and the way the question is worded. However, each step and decision can impact how the results can be interpreted, specifically related to the types of error that can be introduced into the data. Generally, survey researchers consider there to be 7 main sources of error ({Cite Groves book}):

-  Representation
    -  **Coverage Error**: A mismatch between the target population (also known as the population of interest) and the sample frame available
    -  **Sampling Error**: Error produced when selecting a sample from the sample frame (none if conducting a census)
    -  **Nonresponse Error**: Differences between those who responded and did not respond to the survey (unit nonresponse) or to a given question (item nonresponse)
    -  **Adjustment Error**: Error introduced during post-survey statistical adjustments
-  Measurement
    -  **Validity**: A mismatch between the topic of interest and the question used to collect that information
    -  **Measurement Error**: A mismatch between what the researcher asked and how the respondent answered
    -  **Processing Error**: Edits by the researcher to responses provided by the respondent (e.g., adjusts data based on illogical responses)

Almost every survey conducted will have some of each of these types of error, and researchers attempt to conduct a survey that reduces this *total survey error*. One of the most common ways to reduce total survey error is through weighting. However, weighting may result in increased error on its own (*adjustment error*). Therefore when designing surveys, researchers should carefully think about ways to reduce each of these error sources. Additionally when analyzing survey data, understanding decisions that researchers took to minimize these error sources can impact how results are interpreted. The remainder of this chapter dives into considerations for survey development, places where each of these sources of error can be considered, and how these error sources can inform the interpretations of the data.


## Study Design {#overview-design}

Study design is often used to encompass multiple parts of the survey planning process including decisions on target population, survey mode, and timeline. Multiple decisions made prior to the launch of the survey can assist researchers in reducing different error sources, and are fundamental in understanding how to interpret the results of the data. Knowing who and how to survey individuals depends on both the goals of the study and the feasibility of implementation.

### Sampling Design {#overview-design-sampdesign}

Who we want to survey is known as the *target population* in the study. This could be broad, such as, "all adults age 18+ living in the U.S.", or it could be a very specific target population based on a specific characteristic or location. For example, we may want to know about "adults age 18-24 who live in North Carolina" or "eligible voters living in Illinois". However, to survey individuals in these target populations, a *sampling frame* is needed with contact information. If researchers are looking at eligible voters, the sampling frame could be the voting registry for a given state or area. For more broad target populations like all adults living in the U.S., the sampling frame will most likely be imperfect. In these cases, researchers may choose to use a sample frame of mailing addresses and send the survey to households, or they may choose to use random digit dialing (RDD) and call random phone numbers. These imperfect sampling frames can result in *coverage error* where there is a mismatch between the target population and the list of individuals researchers can select. For example, if a researcher is looking to obtain estimates for "all adults age 18+ living in the U.S.", using a sample frame from mailing addresses will be missing specific types of individuals such as the homeless or transient populations. Additionally, many households have more than one adult living there, so researchers would need to consider how to get a specific individual to fill out the survey (called within household selection), or adjust the target population to report on "U.S. households" instead of "individuals".

Once the researchers have selected the sample frame, the next step is figuring out how to select individuals for the survey. In rare cases researchers may which to conduct a census and survey everyone on the sampling frame. However, the ability to implement a questionnaire at that scale is something few can do. Instead, researchers choose to *sample* individuals and use weights to estimate numbers in the target population. There are a variety of different sampling methods that can be used, and more information on these can be found in Chapter \@ref(c05). This decision of which sampling method to use, impacts *sampling error* and can be adjusted for in weighting.

#### Example: Number of Pets in a Household {.unnumbered #overview-design-sampdesign-ex}

Let's use a simple example where a researcher is interested in knowing the average number of pets in a household. Our researcher will need to consider what the target population is for this study. Specifically, are they interested in all of the U.S., a different country, or a more local area (e.g., city or state). Let's assume our researcher is interested in the number of pets in a U.S. household where there is at least one adult living (18 years old or older). In this case, using a sampling frame of mailing addresses would provide the least amount of coverage error as the frame would closely match our target population. Specifically, our researcher would most likely want to use the Computerized Delivery Sequence File (CDSF), which is a file of mailing addresses that the United States Postal Service (USPS) creates and covers about 95% of U.S. households ({cite Iannichone<!--Check the 95% number when adding citation, not sure if coverage has increased since then-->}). To sample these households, for simplicity, we will use a stratified simple random sample design, where we stratify by state. 

Throughout this chapter we will build on this example research question to plan a survey. 

### Data Collection Planning {#overview-design-dcplanning}

With the sampling design decided, researchers can then make decisions on how to survey these individuals. Specifically, the *modes* used for contacting and surveying the sample, how frequently to send reminders and follow-ups, and the overall timeline of the study are four of the major data collection determinations. Traditionally, researchers have considered four main modes^[Other modes such as using mobile apps or text messaging can also be considered, but have typically smaller reach or are better for longitudinal studies (i.e., surveying the same individuals over many waves of a single study). For the purposes of this overview we will focus on these four main modes.] for conducting surveys:

-  Computer Assisted Personal Interview (CAPI; also known as face-to-face or in-person interviewing)
-  Computer Assisted Telephone Interview (CATI; also known as phone or telephone interviewing)
-  Web (also known as Computer Assisted Web Interview or CAWI)
-  Paper and Pencil (PAPI)

Researchers can use a single mode to collect data, or multiple modes (also called mixed modes). Using mixed modes can allow for broader reach and can increase response rates depending on the target population ({CITE DeLeeuw mixed mode article}). For example, researchers could both call households to conduct a CATI survey and send mail with a PAPI survey to the household.  Using both of these modes, reseachers could gain participation through the mail from individuals who do not pick up the phone to unknown numbers, or through the phone from individuals who do not open all of their mail. However, mode effects (where responses differ based on the mode of response) can be present in the data and may need to be considered during analysis.

When selecting which mode, or modes, to use, understanding the unique aspects of the chosen target population and sampling frame will provide insight into how they can best be reached and engaged. For example, if we plan to survey adults age 18-24 who live in North Carolina, asking them to complete a survey using CATI (i.e., over the phone) would most likely not be as successful as other modes like web as this age group does not talk on the phone as much as other generations, and often do not answer their phones for unknown numbers. Additionally, the mode for contacting respondents relies on what information is available on the sample frame. For example, if our sampling frame includes email address, we could send email to our selected sample members to convince them to complete a survey. Or if the sampling frame is a list of mailing addresses, researchers would have to contact sample members with a letter. 

It is important to note that there can be a difference between the contact mode and the survey mode. For example, if we have a sample frame with addresses, we can send a letter to our sample members and provide information on how to complete a web survey. Or we could use mixed-mode surveys and send sample members a paper and pencil survey with our letter and also ask them to complete the survey online. Combining different contact modes and different survey modes can be useful in reducing *unit nonresponse error*, as different sample members may respond better to different contact and survey modes. However, when considering which modes to use, it is important to make access to the survey as easy as possible for sample members to reduce burden and unit nonresponse.

Another way to reduce unit nonresponse error is though varying the language of the contact materials ({cite Dillman Book}). People are motivated by different things, so constantly repeating the same messaging may not be helpful. Instead, mixing up the messaging and the type of contact material the sample member receives can increase response rates and reduce unit nonresponse error. For example, instead of only sending standard letters, researchers could consider sending mailings that invoke "urgent" or "important" thoughts by sending priority letters, or using other delivery services like FedEx, UPS, or DHL.

Determining the number and types of contacts may also be determined by a study timeline. If the timeline is long, then there is a lot of time for follow-ups and varying the message in contact materials. If the timeline is short, than fewer follow-ups can be implemented. Many studies will start with the tailored design method put forth by {Dillman et al. (2014)} and implement 5 contacts: (1) prenotice letting sample members know the survey is coming, (2) invitation to complete the survey, (3) reminder postcard that also thanks respondents that may have already completed the survey, (4) reminder letter (with a replacement paper survey if needed), and (5) final reminder postcard. This method is easily adaptable based on the study timeline and needs, but provides a good basis for most studies.


#### Example: Number of Pets in a Household {.unnumbered #overview-design-dcplanning-ex}

Let's return to our example of a researcher who wants to know the average number of pets in a household. We are using a sample frame of mailing addresses, so we recommend starting our data collection with letters mailed to households, but later in data collection we want to send interviewers to the house to conduct an in-person (or CAPI) interview in an effort to decrease unit nonresponse error. This means we will have two contact modes (paper and in-person). As we mentioned above, the survey mode does not have to be the same as the contact mode, so we recommend a mixed-mode study with both Web and CAPI modes. Let's assume we have 6 months for data collection, so we may want to recommend the following protocol:

Table: Protocol Example for 6-month Web and CAPI Data Collection

Week  | Contact Mode          | Contact Message         | Survey Mode Offered
:----: | ------------------------------ | ------------------------------- | -------------------
   1 | Mail: Letter          | Prenotice            | ---
   2 | Mail: Letter          | Invitation           | Web
   3 | Mail: Postcard         | Thank You/Reminder       | Web
   6 | Mail: Letter in large envelope | Animal Welfare Discussion    | Web
  10 | Mail: Postcard         | Inform Upcoming In-Person Visit | Web
  14 | In-Person Visit        | ---               | CAPI
  16 | Mail: Letter          | Reminder of In-Person Visit   | Web, but includes number to call to schedule CAPI
  20 | In-Person Visit        | ---               | CAPI
  25 | Mail: Letter in large envelope | Survey Closing Notice      | Web, but includes number to call to schedule CAPI

This is just one possible protocol that could be used that starts respondents with web (typically done to reduce costs). However, researchers may want to start in-person data collection earlier during the data collection period, or ask their interviewers to attempt more than 2 visits with a household.

### Questionnaire Design {#overview-design-questionnaire}

When developing the questionnaire, it can be helpful to first outline the topics to be asked and include the "why" each question or topic is important to the research question(s). This can help researchers better tailor the questionnaire and potentially reduce the number of questions (and thus burden on the respondent) if topics are deemed irrelevant to the research question. When making these decisions, researchers should also consider questions needed for weighting purposes. While we would love to have everyone sampled answer our survey, this is rarely the case. Thus, including questions about demographics in the survey can assist with weighting for *nonresponse error* (both unit and item nonresponse). Knowing details of the sampling plan and what may impact *coverage error* and *sampling error* can help guide researchers in determining what types of demographics to include.

Researchers can benefit from the work of others by using questions from other surveys. This is common with demographic questions such as race and ethnicity that use questions from a government census or other official surveys. Other survey questions can be found using question banks which are a compilation of questions that have been asked across a variety of surveys such as the [Inter-university Consortium for Political and Social Research (ICPSR) variable search](https://www.icpsr.umich.edu/web/pages/ICPSR/ssvd/).

If a question does not already exist in a question bank, then researchers can craft their own. When creating their own questions, researchers should start with the research question or topic and attempt to write questions that match the concept. The closer the question asked is to the overall concept the better *validity* there is. For example, if the researcher wants to know how people consume TV series and movies, but only ask a question about how many TVs are in the house, then they would be missing other ways that people watch TV series and movies such as on other devices or at places outside of the home.

Additionally, when designing questions, researchers should consider the mode the survey will be administered in and adjust language appropriately. In self-administered surveys (e.g., web or mail), respondents can see all of the questions and response options, but that is not the case in interviewer-administered surveys (e.g., CATI or CAPI). With interviewer-administered surveys, the response options need to be read out loud to the respondents, so the question may need to be adjusted to allow a better flow to the interview. Additionally, with self-administered surveys, because the respondents are viewing the questionnaire, the formatting of the questions is even more important to ensure accurate measurement. Incorrectly formatting or wording questions can result in *measurement error*, so following best practices or using existing validated questions can reduce error. There are multiple resources out there to help researchers draft questions for different modes (e.g., {CITE Dillman, fowler, ed book?, asking questions, tourangeau formatting article?}).


#### Example: Number of Pets in a Household {.unnumbered #overview-design-questionnaire-ex}

As part of our survey on the average number of pets in a household, researchers may want to know what animal the majority of people prefer to have as a pet. Let's say we have the following question in our survey:

<!-- Put this in as an image instead of as a block quote -->

> What animal do you prefer to have as a pet?
>
> <ul class="ro">
>
> <li class="ro">
>
> Dogs
>
> </li>
>
> <li class="ro">
>
> Cats
>
> </li>
>
> </ul>

This question may have validity issues as it is only providing the options of "dogs" and "cats" to respondents and interpretation of the data could be incorrect. For example, if we had 100 respondents who answered the question and 50 selected dogs, then the results of this question cannot be: `50% of the population perfers to have a dog as a pet` as only two response options were provided. If a respondent taking our survey prefers turtles, they could either be forced to choose a response between these two (i.e., interpret the question as "between dogs and cats which do you prefer?" and result in *measurement error*), or they may not answer the question (which results in *item nonresponse error*). Based on this, the interpretation of this question should be `When given the choice between dogs and cats, 50% of respondents preferred to have a dog as a pet`.

To avoid this issue, researchers should consider these possibilities and adjust the question accordingly. One simple way could be to add an "other" response option to give respondents a chance to provide a different response. The "other" response option could then include a way for respondents to write in what their other preference is. For example, this question could be rewritten as

<!-- Put this in as an image instead of as a block quote, or need to add in an open-ended box for write in after the last option. -->

> What animal do you prefer to have as a pet?
>
> <ul class="ro">
>
> <li class="ro">
>
> Dogs
>
> </li>
>
> <li class="ro">
>
> Cats
>
> </li>
>
> <li class="ro">
>
> Other, please specify:
>
> </li>
>
> </ul>

Researchers can then code the responses from the open-ended box and get a better understanding of the respondent's choice of preferred pet. Interpreting this question becomes easier as researchers no longer need to qualify the results with the choices provided.

This is a very simple example of how the question presentation and options can impact the findings. More complex topics and questions will need researchers to thoroughly consider how to mitigate any impacts from the presentation, formatting, wording, and other aspects. As survey analysts, reviewing not only the data but also the wording of the questions is crucial to ensure the results are presented in a manner consistent with the question asked.


## Data Collection {#overview-datacollection}

Once the data collection starts, researchers try to stick to the data collection protocol designed during pre survey planning. However, a good researcher will adjust their plans and adapt as needed to the current progress of data collection ({cite Peytchev book on adaptive survey design}). Some extreme examples could be natural disasters that could prevent mail or interviewers from getting to the sample members. Others could be smaller in that something news worthy occurs that is connected to the survey, so researchers could choose to play this up in communication materials. In addition to these external factors, there could be factors unique to the survey such as the response rates are lower for a specific sub-group, so the data collection protocol may need to find ways to improve response rates for a specific group.


## Post Survey Processing {#overview-post}

After data collection, there are a variety of activities that need to be conducted before the survey can be analyzed. Multiple decisions made during this post survey phase can assist researchers in reducing different error sources such as through weighting to account for the sample selection. Knowing the decisions researchers made in creating the final analytic data can impact how analysts use the data and interpret the results.


### Data Cleaning and Imputation {#overview-post-cleaning}

Post survey cleaning and imputation is one of the first steps researchers will do to get the survey responses into a dataset for use by analysts. Data cleaning can comprise of cleaning inconsistent data (e.g., with skip pattern errors or multiple questions throughout the survey being consistent with each other), editing numeric entry or open-ended responses for grammar and consistency, or recoding open-ended questions into categories for analysis. Each project should create their own rules for how to handle different cleaning situations, and there are no specific rules that must be followed. Instead, researchers should use their best judgement in ensuring data integrity remains and all decisions should be documented and available to those using the data in analysis. Each decision a researcher makes has an impact on *processing error*, so often researchers often will have multiple people review these rules or recode open-ended data and adjudicate any differences in an attempt to reduce this error.

Another crucial step in post survey processing is imputation. Often times there is item nonresponse where respondents do not answer specific questions. If the questions are crucial to analysis efforts or to the research question, researchers will implement imputation in an effort to reduce *item nonresponse error*. However, as imputation is a way of assigning a value to missing data based on an algorithm or model, it can introduce *processing error* as well, so researchers should consider the overall implications of imputing data compared to having item nonresponse. There are multiple ways that imputation can be conducted, we recommend reviewing other resources like {cite Kim & Shao, Statistical Methods for Handling Incomplete Data} for more information.

#### Example: Number of Pets in a Household {.unnumbered #overview-post-cleaning-ex}

Let's return to the question we created to ask about [animal preference](#overview-design-questionnaire-ex). The "other specify" invites respondents to specify the type of animal they prefer to have as a pet. If respondents entered answers such as "puppy", "turtle", "rabit", "rabbit", "bunny", "ant farm", "snake", "Mr. Purr", then researchers may wish to categorize these write-in responses to help with analysis. In this example, "puppy" could be assumed to be a reference to a `Dog`, and could be recoded there. The misspelling of "rabit" could be coded along with "rabbit" and "bunny" into a single category of `Bunny or Rabbit`. These are relatively standard decisions that a researcher could make. The remaining write-in responses could be categorized into a few different ways. "Mr. Purr", which may be someone's reference to their own cat, could be recoded to `Cat`, or it could remain as `Other` or some category that is `Unknown`. Depending on the number of responses related to each of others, they could all be combined into a single `Other` category, or maybe categories such as `Reptiles` or `Insects` could be created. Each of these decisions may impact the interpetation of the data, so our researcher should make sure to document the types of responses that fall into each of the new categories.



### Weighting {#overview-post-weighting}

Weighting can typically be used to address some of the error sources identified in the previous sections.  For example, weights may be used to address coverage, sampling, and nonresponse errors and many published surveys will include an "analysis weight" variable that combines these adjustments. However, weighting itself can also introduce *adjustment error*, so researchers need to balance which types of errors should be corrected with weighting. The construction of weights is outside the scope of this book and researchers should reference other materials if interested constructing their own (e.g., {CITE Jill's Book}). Instead, this book assumes the survey has been completed, weights are constructed, and data is made available for users. We will walk users through how to read documentation and work with the data and analysis weights provided to analyze and interpret survey results correctly.

#### Example: Number of Pets in a Household {.unnumbered #overview-post-weighting-ex}

In the simple example of our survey, we decided to use a stratified sample by state to select our sample members.  Knowing this sampling design our researcher can include selection weights for analysis that account for how the sample members were selected into the survey.  Additionally, the sampling frame may have the type of building associated with each address, so we could include the building type as a potential nonresponse weighting variable, along with some interviewer observations that may be related to our research topic of average number of pets in a household.  Combining these weights we could create an analytic weight that researchers can use when analyzing the data.


### Disclosure {#overview-post-disclosure}

Before data is allowed to be made publicly available, researchers will need to ensure that individual respondents can not be identified by the data when confidentiality is required.  There are a variety of different methods that can be used, including data swapping, top or bottom coding, coarsening, and perturbation.  In data swapping, researchers may swap specific data values across different respondents such that it does not impact insights that come from the data, but ensures that specific individuals cannot be identified.  For extreme values, top and bottom coding is sometimes used. For example, researchers may top-code incomes values such that households with income greater than \$99,999,999 are coded into a single category of \$99,999,999 or more. Other methods for disclosure may include aggregating response categories or location information to avoid having only a few respondents in a given group, and thus be identified.  For example, researchers couse use coarsening to display income in categories instead of as a continuous variable.  Data producers may also perturb the data by adding random noise. There is as much art as there is science to the methods used for disclosure, and in documentation researchers should only provide a high level comments that disclosure was conducted and not specific details to ensure nobody can reverse the disclosure and thus identify individuals.  For more information on different disclosure methods please see {CITE Skinner chapter in this book: https://www.sciencedirect.com/science/article/abs/pii/S0169716108000151
AAPOR checklist/standards: https://www-archive.aapor.org/Standards-Ethics/AAPOR-Code-of-Ethics/Survey-Disclosure-Checklist.aspx}.


## This book

After all of these steps are taken, the data is ready for use by analysts. The rest of this book works from this point. If you're interested in learning more about the steps talked about here, we recommend you look into the references cited throughout this chapter.

<!--chapter:end:02-introducing-survey-data.Rmd-->

# Understanding survey data files {#c03}

<!--chapter:end:03-understanding-survey-data-files.Rmd-->

# Introducing the srvyr package {#c04}

<!--chapter:end:04-introducing-the-srvyr-package.Rmd-->

# Specifying sample designs and replicate weights in srvyr {#c05}

The primary reason for using packages like {survey} and {srvyr} are to incorporate the sampling design or replicate weights into estimates. By incorporating the sampling design or replicate weights, precision estimates (e.g., standard errors and confidence intervals) are appropriately calculated. In this chapter, we will introduce common sampling designs and common types of replicate weights, the mathematical methods for calculating estimates and standard errors for a given sampling design, and the R syntax to specify the sampling design or replicate weights. While we will show the math behind the estimates, the functions in these packages will do the calculation. To deeply understand the math and the derivation, refer to @sarndal2003model, @wolter2007introduction, or @fuller2011sampling.

The general process for estimation in the {srvyr} package is to:

1. Create a `tbl_svy` object (a survey object) using: `as_survey_design` or `as_survey_rep`

2. Subset data (if needed) using `filter` (subpopulations)

3. Specify domains of analysis using `group_by` 

4. Within `summarize`, specify variables to calculate including means, totals, proportions, quantiles, and more

This chapter includes details on the first step - creating the survey object. The other steps are detailed in the next several chapters.

## Common sampling designs

A sampling design is the method used to draw a sample. Both logistical and statistical elements are considered when developing a sampling design. When specifying a sampling design in R, the levels of sampling are specified along with the weights. Each record of a weight is constructed so that the particular record represents that many units in the population. For example, in a survey of 6th grade students in the United States, the weight associated with each responding student reflects how many students that record represents. Generally, the sum of the weights sum to the population total though some studies have the sum of the weights sum to the number of respondent records.

Some common terminology across the designs are:

- sample size, generally denoted as $n$, is the number of units selected
- population size, generally denoted as $N$, is the number of units in the population
- sampling frame is the list of units from which the sample is drawn

### Simple random sample without replacement

- **Description**: The simple random sample (SRS) without replacement is a sampling design where a fixed sample size is selected from a sampling frame, and every possible subsample has equal probability of selection.
- **Requirements**: The sampling frame must include the entire population.
- **Advantages**: SRS requires no information about the units apart from contact information.
- **Disadvantages**: The sampling frame may not be available for entire population. This design is not generally feasible for in-person data collection.
- **Example**: Randomly students in a university from a roster provided by the registrar's office.

#### The math {-}

The estimate for the population mean of variable $y$ is:

$$\bar{y}=\frac{1}{n}\sum_{i=1}^n y_i$$

and the estimate of the standard error of mean is:

$$se(\bar{y})=\sqrt{\frac{s^2}{n}\left( 1-\frac{n}{N} \right)}$$ where

$$s^2=\frac{1}{n-1}\sum_{i=1}^n\left(y_i-\bar{y}\right)^2.$$

This standard error estimate might look very similar to equations in other applications except for the part on the right side of the equation: $1-\frac{n}{N}$. This is called the finite population correction factor (FPC), and if the size of the frame, $N$, is very large, the FPC is negligible so it is often ignored. 

To estimate proportions, we define $x_i$ as the indicator if the outcome is observed. That is, $x_i=1$ if the outcome is observed and $x_i=0$ if the outcome is not observed. Then the estimated proportion from a SRS design is:

$$\hat{p}=\frac{1}{n}\sum_{i=1}^n x_i $$
and the estimated standard error of the proportion is:

$$se(\hat{p})=\sqrt{\frac{\hat{p}(1-\hat{p})}{n-1}\left(1-\frac{n}{N}\right)} $$

#### The syntax {-} 

If a sample was drawn through SRS and had no nonresponse or other weighting adjustments, in R specify this design as:

```r
des_srs1 <- dat %>%
  as_survey_design(fpc = fpcvar)
```

where `dat` is a tibble or data.frame with the survey data and `fpcvar` is a variable on the tibble which indicates the size of the sampling frame. If the frame was very large, sometimes the frame size is not provided. In that case, the fpc is not needed and specify the design as:

```r
des_srs2 <- dat %>%
  as_survey_design()
```

If some post-survey adjustments were implemented and the weights are not all equal, specify the design as:

```r
des_srs3 <- dat %>%
  as_survey_design(weights = wtvar, fpc = fpcvar)
```

where `wtvar` is the variable for the weight on the data. Again, the fpc can be omitted if it is not necessary because the frame is large.

#### Example {-} 

The {survey} package in R provides some example datasets to use and those will be used throughout this chapter. Reading the documentation about these datasets provide detail on the variables. One of the example datasets we will use is from the Academic Performance Index (API).  The API was a program administered by the California Department of Education and the {survey} package includes a population file (frame) of all schools with at least 100 students and several different samples pulled from that data using different sampling methods. For this first example, we will use the `apisrs` dataset, which contains a SRS of 200 schools.  For printing purposes, we create a new dataset called `apisrs_slim`, which sorts the data by school district and school ID and subsets the data to only a few columns. The SRS sample data is illustrated below:

```{r}
#| label: apisrs_display
options(tidyverse.quiet = TRUE)
library(tidyverse)
library(survey)
library(srvyr)

data(api)

apisrs_slim <- 
  apisrs %>%
  as_tibble() %>%
  arrange(dnum, snum) %>%
  select(cds, dnum, snum, dname, sname, fpc, pw)

apisrs_slim
```

School districts have identifiers `dnum` within counties and schools have identifiers of `snum` within districts along with their names, `dname` and `sname`. The unique identifier for a school is `cds` which is unique within the state. Additionally, the `fpc` is included as `fpc` and the weight as `pw`. To create the `tbl_survey` object for this SRS data, the design should be specified as:

```{r}
#| label: apisrs_des
des_apisrs <- apisrs_slim %>%
  as_survey_design(weights = pw, fpc = fpc)

des_apisrs
summary(des_apisrs)
```

In the printed design object above, the design is described as an "Independent Sampling design" which is another terminology for SRS. The ids are specified as `1` which means there is no clustering (a topic described later in this chapter), the fpc variable is indicated, and the weights are indicated. When looking at the summary of the design object, the population size is given as a summary of the probabilities (inverse of the weights).

### Simple random sample with replacement

- **Description**: The simple random sample with replacement (SRSWR) is a sampling design where a sample is selected from a sampling frame with the units being replaced before drawing again, so units can be selected more than once.
- **Requirements**: The sampling frame must include the entire population.
- **Advantages**: SRSWR requires no information about the units apart from contact information.
- **Disadvantages**: The sampling frame may not be available for entire population. This design is not generally feasible for in-person data collection. Units can be selected more than once resulting in a smaller realized sample size. For small populations, SRSWR has larger standard errors than SRS designs.
- **Example**: A professor puts all students names on paper slips and selects them randomly to ask students questions but the professor replaces the paper after calling on the student so they can be selected again at any time.


#### The math {-}

The estimate for the population mean of variable $y$ is:

$$\bar{y}=\frac{1}{n}\sum_{i=1}^n y_i$$

and the estimate of the standard error of mean is:

$$se(\bar{y})=\sqrt{\frac{s^2}{n}}$$ where

$$s^2=\frac{1}{n-1}\sum_{i=1}^n\left(y_i-\bar{y}\right)^2.$$
To calculate the estimated proportion, we define $x_i$ as the indicator that the outcome is observed (as we did with SRS):

$$\hat{p}=\frac{1}{n}\sum_{i=1}^n x_i $$
and the estimated standard error of the proportion is:

$$se(\hat{p})=\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} $$

#### The syntax {-} 

If you had a sample that was drawn through SRSWR and had no nonresponse or other weighting adjustments, in R, you should specify this design as:

```r
des_srswr1 <- dat %>%
  as_survey_design()
```

where `dat` is a tibble or data.frame with your survey data.

If some post-survey adjustments were implemented and the weights are not all equal, specify the design as:

```r
des_srswr2 <- dat %>%
  as_survey_design(weights = wtvar)
```

where `wtvar` is the variable for the weight on the data.

#### Example {-} 

The {survey} package does not include an example of SRSWR so we create an example from the population data provided. We call this new dataset `apisrswr`.

```{r}
#| label: apisrs_wr_display
set.seed(409963)

apisrswr <- apipop %>%
  as_tibble() %>%
  slice_sample(n = 200, replace = TRUE) %>%
  select(cds, dnum, snum, dname, sname) %>%
  mutate(
    weight = nrow(apipop)/200
  )

head(apisrswr)

```

Because this is a SRS design *with replacement* there will be duplicates in the data.  It is important to keep the duplicates in the data for proper estimation, but for reference here are the duplicates in the example we just created.

```{r}
#| label: apisrs_wr_duplicates

apisrswr %>%
  group_by(cds) %>%
  filter(n()>1) %>%
  arrange(cds)
```


A weight variable was added which is the inverse of the probability of selection. To specify the sampling design for `apisrswr`, the following syntax should be used:

```{r}
#| label: apisrswr_des
des_apisrswr <- apisrswr %>%
  as_survey_design(weights = weight)

des_apisrswr
summary(des_apisrswr)
```

In the chunk above, the design object is printed and the object summary is shown. Both of these note that the sampling is done "with replacement" because no fpc was specified. In the summary, the probabilities, which are derived from the weights, are summarized.

### Stratified sampling

- **Description**: A population is divided into mutually exclusive subpopulations (strata) and then samples are selected independently within each stratum.
- **Requirements**: The sampling frame must include the information to divide the population into groups for every unit.
- **Advantages**: This design ensures sample representation in all subpopulations. Often, for the same sample size as a SRS sample, the standard errors are smaller so it is a more efficient design. This is true if the strata are correlated with outcomes.
- **Disadvantages**: Auxiliary data may not exist to divide sampling frame into groups or the data may be outdated.
- Examples: 
  - **Example 1**: A population of North Carolina residents could be separated into urban and rural areas and then a SRS of residents from both rural and urban areas is selected independently. This ensures there are rural residents in the sample.
  - **Example 2**: There are 3 primary general purpose law enforcement agencies in the US - local police, sheriff's departments, and state police. In a survey of law enforcement agencies, the agency type could be used to form strata.

#### The math {-} 

Let $\bar{y}_h$ be the sample mean for stratum $h$, $N_h$ be the population size of stratum $h$, and $n_h$ be the sample size of stratum $h$. Then the estimate for the population mean under stratified SRS sampling is:

$$\bar{y}=\frac{1}{N}\sum_{h=1}^H N_h\bar{y}_h$$
and the estimate of the standard error of $\bar{y}$ is:

$$se(\bar{y})=\sqrt{\frac{1}{N^2} \sum_{h=1}^H N_h^2 s_h^2\left(1-\frac{n_h}{N_h}\right)} $$ 

where 
$$s_h^2=\frac{1}{n_h-1}\sum_{i=1}^{n_h}\left(y_{i,h}-\bar{y}_h\right)^2.$$

For estimates of proportions, let $\hat{p}_h$ be the estimated proportion in stratum $h$. Then the population proportion estimate is:

$$\hat{p}= \frac{1}{N}\sum_{h=1}^H N_h \hat{p}_h$$

and the standard error of the proportion is:

$$se(\hat{p}) = \frac{1}{N} \sqrt{ \sum_{h=1}^H N_h^2 \frac{\hat{p}_h(1-\hat{p}_h)}{n_h-1} \left(1-\frac{n_h}{N_h}\right)}$$

#### The syntax {-} 

To specify a stratified SRS design in {srvyr} where the population sizes of the strata are not too large and are known, that is you are using the fpc, specify the design as:

```r
des_stsrs1 <- dat %>%
  as_survey_design(fpc = fpcvar, strata = stratvar)
```

where `fpcvar` is a variable on your data which indicates $N_h$ for each row and `stratavar` is a variable indicating the stratum for each row. You can omit the fpc if it is not applicable. Additionally, you can indicate the weight variable if it is present where `wtvar` is a variable on your data with a numeric weight.

```r
des_stsrs2 <- dat %>%
  as_survey_design(weights = wtvar, strata = stratvar)
```

#### Example {-} 

In the example API data, `apistrat` is a stratified random sample, stratified by school type (`stype`). As with the SRS example above, we sort and select specific variables for use of printing.  The data are illustrated below including a count of the number of cases per stratum:

```{r}
#| label: apistrat_dis
apistrat_slim <- 
  apistrat %>%
  as_tibble() %>%
  arrange(dnum, snum) %>%
  select(cds, dnum, snum, dname, sname, stype, fpc, pw)

apistrat_slim %>% 
  count(stype, fpc)
```

The fpc is the same within each stratum and 100 elementary schools were sampled while 50 schools were sampled from both the middle and high school levels. This design should be specified as:

```{r}
#| label: apistrat_des
des_apistrat <- apistrat_slim %>%
  as_survey_design(strata = stype, weights = pw, fpc = fpc)

des_apistrat
summary(des_apistrat)
```

When printing the object, it is specified as a "Stratified Independent Sampling design" also known as a stratified SRS and the strata variable is included. In the summary, a numeric summary of the probabilities of selection are displayed as well as the sample and population stratum sizes.

### Clustered sampling

- **Description**: A population is divided into mutually exclusive subgroups called clusters or primary sampling units (PSUs). A random selection of PSUs are sampled and then another level of sampling is done within these clusters. There can be multiple levels of this selection. Clustered sampling is often used when a list of the entire population is not available or data collection involves interviewers needing direct contact with respondents.
- **Requirements**: There must have be a way to divide the population into clusters. Clusters are commonly structural such as institutions (e.g., schools, prisons) or geographic (e.g., states, counties). 
- **Advantages**: Clustered sampling is advantageous when data collection is done in person so interviewers are sent to specific sampled areas rather than completely at random across a country. With cluster sampling, a list of the entire population is not necessary. For example, if sampling students, you do not need a list of all students but only a list of all schools. Once the schools are sampled, lists of students can be obtained within the sampled schools.
- **Disadvantages**: Compared to a simple random sample, for the same sample size, clustered samples generally have larger standard errors of estimates.
- Examples: 
  - **Example 1**: Consider a study needing a sample of 6th grade students in the United States, no list likely exists of all these students. However, it is more likely to be possible to obtain a list of schools that have 6th graders, so a study design could select a random sample of schools that have 6th graders. The selected schools can then provide a list of students to do a second stage of sampling where 6th grade students are randomly sampled within each of the sampled schools. This is a one-stage sample design and will be the type of design we will discuss in formulas below.
  - **Example 2**: Consider a study sending interviewers to households for a survey. This is a more complicated example that requires two levels of selection, to efficiently use interviewers in geographic clusters.  First, in the U.S. counties could be selected as the PSU, then Census block groups within counties could be selected as the secondary sampling unit (SSU).  Households could then randomly sampled within the block groups.  This type of design is popular for in-person survey as it reduces the travel necessary for interviewers.

#### The math {-} 

Consider a population where the are $N$ clusters and $n$ clusters are sampled via SRS. Units within each sampled cluster are sampled via SRS as well. Let $M_i$ be the number of units in cluster $i$ and $\bar{y}_i$ be the sample mean of cluster $i$. Then, a ratio estimator of the population mean is:

$$\bar{y}=\frac{\sum_{i=1}^n M_i \bar{y}_i}{ \sum_{i=1}^n M_i}$$
Note this is a consistent but biased estimator. Often the population size is not known so this is a method to estimate a mean without knowing the population size. The estimated standard error of the mean is:

$$se(\bar{y})=\frac{1}{\hat{N}_{pop} } \sqrt{\frac{N^2 (1-\frac{n}{N})}{n}\frac{1}{n-1} \sum_{i=1}^n (M_i\bar{y}_i -\hat{t}/N)^2  + \frac{N}{n}  \sum_{i=1}^n \frac{M_i^2}{m_i}\left(1-\frac{m_i}{M_i}\right)s^2_i }$$
where $\hat{N}_{pop}$ is the estimated population size, $\hat{t}$ is the estimated total, and $s_i^2$ is the sample variance of cluster $i$.

For estimates of proportions, the estimated proportion is:

$$\hat{p}=\frac{\sum_{i=1}^n M_i \hat{p}_i}{ \sum_{i=1}^n M_i}$$

and the associated standard error estimate is:

$$se(\hat{p})=\frac{1}{\hat{N}_{pop} } \sqrt{\frac{N^2 (1-\frac{n}{N})}{n}\frac{1}{n-1} \sum_{i=1}^n (M_i\hat{p}_i -\hat{t}/N)^2  + \frac{N}{n}  \sum_{i=1}^n \frac{M_i^2}{m_i}\left(1-\frac{m_i}{M_i}\right)s^2_i }$$

where $s^2_i$ is defined as:

$$s^2_i = \frac{m_hp_h(1-p_h)}{m_h-1}$$.

#### The syntax {-} 

To specify a two-stage clustered design without replacement,  use the following syntax:


```r
des_clus2 <- dat %>%
  as_survey_design(weights = wtvar, ids = c(PSU, SSU), fpc = c(N, M))
```

where `PSU` and `SSU` are the variables indicating the PSU and SSU identifiers and `N` and `M` are the variables indicating the population sizes for each level (i.e., `N` is the number of clusters and `M` is the number of units within each cluster). Note that `N` will be the same for all records (within a strata) and `M` will be the same for all records within the same cluster.

If clusters were sampled with replacement or from a very large population, a fpc is not necessary. Additionally, only the first stage of selection is necessary regardless of whether the units were selected with replacement at any stage. The subsequent stages of selection are ignored in computation as their contribution to the variance is overpowered by the first stage, see @sarndal2003model or @wolter2007introduction for a more in-depth discussion. The syntax below will yield the same estimates in the end:

```r
des_clus2wra <- dat %>%
  as_survey_design(weights = wtvar, ids = c(PSU, SSU))

des_clus2wrb <- dat %>%
  as_survey_design(weights = wtvar, ids = PSU)

```

#### Example {-} 

The `survey` package includes a two-stage cluster sample data, `apiclus2`, in which school districts were sampled and then a random sample of 5 schools was selected within each district. For districts with fewer than 5 schools, all schools were sampled. School districts are identified by `dnum` and schools are identified by `snum`. The variable `fpc1` indicates how many districts there are in California (`N`) and `fpc2` indicates how many schools were in a given district with at least 100 students (`M`).  The data has a row for each school. In the data printed below, there are 757 school districts as indicated by `fpc1` and there are 9 schools in district 731, one school in district 742, 2 schools in district 768, and so on as indicated by `fpc2`. For illustration purposes, the object `apiclus2_slim` has been created from `apiclus2`, which subsets the data to only the necessary columns and sorts data.

```{r}
#| label: api2clus_dis
apiclus2_slim <- 
  apiclus2 %>%
  as_tibble() %>%
  arrange(desc(dnum), snum) %>%
  select(cds, dnum, snum, fpc1, fpc2, pw)

apiclus2_slim
```

To specify this design in R, the following syntax should be used:

```{r}
#| label: api2clus_des
des_apiclus2 <- apiclus2_slim %>%
  as_survey_design(ids = c(dnum, snum), fpc = c(fpc1, fpc2), weights=pw)

des_apiclus2
summary(des_apiclus2)
```

The design objects are described as "2 - level Cluster Sampling design" and includes the ids (cluster), fpc, and weight variables. In the summary, it is noted that the sample includes 40 first-level clusters (PSUs) which are school districts and 126 second-level clusters (SSUs) which are schools. Additionally, the summary includes a numeric summary of the probabilities and the population size (number of PSUs) as 757.


## Replicate weights

Replicate weights are often included on analysis files instead of, or in addition to, the design variables (strata and PSUs). Replicate weights are used as another method to estimate variability and often used specifically so that design variables are not published as a measure to limit disclosure risk. There are several types of replicate weights including balanced repeated replication (BRR), Fay's BRR, jackknife, and bootstrap methods. An overview of the process for using replicate weights is:

1. Divide the sample into subsample **replicates** that mirror the design of the sample
2. Calculate weights for each **replicate** using the same procedures for full-sample weight (i.e., nonresponse and post-stratification)
3. Calculate estimates for each **replicate** using the same method as the full-sample estimate
4. Calculate the estimated variance which will be proportional to the variance of the replicate estimates

The different types of replicate weights largely differ in step 1 - how the sample is divided into subsamples and step 4 - which multiplication factors (scales) are used to multiply the variance.

### BRR method
The BRR method requires a stratified sample design with two PSUs in each stratum. Each replicate is constructed by deleting one PSU per stratum using a Hadamard matrix. For the PSU that is included, the weight is generally multiplied by 2 but may have other adjustments, such as post-stratification. A Hadamard matrix is a special square matrix with entries of +1 or -1 with mutually orthogonal rows. Hadamard matrices must have 1 row, 2 rows, or a multiple of 4 rows. An example of a $4\times4$ Hadamard matrix is below:

$$ \begin{array}{rrrr} +1 &+1 &+1 &+1\\ +1&-1&+1&-1\\ +1&+1&-1&-1\\ +1 &-1&-1&+1  \end{array} $$
The columns specify the strata and the rows the replicate. In the first replicate all the values are +1, so in each stratum the first PSU would be used in the estimate. In the second replicate, the first PSU would be used in stratum 1 and 3, while the second PSU would be used in stratum 2 and 4. In the third replicate, the first PSU would be used in stratum 1 and 2, while the second PSU would be used in stratum 3 and 4. Finally, in the fourth replicate, the first PSU would be used in stratum 1 and 4, while the second PSU would be used in stratum 2 and 3.

#### The math {-}

A weighted estimate for the full sample is calculated as $\hat{\theta}$ and then a weighted estimate for each replicate is calculated as $\hat{\theta}_r$ for $R$ replicates. Then the standard error of the estimate is calculated as:

$$se(\hat{\theta})=\sqrt{\frac{1}{R} \sum_{r=1}^R \left( \hat{\theta}_r-\hat{\theta}\right)^2}$$
Specifying replicate weights in R requires specifying the type of replicate weights, the main weight variable, the replicate weight variables, and some other options. One of the key options is for `mse`. If `mse=TRUE`, variances are computed around the point estimate $(\hat{\theta})$, whereas if `mse=FALSE`, variances are computed around the mean of the replicates $(\bar{\theta})$ instead which looks like this:

$$se(\hat{\theta})=\sqrt{\frac{1}{R} \sum_{r=1}^R \left( \hat{\theta}_r-\bar{\theta}\right)^2}$$ where $$\bar{\theta}=\frac{1}{R}\sum_{r=1}^R \hat{\theta}_r$$

The default option for `mse` is to use the global option of "survey.replicates.mse" which is set to `FALSE` initially unless a user changes it. Unless documentation states otherwise, for BRR, set `mse` to `TRUE`.

#### The syntax {-} 

Replicate weights generally come in groups and are sequentially numbered such as PWGTP1, PWGTP2, ..., PWGTP80 in the American Community Survey (ACS) or BRRWT1, BRRWT2, ..., BRRWT96 in the 2015 Residential Energy Consumption Survey (RECS). The {srvyr} package relies on tidy selection^[dplyr documentation on tidy-select: https://dplyr.tidyverse.org/reference/dplyr_tidy_select.html] to choose variables. If replicate weight variables need to be specified with a character vector, use the `all_of` function to select variables from a character vector. Some other methods are also illustrated below in the examples but these apply to any type of replicate weights and not just BRR.

If a dataset had WT0 for the main weight and had 20 BRR weights indicated WT1, WT2, ..., WT20, use the following syntax (both are equivalent):

```r
des_brr <- dat %>%
  as_survey_rep(weights = WT0, repweights= all_of(str_c("WT", 1:20)),
                type="BRR", mse=TRUE)

des_brr <- dat %>%
  as_survey_rep(weights = WT0, repweights= num_range("WT", 1:20),
                type="BRR", mse=TRUE)

```

If a dataset had WT for the main weight and had 20 BRR weights indicated REPWT1, REPWT2, ..., REPWT20, the following syntax could be used (both are equivalent):

```r
des_brr <- dat %>%
  as_survey_rep(weights = WT, repweights= all_of(str_c("REPWT", 1:20)),
                type="BRR", mse=TRUE)

des_brr <- dat %>%
  as_survey_rep(weights = WT, repweights= starts_with("REPWT"),
                type="BRR", mse=TRUE)
```

If the replicate weight variables are on the file consecutively, the following syntax can also be used:

```r
des_brr <- dat %>%
  as_survey_rep(weights = WT, repweights= REPWT1:REPWT20, type="BRR",
                mse=TRUE)
```

Typically, the replicate weights sum to a value similar to the main weight as they are both supposed to provide population estimates. Rarely, an alternative method will be used where the replicate weights have values of 0 or 2 in the case of BRR weights. This would be indicated in the documentation and in Section \@ref(und-surv-doc), we discuss how to understand documentation. In this case, the replicate weights are not combined and the option `combined_weights = FALSE` should be indicated, as the default value for this argument is TRUE. This specific syntax is shown below:

```r
des_brr <- dat %>%
  as_survey_rep(weights = WT, repweights= starts_with("REPWT"), 
                type="BRR", combined_weights = FALSE, mse=TRUE)
```

#### Example {-} 

The {survey} package includes a data example from Section 12.2 of @levy2013sampling. In this fictional data, two out of five ambulance stations were sampled from each of three emergency service areas (ESAs) thus BRR weights are appropriate with 2 PSUs (stations) sampled in each stratum (ESA). In the code below, BRR weights are created as was done in @levy2013sampling.

```{r}
#| label: brr_display
data(scd)
scdbrr <- scd %>%
  as_tibble() %>%
  mutate(
    wt=5/2,
    rep1 = 2 * c(1, 0, 1, 0, 1, 0),
    rep2 = 2 * c(1, 0, 0, 1, 0, 1),
    rep3 = 2 * c(0, 1, 1, 0, 0, 1),
    rep4 = 2 * c(0, 1, 0, 1, 1, 0))

scdbrr
```

To specify the BRR weights, the following syntax is used:

```{r}
#| label: brr_des
des_brr_scd <- scdbrr %>%
  as_survey_rep(type = "BRR", repweights = starts_with("rep"),
                combined_weights = FALSE, weight=wt)

des_brr_scd

summary(des_brr_scd)
```

Note that `combined_weights` was specified as `FALSE` because these weights are simply specified as 0 and 2 and do not incorporate the overall weight. When printing the object, the type of replication is noted as Balanced Repeated Replicates, the replicate weights are specified, and the weight variable. When looking at the summary, the only additional information provided are the variables included.

### Fay's BRR Method

Fay's BRR method for replicate weights still uses a Hadamard matrix to construct replicate weights but rather than deleting PSUs for each replicate, half of the PSUs have a replicate weight which is the main weight multiplied by $\rho$ and the other half have the main weight multiplied by $(2-\rho)$ where $0 \le \rho < 1$. Note that when $\rho=0$, this is equivalent to the standard BRR weights and as $\rho$ becomes closer to 1, this method is more similar to jackknife discussed in the next section. To obtain the value of $\rho$, it is necessary to read the documentation as discussed in Section \@ref(und-surv-doc).

#### The math {-}

The standard error estimate for $\hat{\theta}$ is slightly different and calculated as:

$$se(\hat{\theta})=\sqrt{\frac{1}{R (1-\rho)^2} \sum_{r=1}^R \left( \hat{\theta}_r-\hat{\theta}\right)^2}$$

#### The syntax {-} 

The syntax is very similar for BRR and Fay's BRR. If a dataset had WT0 for the main weight and had 20 BRR weights indicated WT1, WT2, ..., WT20, and Fay's multiplier is 0.5, use the following syntax:

```r
des_fay <- dat %>%
  as_survey_rep(weights = WT0, repweights= num_range("WT", 1:20),
                type="Fay", mse=TRUE, rho=0.5)
```

#### Example {-} 

The 2015 RECS uses Fay's BRR weights with the final weight as NWEIGHT and replicate weights as BRRWT1 - BRRWT96 with $\rho=0.5$^[Using the 2015 microdata file to compute estimates and standard errors (RSEs):  https://www.eia.gov/consumption/residential/data/2015/pdf/microdata_v3.pdf]. On the file, DOEID is a unique identifier for each respondent, TOTALDOL is the total cost of energy, TOTSQFT_EN is the total square footage of the residence, and REGOINC is the Census region.

To specify this design, use the following syntax:

```{r}
#| label: recs
#| eval: TRUE
#| cache: TRUE
recs_in <- read_csv(
  here::here("RawData", "RECS_2015", "recs2015_public_v4.csv"))

des_recs <- recs_in %>%
  as_survey_rep(
    weights=NWEIGHT, 
    repweights=BRRWT1:BRRWT96, 
    type="Fay",
    rho=0.5, 
    mse=TRUE, 
    variables=c(DOEID, TOTALDOL, TOTSQFT_EN, REGIONC))

des_recs

summary(des_recs)
```

In specifying the design, the `variables` option was also used to include which variables might be used in analyses. This is optional but can make your object smaller. When printing the design object or looking at the summary, the replicate weight type is re-iterated as `Fay's variance method (rho= 0.5) with 96 replicates and MSE variances` and the variables are included. No weight or probability summary is included as was done in some other design objects.

### Jackknife method

There are three jackknife estimators implemented in {srvyr} - Jackknife 1 (JK1), Jackknife n (JKn), and Jackknife 2 (JK2). The JK1 method can be used for unstratified designs and replicates are created by removing one PSU at a time so the number of replicates is the same as the number of PSUs. If there is no clustering, then the PSU is the ultimate sampling unit (e.g., unit). 

The JKn method is used for stratified designs and requires 2 or more PSUs per stratum. In this case, each replicate is created by deleting one PSU from each stratum so the number of replicates is the number of total PSUs across all strata. The JK2 method is a special case of JKn when there are exactly 2 PSUs sampled per stratum. For variance estimation, scaling constants must also be specified.

#### The math {-}

For the JK1 method, the standard error estimate for $\hat{\theta}$ is calculated as:

$$se(\hat{\theta})=\sqrt{\frac{R-1}{R} \sum_{r=1}^R \left( \hat{\theta}_r-\hat{\theta}\right)^2}$$
The JKn method is a bit more complex but the coefficients are generally provided with restricted and public use files. For each replicate, one stratum has a PSU removed and the weights are adjusted by $n_h/(n_h-1)$ where $n_h$ is the number of PSUs in the stratum. The coefficients in other strata are set to 1. Denote the coefficient that results from this process for replicate $r$ as $\alpha_r$ then the standard error estimate for $\hat{\theta}$ is calculated as:

$$se(\hat{\theta})=\sqrt{\sum_{r=1}^R \left(\alpha_r \hat{\theta}_r-\hat{\theta}\right)^2}$$

#### The syntax {-}

To specify the Jackknife method, the type would be `JK1`, `JKn`, or `JK2`. Additionally, the overall multiplier for JK1 is specified with the scale argument, whereas the replicate specific multiplier ($\alpha_r) is specified with the rscales argument.

Consider a case for the JK1 method where the multiplier, $(R-1)/R=19/20=0.95$ and the dataset had WT0 for the main weight and had 20 JK1 weights indicated WT1, WT2, ..., WT20, then the syntax would be

```r
des_jk1 <- dat %>%
  as_survey_rep(weights = WT0, repweights= num_range("WT", 1:20),
                type="JK1", mse=TRUE, scale=0.95)
```

Consider a case for the JKn method where $\alpha_r=0.1$ for all replicates and the dataset had WT0 for the main weight and had 20 JK1 weights indicated WT1, WT2, ..., WT20, then the syntax would be:

```r
des_jkn <- dat %>%
  as_survey_rep(weights = WT0, repweights= num_range("WT", 1:20),
                type="JKN", mse=TRUE, rscales=rep(0.1, 20))
```

#### Example {-}

The American Community Survey releases public use microdata with JK1 weights at the person and household level. This example includes data at the household level where the replicate weights are specified as WGTP1, ..., WGTP80 and the main weight is WGTP^[American Community Survey 2016-2020 5-Year PUMS File: https://www2.census.gov/programs-surveys/acs/tech_docs/pums/ACS2016_2020_PUMS_README.pdf]. Using the {tidycensus} package^[tidycensus package: https://walker-data.com/tidycensus/], data is downloaded from the Census API. This request gets data for each person in each household in two Public Use Microdata Areas (PUMAs) in Durham County, NC^[Public Use Microdata Areas in North Carolina: https://www.census.gov/geographies/reference-maps/2010/geo/2010-pumas/north-carolina.html]. The variables requested are NP (number of persons in household), BDSP (number of bedrooms),  HINCP (household income), and TYPEHUGQ (type of household). By default, several other variables will come along including SERIALNO (a unique identifier for each household), SPORDER (a unique identifier for each person within each household), PUMA, ST (state), person weight (PWGTP), and the household weights (WGTP, WGTP1, ..., WGTP80). Filtering to records where SPORDER=1 yields only one record per household and TYPEHUGQ=1 filters to only households and not group quarters.

```{r}
#| label: acsexamp
#| cache: TRUE
#| results: 'hide'
library(tidycensus)

pums_in <- get_pums(variables=c("NP", "BDSP", "HINCP"), state="37", 
                    puma=c("01301", "01302"), rep_weights = "housing", 
                    year=2020, survey="acs5",
                    variables_filter=list(SPORDER=1, TYPEHUGQ=1))
```

```{r}
#| label: acsexampcont
#| cache: TRUE
#| dependson: 'acsexamp'
pums_in

des_acs <- pums_in %>%
    as_survey_rep(weights = WGTP, repweights= num_range("WGTP", 1:80),
                type="JK1", mse=TRUE, scale=4/80)

des_acs

summary(des_acs)
```

When printing the design object or looking at the summary, the replicate weight type is re-iterated as `Unstratified cluster jacknife (JK1) with 80 replicates and MSE variances` and the variables are included. No weight or probability summary is included as was done in some other design objects.

### Bootstrap method

In bootstrap resampling, replicates are created by selecting random samples of the PSUs with replacement. If there are $M$ PSUs in the sample, then each replicate will be created by selecting a random sample of $M$ PSUs with replacement. Each replicate is created independently and the weights for each replicate are adjusted to reflect the population, generally using the same method as how the analysis weight was adjusted. 

#### The math {-}

A weighted estimate for the full sample is calculated as $\hat{\theta}$ and then a weighted estimate for each replicate is calculated as $\hat{\theta}_r$ for $R$ replicates. Then the standard error of the estimate is calculated as:

$$se(\hat{\theta})=\sqrt{\alpha \sum_{r=1}^R \left( \hat{\theta}_r-\hat{\theta}\right)^2}$$

#### The syntax {-}

If a dataset had WT0 for the main weight, 20 bootstrap weights indicated WT1, WT2, ..., WT20, and $\alpha=.02$, use the following syntax:

```r
des_bs <- dat %>%
  as_survey_rep(weights = WT0, repweights= num_range("WT", 1:20),
                type="bootstrap", mse=TRUE, scale=.02)

```

Note that the scale is usually provided in documentation and is a constant, so is not provided as a variable in the tibble.

#### Example {-}

Returning to the api example, bootstrap weights were constructed for a one cluster design. 50 replicate weights were created on a dataset `apiclus1_slim` which has some familiar variables including cds, dnum, fpc, and pw but now additionally includes bootstrap weights pw1, ..., pw50. The scale $(\alpha)$ is $15/(14*49)=0.02186589$

```{r}
#| label: genbs
#| include: FALSE
apiclus1_slim <- 
  apiclus1 %>%
  as_tibble() %>%
  arrange(dnum) %>%
  select(cds, dnum, fpc, pw)

set.seed(662152)
apibw <- bootweights(psu=apiclus1_slim$dnum, strata=rep(1, nrow(apiclus1_slim)), fpc=apiclus1_slim$fpc, replicates=50)

bwmata <- apibw$repweights$weights[apibw$repweights$index, ]*apiclus1_slim$pw

apiclus1_slim <- bwmata %>%
  as.data.frame() %>%
  set_names(str_c("pw", 1:50)) %>%
  cbind(apiclus1_slim) %>%
  as_tibble() %>%
  select(cds, dnum, fpc, pw, everything())
```

```{r}
#| label: bsexamp
apiclus1_slim

des_api1_bs <- apiclus1_slim %>% 
  as_survey_rep(weights=pw, repweights=pw1:pw50, type="bootstrap", 
                scale=0.02186589, mse=TRUE)

des_api1_bs

summary(des_api1_bs)
```

As with other replicate design objects, when printing the object or looking at the summary, the replicate weights are provided along with the data variables.

## Understanding survey design documentation {#und-surv-doc}

SRS, stratified, and clustered designs are the backbone of sampling designs and the features are often combined in one design. Additionally, rather than using SRS for selection, other sampling mechanisms are commonly used such as probability proportional to size (PPS), systematic sampling, or selection with unequal probabilities which are briefly described here. In PPS sampling, a size measure is constructed for each unit - perhaps the population of the PSU or the number of occupied housing units, and then units with larger size measures are more likely to be sampled. Systematic sampling is commonly used to ensure representation across a population. Units are sorted by a feature and then every $k$ units are selected from a random start point so the sample is spread across the population. In addition to PPS, other unequal probabilities of selection may be used. As an example, in a study of establishments that conducts a survey every year, an establishment that recently participated (e.g., participated last year) has a reduced chance of selection in a subsequent round to reduce the burden on the establishment. To learn more about sampling designs, refer to @valliant2013practical, @cox2011business, @cochran1977sampling, and @deming1991sample.

A common method of sampling is to stratify PSUs, select PSUs within stratum using PPS selection, and then select units within the PSUs either with SRS or PPS. Reading survey documentation is an important first step of survey analysis to understand the design and variables necessary to specify the design. Good documentation will highlight the variables necessary to specify the design. This is often found in User's Guides, methodology, analysis guides, or technical documentation.

For example, the 2017-2019 National Survey of Family Growth (NSFG)^[2017-2019 National Survey of Family Growth (NSFG): Sample Design Documentation -  https://www.cdc.gov/nchs/data/nsfg/NSFG-2017-2019-Sample-Design-Documentation-508.pdf] had a stratified multi-stage area probability sample. Counties or collections of counties were the primary sampling units which were stratified by Census region/division, size (population), and MSA status. Within each stratum, PSUs were selected via PPS. At the second stage, neighborhoods were selected within the sampled PSUs using PPS selection. At the third stage, housing units were selected within the sampled neighborhoods. At the fourth stage, a person was randomly chosen within the selected housing units among eligible persons using unequal probabilities based on person's age and sex. The public use file does not include all these levels of selection and instead includes pseudo-strata and pseudo-clusters which are the variables used in R to specify the design. As specified on page 4 of the documentation, the stratum variable is `SEST`, the cluster variable is `SECU`, and the weight variable is `WGT2017_2019`. Thus, to specify this design in R, one would use the following syntax:

```r
des_nsfg <- nsfgdata %>%
  as_survey_design(ids = SECU, strata = SEST, weights = WGT2017_2019)
```

## Exercises

<!-- For this chapter, the exercises entail reading public documentation to determine how to specify the survey design. While reading the documentation, be on the lookout for description of the weights and the survey design variables or replicate weights. -->

1. The American National Election Studies (ANES) collect data before and after elections approximately every 4 years around the presidential election cycle. Each year with the data release, a user's guide is also released^[ANES 2020 User's Guide: https://electionstudies.org/wp-content/uploads/2022/02/anes_timeseries_2020_userguidecodebook_20220210.pdf]. What is the syntax for specifying the analysis of the full sample post-election data?

```{r}
#| label: anes-ex
#| eval: FALSE
svy_anes <- anes_data %>%
  as_survey_design(weight)
```

2. The General Social Survey is a survey that has been administered since 1972 on social, behavioral, and attitudinal topics. The 2016-2020 GSS Panel codebook^[2016-2020 GSS Panel Codebook Release 1a: https://gss.norc.org/Documents/codebook/2016-2020%20GSS%20Panel%20Codebook%20-%20R1a.pdf] provides examples of setting up syntax in SAS and Stata but not R. How would you specify the design in R?

```{r}
#| label: gss-ex
#| eval: FALSE
svy_gss <- gss_data %>%
  as_survey_design(ids = VPSU_2, strata = VSTRAT_2, weights = WTSSNR_2)
```


<!--chapter:end:05-specifying-sample-designs.Rmd-->

# Descriptive analyses in srvyr {#c06}

<!--chapter:end:06-descriptive-analysis-in-srvyr.Rmd-->

# Statistical testing {#c07}

When analyzing results from a survey, the point estimates described in Chapter \@ref(c06) are helpful for understanding the data at a high level, but researchers and the public often want to make comparisons between different groups. These comparisons are calculated through statistical testing where we compare the point estimates and the variance estimates of each statistic to see if there are statistically significant differences. The general idea of statistical testing is the same for data obtained through surveys and data obtained through other methods, but the importance lies in ensuring the variance is calculated correctly. Functions in the {survey} packages allow for the correct estimation of the variances. This chapter will cover the following statistical tests with survey data and functions:

-  Comparison of proportions `svyttest()`
-  Comparison of means `svyttest()`
-  Goodness of fit tests `svygofchisq()`
-  Tests of independence `svychisq()`
-  Tests of homogeneity `svychisq()`

Up to this point, the functions that we've provided have used the wrappers from the {srvyr} package. This means that the functions work with tidyverse syntax. However, the functions in this chapter do not have wrappers from the {srvyr} package and are instead used directly from the {survey} package. This means that the design object is *not* the first argument and that to use these functions with the magrittr pipe `%>%` and tidyverse syntax we will need to use dot (`.`) notation. Functions that work with the magrittr pipe `%>%` have the first argument as data. When data is run, it automatically places anything to the left of the pipe into the first argument of the function to the right of the pipe. For example, if we wanted to take the `mtcars` data and filter to only cars with 6 cylinders we can write the code in one of three ways:

1. `filter(mtcars, cyl == 6)`
2. `mtcars %>% filter(cyl == 6)`
3. `mtcars %>% filter(., cyl == 6)`
4. `mtcars %>% filter(.data= ., cyl == 6)`

Each of these lines of code will produce the same output since the argument that takes the data is in the first spot in `filter()`. Those who have worked with the tidyverse, the first two are probably familiar. The third option functions the same way as the second one, but is explicit that `mtcars` goes in the first argument and the fourth option indicates that mtcars is going into the named argument of `.data`. Here we're telling R to take what's on the left side of the pipe `mtcars` and pipe it into the spot with the dot (`.`)---the first argument.

In functions that are not part of the tidyverse, the data argument may be in a different spot in the functions. For example, in `svyttest()` the data argument is in the second spot, which means we need to place the dot (`.`) in the second spot and not the first. For example:

```
svydata_des %>% 
  svyttest(x~y, .)
```


Placing the dot (`.`) in the second argument spot, indicates that the survey design object `svydata_svy` should be used in the second argument and not the first (which is the default). 

Alternatively, named arguments could be used to place the dot first as in the following:

```
svydata_des %>% 
  svyttest(design = ., x~y)
```
## Chapter Set-Up {#stattest-setup}

For this chapter, we will be using the same data as we did in \@ref(c06): ANES and RECS. As a reminder, we will need to create survey design objects to work with. These design objects ensure that the variance estimation is calculated accurately, and thus we can accurately determine statistical significance.

First, make sure install and load the following packages:
```{r stattest-pkgs}
#| error: FALSE
#| warning: FALSE
#| message: FALSE

library(tidyverse)
library(survey) # for survey analysis
library(srvyr) # for tidy survey analysis
library(readr)
library(here)
library(gt) # for output of readable tables
```

Second, we need to read in the data and create the design objects.

Here is how to create the design object for the ANES data, remember that we need to adjust the weight so it sums to the population instead of the sample. We do that by multiplying the weights (see the ANES methodology documentation for more information).
```{r stattest-anesdes}
anes <- read_rds(here("AnalysisData", "anes_2020.rds")) %>%
  mutate(Weight=Weight/sum(Weight)*231592693) 

anes_des <- anes %>%
  as_survey_design(weights = Weight,
          strata = Stratum,
          ids = VarUnit,
          nest = TRUE)
```

Here is how to create the design object for the RECS data:
```{r stattest-recsdes}
recs <-read_rds(here("AnalysisData", "recs_2015.rds"))

recs_des <- recs %>%
 as_survey_rep(weights = NWEIGHT,
        repweights = starts_with("BRRWT"),
        type = "Fay",
        rho = 0.5,
        mse = TRUE)
```

## Comparison of Proportions and Means {#stattest-ttest}

To compare two proportions or means, we use t-tests. This allows us to determine if one proportion or mean is statistically different from the other. T-tests are commonly used to determine if a single estimate is different from a known value (e.g., 0 or 50%) or to compare two group means (e.g., males vs females). For comparing a single estimate to a known value, this is called a *one sample t-test* and we can set up the hypothesis test as:

- $H_0: \mu = 0$ where $\mu$ is the is the mean outcome and $0$ is the value we are comparing it to
- $H_A: \mu \neq 0$

For comparing two estimates, this is called a *two sample t-test* and we can set up the hypothesis test as:

- $H_0: \mu_1 = \mu_2$ where $\mu_i$ is the is the mean outcome for group $i$
- $H_A: \mu_1 \neq \mu_2$

Two sample t-tests can also be *paired* or *unpaired*. If the data come from two different populations (e.g., males vs females), then the t-test run will be an *unpaired* or *independent samples* t-test. *Paired* t-tests occur when the data come from the same population. This is commonly seen with data taken from the same population in two different time periods (e.g., before and after an intervention). 

The difference between using t-tests with non-survey data and with survey data is based on the underlying variance estimation difference. Chapter \@ref(c05) provides the detailed overview of the math behind the mean and sampling error calculations for various sample designs. The functions in the {survey} package will account for these nuances provided the design object is correctly defined.  

### Syntax {#stattest-ttest-syntax}

When we do not have survey data, we may be able to use the `t.test()` function. This function does not allow for weights or the variance structure to be accounted for with survey data. Therefore, when using survey data, we need to use the `svyttest()` function. Many of the arguments are the same between the two functions, but there are a two key differences:

- We need to use the survey design object, instead of data
- We can only use a formula and not separate x and y data

Here is the syntax for the `svyttest()` function:

```
svyttest(formula, 
         design, 
         na.rm=FALSE,
         level=0.95,
         ...)
```

Notice that the first argument here is the `formula` and not the `design`. This means that we must use the dot `(.)` if we pipe in the survey design object (as described at the beginning of this chapter). 

The `formula` argument can take on a couple of different forms depending on what it is that we are measuring.  Here are a few common scenarios:

1. **One-sample t-test:** 
    a. **Comparison to 0:** `var ~ 0`, where `var` is the measure of interest and `0` is the value we are comparing it to. For example, we could test if the proportion of the population that has blue eyes is different from `0`.
    b. **Comparison to a different value:** `I(var - value) ~ 0`, where `var` is the measure of interest and `value` is what we are comparing to. We need to use the `I()` function, to tell the program to calculate the difference between the variable and the comparison value prior to testing. For example, we could test if the proportion of the population that has blue eyes is different from `25%` by using `I(var - 0.25)~0`.
2. **Two-sample t-test:**
    a. **Unpaired:**
        - **2 level grouping variable:** `var ~ groupVar`, where `var` is the measure of interest and `groupVar` is a variable with two categories.  For example, we could test if the proportion of the population that has blue eyes is different for children aged 5-10 years old compared to children under 5 years old. 
        - **3+ level grouping variable:** `var ~ I(groupVar == level)`, where `var` is the measure of interest, `groupVar` is the categorical variable, and `level` is the category level to isolate. Again we need to use the `I()` function to tell the program to isolate the category before doing the comparison across groups.  For example, we could test if the test scores in one classroom differed from all other classrooms.
    b. **Paired:** `I(var_1 - var_2) ~ 0`, where `var_1` is the first variable of interest and `var_2` is the second variable of interest. We again will have to use the `I()` function to have the program calculate the difference between the two variables before comparing it against `0`.  For example, we could test if test scores on a subject differed between the start and the end of a course.

Additionally, the `na.rm` argument defaults to `FALSE`, which means if any data is missing the t-test will not compute. Throughout this chapter we will always set `na.rm=TRUE`, but before analyzing the survey data, make sure to review the notes provided in Chapter \@ref(c03) to better understand how to handle missing data.  Finally, the `level` argument is $1-\alpha$, or the amount of type 1 error.  The default is $0.95$.

### Examples {#stattest-ttest-examples}
Let's walk through a few examples using the ANES and RECS data.  See Section \@ref(stattest-setup) above to set up the design objects.

#### Example 1: One-sample t-test {.unnumbered #stattest-ttest-ex1}
<!--Would love to do a comparison to 0 for this example, but have changed it for now.  Thoughts?-->

ANES asked respondents if they voted for president in the 2020 election.  In our data, we've called this variable `VotedPres2020`. Let's look at the proportion of the U.S. voting eligible population that voted for president in 2020 using the `survey_prop()` function we learned in Chapter \@ref(c06). 

```{r}
#| label: stattest-ttest-voteprop

voteprop<-anes_des %>% 
  group_by(VotedPres2020) %>% 
  summarize(survey_prop())

voteprop

```

Based on this, we see that `r signif(voteprop*100,3)`% of the U.S. voting eligible population voted for president in 2020. If we wanted to know how this compares to other countries, we can use `svyttest()`.  For example, if we know that the voter turnout in Germany in the 2017 general election was 76.2%, we could set up our hypothesis as:

- $H_0: p = 0.762$ where $p$ is the proportion of U.S. voting eligible population that voted for president in 2020
- $H_A: p \neq 0.762$

To conduct this in R, we would then use the `svyttest()` function:
```{r}
#| label: stattest-ttest-ex1
ttest_ex1<-anes_des %>%
   svyttest(formula=I(I(VotedPres2020=="Yes")-0.762)~0,
            design=.,
            na.rm=TRUE)

ttest_ex1
```

Note that because `VotedPres2020` is a factor, we need to specify which level we are interested in.  In this case we want to isolate those that did vote for president in 2020.  The output from the `svyttest()` function can be a bit hard to read.  Using the {broom} package from the tidyverse we can clean up the output into a tibble to more easily what the test is telling us.
```{r}
#| label: stattest-ttest-ex1-broom
broom::tidy(ttest_ex1)
```

The estimate that is presented here is actually the difference between the U.S. proportion and the Germany proportion we are comparing to.  We can see that there is a difference of `r signif(ttest_ex1$estimate*100,3)` percentage points.  Additionally, we can see the t-statistic value in the `statistic` column is `r signif(ttest_ex1$statistic,3)` and the p-value is `r signif(ttest_ex1$p.value,3)`. These results indicate that the U.S. and Germany have similar voter turnout.

<!--Add in callout box about how to use the $ notation to help call out the different values?  Maybe indicate how this will be covered more in the reporting chapter?-->


#### Example 2: One-sample t-test {.unnumbered #stattest-ttest-ex2}

RECS asks respondents to indicate what temperature they set their house to during the summer at night.  In our data, we've called this variable `SummerTempNight`.  If we want to see if the U.S. household sets their temperature at a value different from 68$^\circ$F, we could set up the hypothesis as follows:

- $H_0: \mu = 68$ where $\mu$ is the is average temperature U.S. Households set their thermostat to in the summer at night
- $H_A: \mu \neq 68$

To conduct this in R, we would then use the `svyttest()` function, and the `I()` function in the formula:
```{r}
#| label: stattest-ttest-ex2
ttest_ex2<-recs_des %>%
   svyttest(formula=I(SummerTempNight-68)~0,
            design=.,
            na.rm=TRUE)

ttest_ex2
```

The estimate in this case differs from example one in that the estimate is not displaying $\mu$ but rather $\mu - 68$.  If we wanted the average we could do one of the following:
```{r}
#| label: stattest-ttest-ex2-svymean
recs_des %>% summarize(mu=survey_mean(SummerTempNight,na.rm=TRUE))
```

Or, we could take our t-test estimate (`ttest_ex2$estimate`) and add it to 68: 
```{r}
#| label: stattest-ttest-ex2-add
ttest_ex2$estimate + 68
```

The result is the same in both methods, so we see that the average temperature U.S. Households set their thermostat to in the summer at night is `r round(ttest_ex2$estimate + 68,1)`.  Looking at the output from the `svyttest()`, the t-statistic is `r ttest_ex2$statistic` and the p-value is `r ttest_ex2$p.value`, indicating that the average is statistically different from 68$^\circ$F at an $\alpha$ level of $0.05$.


#### Example 3: Unpaired two-sample t-test {.unnumbered #stattest-ttest-ex3}

Two additional variables we have on the RECS data are the electric bill cost (`DOLLAREL`) and whether the house used AC or not (`ACUsed`).  If we want to know if the U.S. households that used AC had higher electrical bills compared to those that did not, we could set up the hypothesis as follows:

- $H_0: \mu_{AC} = \mu_{noAC}$ where $\mu_{AC}$ is the electrical bill cost for U.S. households that used AC and $\mu_{noAC}$ is the electrical bill cost for U.S. household that did not use AC
- $H_A: \mu_{AC} \neq \mu_{noAC}$
 
Let's take a quick look at the data to see the format the data are in:
```{r}
#| label: stattest-ttest-ex3-desc
recs_des %>% 
  group_by(ACUsed) %>% 
  summarize(mean=survey_mean(DOLLAREL,na.rm=TRUE))
```

To conduct this in R, we would then use `svyttest()`:
```{r stattest-ttest-ex3}
#| label: stattest-ttest-ex3
ttest_ex3<-recs_des %>%
   svyttest(formula=DOLLAREL~ACUsed,
            design=.,
            na.rm=TRUE)

ttest_ex3
```
The results indicate that the difference in electrical bill for those that used AC and those that did not is on average \$`r round(ttest_ex3$estimate,2)`. The difference does appear to be statistically significant as the t-statistic is `r ttest_ex3$statistic` and the p-value is `r ttest_ex3$p.value`.

#### Example 4: Paired two-sample t-test {.unnumbered #stattest-ttest-ex4}

To conduct a paired t-test that looks at differences at two time points, we use the same `I()` notatation we've been using.  For example, let's say we want to test whether the temperature that U.S. households set their thermostat to differs depending on the season (comparing summer temperature and winter temperature).  We could set up the hypothesis as follows:

- $H_0: \mu_{summer} = \mu_{winter}$ where $\mu_{summer}$ is the temperature that U.S. households set their thermostat to during summer nights, and $\mu_{winter}$ is the temperature that U.S. households set their thermostat to during winter nights
- $H_A: \mu_{summer} \neq \mu_{winter}$

To conduct this in R, we would then use `svyttest()` and `I()`:
```{r}
#| label: stattest-ttest-ex4
ttest_ex4<-recs_des %>%
   svyttest(design=.,
            formula=I(SummerTempNight-WinterTempNight)~0,
            na.rm=TRUE)

ttest_ex4
```

U.S. households set their thermostat on average `r round(ttest_ex4$estimate,1)`$^\circ$F warmer in summer nights than winter nights, and it is statistically significant (t=`r ttest_ex4$statistic`, p-value=`r ttest_ex4$p.value`).

### Exercises {#stattest-ttest-exercises}

Here are some exercises for practicing conducting t-tests using `svyttest()`:

1. Using the RECS data, do more than 50% of U.S. household use AC (`ACUsed`)?
```{r}
#| label: stattest-ttest-solution1
ttest_solution1<-recs_des %>%
   svyttest(design=.,
            formula=I((ACUsed==TRUE)-0.5)~0,
            na.rm=TRUE)

ttest_solution1
```
2. Using the RECS data, does the average temperature that U.S. households set their thermostats to differ between the day and night in the winter (`WinterTempDay` and `WinterTempNight`)?
```{r}
#| label: stattest-ttest-solution2
ttest_solution2<-recs_des %>% 
  svyttest(design=.,
           formula=I(WinterTempDay-WinterTempNight)~0,
           na.rm=TRUE)

ttest_solution2
```
3. Using the ANES data, does the average age (`Age`) of those who voted for Biden in 2020 (`VotedPres2020_selection`) differ from those that voted for another candidate?
```{r}
#| label: stattest-ttest-solution3
ttest_solution3<-anes_des %>%
   svyttest(design=.,
            formula=Age~I(VotedPres2020_selection=="Biden"),
            na.rm=TRUE)

ttest_solution3
```

## Chi-Square Tests {#stattest-chi}

Chi-square tests ($\chi^2$) allow us to examine multiple proportions using goodness-of-fit test, test of independence, or test of homogeneity. All three of these tests the same $\chi^2$ distributions but with slightly different underlying assumptions.

First, **goodness-of-fit** tests are used when comparing *observed* data to *expected* data. For example, this could be used to determine if respondent demographics (the observed data) match known population information (the expected data). In this case, we can set up the hypothesis test as:

- $H_0: p_1 = \pi_1, ~ p_2 = \pi_2, ~ ..., ~ p_k = \pi_k$ where $p_i$ is the observed proportion for category $i$, $\pi_i$ is expected proportion for category $i$, and $k$ is the number of categories
- $H_A:$ at least one level of $p_i$ does not match $\pi_i$

Second, **tests of independence** are used when comparing two types of *observed* data to see if there is a relationship. For example, this could be used to determine if the proportion of respondents who voted for each political party in the Presidential election matches the proportion of respondents who voted for each political party in a local election. In this case, we can set up the hypothesis test as:

- $H_0:$ The two variables/factors are independent
- $H_A:$ The two variables/factors are *not* independent 

Third, **tests of homogeneity** are used to compare two distributions to see if they match. For example, this could be used to determine if the highest education achieved is the same for both men and women.  In this case, we can set up the hypothesis test as:

- $H_0: p_{1a} = p_{1b}, ~ p_{2a} = p_{2b}, ~ ..., ~ p_{ka} = p_{kb}$ where $p_{ia}$ is the observed proportion of category $i$ for subgroup $a$, $p_{ib}$ is the observed proportion of category $i$ for subgroup $a$, and $k$ is the number of categories
- $H_A:$ at least one category of $p_{ia}$ does not match $p_{ib}$

The difference between using $\chi^2$ tests with non-survey data and with survey data is based on the underlying variance estimation difference. The functions in the {survey} package will account for these nuances provided the design object is correctly defined. For basic variance estimation formulas for different survey design types, refer to Chapter \@ref(c05). 

### Syntax {#stattest-chi-syntax}

As with t-tests, when we do not have survey data, we may be able to use the `chisq.test()` function.  However, this function does not allow for weights or the variance structure to be accounted for with survey data. Therefore, when using survey data, we need to use one of two functions:

-  `svygofchisq()`: For goodness of fit tests
-  `svychisq()`: For tests of independence and homogeneity

The non-survey data function of `chisq.test()` requires either a single set of counts and given proportions (for goodness of fit tests), or two sets of counts for tests of independence and homogeneity.  The functions that we use with survey data require respondent level data and formulas instead of counts.  This ensures that the variances are correctly calculated.

First, the function for the goodness of fit tests is `svygofchisq()`:

```
svygofchisq(formula,
            p,
            design, 
            na.rm=TRUE,
            ...)
```

In this function, you'll notice that the first argument is the `formula`, the second argument is `p` which are the expected proportions, and the third argument is the `design`.  Therefore, we again must use the dot `(.)` notation if we pipe in the survey design object (as described at the beginning of this chapter).  For the goodness of fit tests, the formula will be a single variable `formula=~VARIABLE` as we are comparing the observed data from this variable to expected data.  The expected probabilities are then entered in the `p` argument, and needs to be a vector of the same length as the number of categories in the variable.  For example, if we want to know if the proportion of males and females match a distribution of 30/70, then the sex variable (with 2 categories) would be used in the formula `formula=~SEX` and the proportions would be included as `p=c(.3,.7)`.  It is important to note that the variable entered into the formula should be formatted as either a factor or a character.  The examples below provide more detail and tips on how to make sure the levels are matching up correctly.

The function for tests of independence and homogeneity (`svychisq()`) is similar to the goodness of fit function in that the `formula` argument is first.  However, instead of an argument for the expected proportions, the `svychisq()` function has an argument to select the statistic used for the test:

```
svychisq(formula,
         design, 
         statistic = c("F",  "Chisq", "Wald", "adjWald", 
                       "lincom", "saddlepoint"),
         na.rm=TRUE,
         ...)
```
There are 6 statistics that are accepted in this formula.  For tests of homogeneity (when comparing cross tabulations) the `F` or `Chisq` statistics should be used.^[These two statistics can also be used for goodness of fit tests, if the `svygofchisq()` function is not used.]  The `F` statistic is the default and uses the Rao-Scott second-order correction.  This correction is designed to assist with complicated sampling design (i.e., those other than a simple random sample) (CITE)^[http://www.asasrms.org/Proceedings/y2007/Files/JSM2007-000874.pdf].  The `Chisq` statistic is an adjusted version of the Pearson $\chi^2$ statistic. The version of this statistic in the `svychisq()` function compares the design effect estimate from the provided survey data to what the $\chi^2$ distribution would have been if the data came from a simple random sampling.For tests of independence, the `Wald` and `adjWald` are recommended as they provide a better adjustment for variable comparisons.  If the data has a small number of primary sampling units (PSUs) in comparison to the degrees of freedom, then the `adjWald` statistic should be used to account for this.  The `lincom` and `saddlepoint` statistics are available for more complicated data structures.<!--I'm struggling to find a lot of information around these last two statistics, any thoughts/suggestions?-->

The formula argument will always be one sided unlike the `svyttest()` function. The two variables of interest should be included with a plus sign: `formula=~VAR1+VAR2`. As with the `svygofchisq()` function, the variables entered into the formula should be formatted as either a factor or a character. 

Additionally, as with the t-test function, both `svygofchisq()` and `svychisq()` have the `na.rm` argument.  This argument defaults to `FALSE`, however, unlike the t-test function if any data is missing the $\chi^2$ tests will assume that `NA` is a category and will include it in the calculation. Throughout this chapter we will always set `na.rm=TRUE`, but before analyzing the survey data, make sure to review the notes provided in Chapter \@ref(c03) to better understand how to handle missing data.


### Examples {#stattest-chi-examples}

Let's walk through a few examples using the ANES data.  See Section \@ref(stattest-setup) above to set up the design object.

#### Example 1: Goodness of Fit Test {.unnumbered #stattest-chi-ex1}

ANES asked respondents their highest education level.  Based on the data from the 2020 American Community Survey (ACS) 5-year estimates^[Data was pulled from data.census.gov using the S1501 Education Attainment 2020: ACS 5-Year Estimates Subject Tables], the education distribution of those 18+ in the U.S. is as follows:
-  11% had less than High School degree
-  27% had a High School degree
-  29% had some college or associate's degree
-  33% had a bachelor's degree or higher

If we want to see if the weighted distribution from the ANES 2020 data matches this distribution, we could set up the hypothesis as follows:

- $H_0: p_1 = 0.11, ~ p_2 = 0.27, ~ p_3 = 0.29, ~ p_4 = 0.33$
- $H_A:$ at least one of the education levels does not match between the ANES and the ACS

To conduct this in R, let's first take a look at the education variable (`Education`) we have on the ANES data.  Using the `survey_mean()` function discussed in Chapter \@ref(ch06), we can see the different levels of education we have and estimated proportion.

```{r}
#| label: stattest-chi-ex1-educmean

anes_des %>%
  group_by(Education) %>%
  filter(!is.na(Education)) %>%
  summarise(p=survey_mean())

```

Based on this output, we can see that we have different levels than the ACS data provides. Specifically, the education data from ANES has two levels for Bachelor's Degree or Higher (Bachelor's and Graduate), so these two categories need to be collapsed into a single category to match the ACS data.  We case use the {forcats} package and the `fct_collapse()` function to create a new variable to use. Then we will use the `svygofchisq()` function to compare the ANES data to the ACS data:

```{r}
#| label: stattest-chi-ex1
anes_des_educ<-anes_des %>% 
  mutate(Education2=
           fct_collapse(Education,
                        "Bachelor or Higher"=c("Bachelor's",
                                               "Graduate")))

chi_ex1<-anes_des_educ %>% 
  svygofchisq(formula=~Education2,
              p=c(0.11,0.27,0.29,0.33),
              design=.,
              na.rm=TRUE)

chi_ex1
```
The output from the `svygofchisq()` indicates that at least one proportion from ANES does not match the ACS data ($\chi^2=$`r chi_ex1$statistic`; $p-value=$`r chi_ex1$p.value`. To get a better idea of the differences, we can use the `expected` output along with `survey_mean()` to create a comparison table:
<!--This doesn't render correctly in HTML but does ok in PDF.-->

```{r}
#| label: stattest-chi-ex1-table
ex1_expected<-tibble(ExpectedCount=chi_ex1$expected) %>%
  mutate(Education=names(chi_ex1$expected),
         Education=str_sub(Education,11,nchar(Education)),
         ExpectedProb=ExpectedCount/sum(ExpectedCount)) %>% 
  select(Education,Expected=ExpectedProb)

ex1_observed<-anes_des_educ %>% 
  filter(!is.na(Education2)) %>% 
  group_by(Education2) %>% 
  summarize(Observed=survey_mean(vartype = "ci")) %>% 
  rename(Education=Education2)

ex1_table<-ex1_expected %>% 
  left_join(ex1_observed,by="Education") %>% 
  mutate(Education=factor(Education,
                          levels=c("Less than HS","High school",
                                   "Post HS","Bachelor or Higher")))

ex1_table
```

This output includes our expected proportions from the ACS that we provided the `svygofchisq()` function along with output of the observed proportions and their confidence intervals.  From this table we can see that the "High school" and "Post HS" categories have proportions that are nearly identical, but that the other two categories are slightly different.  Looking at the confidence intervals we can see that the ANES data skews to include fewer people in the "Less than HS" category and more people in the "Bachelor or Higher" category.  This may be easier to see in graphical form:
<!--Will want to remove the legend title...need to look this up.  Is there anyway to reduce the width of the error bars, the ends are really wide?-->
```{r}
#| label: stattest-chi-ex1-graph
ex1_table %>% 
  pivot_longer(cols=c("Expected","Observed"),
               names_to="Names",
               values_to="Proportion") %>%
  mutate(Observed_low=case_when(Names=="Observed"~Observed_low),
         Observed_upp=case_when(Names=="Observed"~Observed_upp)) %>%
  ggplot(aes(x=Education,y=Proportion,color=Names)) + 
    geom_point(alpha=0.5) + 
    geom_errorbar(aes(ymin=Observed_low,ymax=Observed_upp)) + 
    theme_bw()
```

#### Example 2: Test of Independence {.unnumbered #stattest-chi-ex2}

ANES asked respondents two questions about trust:
-  How often can you trust the federal government to do what is right?
-  How often can you trust other people?

If we want to see if the distributions of these two questions are similar or not we can conduct a test of independence.  Here is how the hypothesis could be set up:

- $H_0:$ People's trust in the federal government and their trust in other people are independent (i.e, *not* related)
- $H_A:$ People's trust in the federal government and their trust in other people are *not* independent (i.e, they are related)

To conduct this in R, we will use the `svychisq()` function to compare the two variables:
```{r}
#| label: stattest-chi-ex2
chi_ex2<-anes_des %>% 
  svychisq(formula=~TrustGovernment+TrustPeople,
           design=.,
           statistic="Wald",
           na.rm=TRUE)

chi_ex2
```
The output from `svychisq()` indicates that at the distribution of people's trust in the federal government and their trust in other people are *not* independent, meaning that they are related.  Let's out put the distributions in a table to see the relationship. The `observed` output from the test provides a cross tabulation of the counts for each category:
```{r}
#| label: stattest-chi-ex2-counts
chi_ex2$observed
```
However, as researchers, we often want to know about the proportions and not just the respondent counts from the survey.  There are a couple of different ways that you can do this.  The first is using the counts from `chi_ex2$observed` and calculating the proportion from that.  We can then pivot the table to create a cross tabulation similar to the counts table above.  Adding in `group_by()` to the code, means that we are obtaining the proportions within each level of that variable.  In this case, we are looking at the distribution of `TrustGovernment` for each level of `TrustPeople`.
```{r}
#| label: stattest-chi-ex2-prop1
chi_ex2$observed %>% as_tibble() %>% 
  group_by(TrustPeople) %>% 
  mutate(prop=round(n/sum(n),3)) %>% 
  select(-n) %>% 
  pivot_wider(names_from=TrustPeople,values_from=prop) %>% 
  gt(rowname_col = "TrustGovernment") %>% 
  tab_stubhead(label = "Trust in Government") %>% 
  tab_spanner(
    label = "Trust in People",
    columns = everything()
  )
```
The second option is to use `group_by()` and `survey_mean()` functions to calculate the proportions from the ANES design object. A reminder, that with more than one variable listed in the `group_by()` statement, the proportions are within the first variable listed.  As above, we are looking at the distribution of `TrustGovernment` for each level of `TrustPeople`.
```{r}
#| label: stattest-chi-ex2-prop2
chi_ex2_obs<-anes_des %>% 
  filter(!is.na(TrustPeople),!is.na(TrustGovernment)) %>% 
  group_by(TrustPeople,TrustGovernment) %>% 
  summarize(Observed=round(survey_mean(vartype = "ci"),3)) 

chi_ex2_obs %>% 
  mutate(prop=paste0(Observed," (",Observed_low,", ",
                     Observed_upp,")")) %>% 
  select(TrustGovernment,TrustPeople,prop) %>% 
  pivot_wider(names_from=TrustPeople,values_from=prop) %>% 
  gt(rowname_col = "TrustGovernment") %>% 
  tab_stubhead(label = "Trust in Government") %>% 
  tab_spanner(
    label = "Trust in People",
    columns = everything()
  )
```
Both methods produce the same output as the the `svychisq()` function does account for the survey design.  However, calculating the proportions directly from the design object means that you can also obtain the variance information.  In this case, the table output displays the survey estimate followed by the confidence intervals.  Based on the output, we can see that of those that never trust people 50.3% also never trust the government, while the proportions of never trusting the government are much lower for each of the other levels of trusting people.  

We may find it easier to look at these proportions graphically.  We can use `gglot()` and facets to provide an overview:
<!--Need to clean this up to make it readable. Need to also add in a higher facet title to indicate that is trusting people-->
<!--Perhaps a mosaic plot instead?-->
```{r}
#| label: stattest-chi-ex2-graph
chi_ex2_obs %>% 
  ggplot(aes(x=TrustGovernment,y=Observed,color=TrustGovernment)) + 
    facet_wrap(~TrustPeople,ncol=5) +
    geom_point() + 
    geom_errorbar(aes(ymin=Observed_low,ymax=Observed_upp)) + 
    ylab("Proportion") +
    theme_bw()


```

#### Example 3: Test of Homogeneity {.unnumbered #stattest-chi-ex3}

Each election cycle, researchers and politicians often look at specific demographics to understand how each group is leaning or voting towards candidates.  The ANES data is post-election, but we can still look to see if there are differences in how specific demographic groups voted.

If we want to see if there is a difference in how each age group voted for the 2020 candidates, this would be a test of homogeneity and the hypothesis could be set up as:

<!--Need to figure out how to make this readable.-->
- $H_0: p_{1_{Biden}} = p_{1_{Trump}} = p_{1_{Other}},\\
        p_{2_{Biden}} = p_{2_{Trump}} = p_{2_{Other}},\\
        p_{3_{Biden}} = p_{3_{Trump}} = p_{3_{Other}},\\
        p_{4_{Biden}} = p_{4_{Trump}} = p_{4_{Other}},\\
        p_{5_{Biden}} = p_{5_{Trump}} = p_{5_{Other}},\\
        p_{6_{Biden}} = p_{6_{Trump}} = p_{6_{Other}}$
        where $p_{i_{Biden}}$ is the observed proportion of each age group ($i$) that voted for Biden, $p_{i_{Trump}}$ is the observed proportion of each age group ($i$) that voted for Trump, and $p_{i_{Other}}$ is the observed proportion of each age group ($i$) that voted for another candidate
- $H_A:$ at least one category of $p_{i_{Biden}}$ does not match $p_{i_{Trump}}$ or $p_{i_{Other}}$

To conduct this in R, we will use the `svychisq()` function to compare the two variables:
```{r}
#| label: stattest-chi-ex3
chi_ex3<-anes_des %>% 
  filter(VotedPres2020=="Yes" & 
           !is.na(VotedPres2020_selection) & 
           !is.na(AgeGroup)) %>% 
  svychisq(formula=~AgeGroup+VotedPres2020_selection,
           design=.,
           statistic="Chisq",
           na.rm=TRUE)

chi_ex3
```
The output from `svychisq()` indicates that there is a difference in how each age group voted in the 2020 election. To get a better idea of the different distributions, let's out put proportions to see the relationship. As we learned in Example 2 above, we can use `chi_ex3$observed` or if we want to get the variance information (which is crucial with survey data), we can use `survey_mean()`. Remember, when we have two variables in `group_by()`, we are obtaining the proportions within each level of the variable listed.  In this case, we are looking at the distribution of `AgeGroup` for each level of `VotedPres2020_selection`.
```{r}
#| label: stattest-chi-ex3-table
chi_ex3_obs<-anes_des %>% 
filter(VotedPres2020=="Yes" & 
           !is.na(VotedPres2020_selection) & 
           !is.na(AgeGroup)) %>% 
  group_by(VotedPres2020_selection,AgeGroup) %>% 
  summarize(Observed=round(survey_mean(vartype = "ci"),3)) 

chi_ex3_obs %>% 
  mutate(prop=paste0(Observed," (",Observed_low,", ",
                     Observed_upp,")")) %>% 
  select(AgeGroup,VotedPres2020_selection,prop) %>% 
  pivot_wider(names_from=VotedPres2020_selection,values_from=prop) %>% 
  gt(rowname_col = "AgeGroup") %>% 
  tab_stubhead(label = "Age Group")
```

We can see that the age group distribution was younger for Biden and other candidates, and older for Trump.  For example, of those that voted for Biden, 20.4% were in the 18-29 age group compared to only 11.4% of those that voted for Trump where in that age group.  On the other side, 23.4% of those that voted for Trump were in the 50-59 age group compared to only 15.4% of those that voted for Biden.


### Exercises {#stattest-chi-exercises}

Here are some exercises for practicing conducting chi-squared tests using `svygofchisq()` and `svychisq()`:

1. If you wanted to determine if the political party affiliation differed for males and females, what test would you use?
    a. Goodness of fit test (`svygofchisq()`)
    b. Test of independence (`svychisq()`)
    c. Test of homogeneity (`svychisq()`)
```{r}
#| label: stattest-chisq-solution1
chisq_solution1<-"c. Test of homogeneity (`svychisq()`)"
chisq_solution1
```
2. In the RECS data, is there a relationship between the type of housing unit (`HousingUnitType`) and the year the house was built (`YearMade`)?
```{r}
#| label: stattest-chisq-solution2
chisq_solution2<-recs_des %>% 
  svychisq(formula=~HousingUnitType+YearMade,
           design=.,
           statistic="Wald",
           na.rm=TRUE)

chisq_solution2
```
3. In the ANES data, is there a difference in the distribution of gender (`Gender`) across early voting status in 2020 (`EarlyVote2020`)?

```{r}
#| label: stattest-chisq-solution3
chisq_solution3<-anes_des %>% 
  svychisq(formula=~Gender+EarlyVote2020,
           design=.,
           statistic="F",
           na.rm=TRUE)

chisq_solution3
```



<!--chapter:end:07-statistical-testing.Rmd-->

# Modeling {#c08}

<!--chapter:end:08-modeling.Rmd-->

# Presenting results {#c09}

<!--chapter:end:09-presenting-results.Rmd-->

\cleardoublepage 

# (APPENDIX) Appendix {-}

# More to Say

Yeah! I have finished my book, but I have more to say about some topics. Let me explain them in this appendix.

To know more about **bookdown**, see https://bookdown.org.

This is for testing GH Actions.

<!--chapter:end:90-more.Rmd-->

`r if (knitr:::is_html_output()) '# References {-}'`

```{r include=FALSE}
# generate a BibTeX database automatically for some R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:99-references.Rmd-->

