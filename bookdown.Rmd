--- 
title: "Exploring Complex Survey Data Analysis Using R"
subtitle: "A Tidy Introduction with {srvyr} and {survey}"
author: "Stephanie A. Zimmer, Rebecca J. Powell, and Isabella C. Velásquez"
date: "`r Sys.Date()`"
documentclass: krantz
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
colorlinks: true
lot: true
lof: true
site: bookdown::bookdown_site
description: "Exploring Complex Survey Data Analysis Using R: A Tidy Introduction with {srvyr} and {survey}"
github-repo: tidy-survey-r/tidy-survey-book
graphics: yes
favicon: images/favicon.png/
cover-image: images/cover.png/
header-includes:
   - \usepackage[titles]{tocloft}
---


```{r setup}
#| include: false

knitr::opts_chunk$set(fig.pos = "h!", out.extra = "")

library(styler)
options(
  htmltools.dir.version = FALSE, formatR.indent = 2, digits = 4
)
if (knitr:::is_html_output()){
  options(width=72)
} else{
  options(width=72)
}

library(prettyunits)

book_colors <- c("#0b3954", "#087e8b", "#bfd7ea", "#ff8484", "#8d6b94")

as_latex_with_caption <- function(gtobj, chunk_label) {
  lt <- nrow(gtobj[["_data"]]) >= 5
  gt_l <- gtobj %>% tab_options(latex.use_longtable=lt, latex.tbl.pos="H") %>% gt::as_latex()
  caption <- paste0(
    "\\caption{\\label{tab:", chunk_label, "}(ref:", chunk_label, ")}")
    if (lt){
    caption <- paste0(caption, " \\\\")
  }
  latex <- strsplit(gt_l[1], split = "\n")[[1]]
  idxtable <- which(stringr::str_detect(latex, "begin") & stringr::str_detect(latex, "table"))
  # https://tex.stackexchange.com/questions/95236/as-first-character-in-table-row 
  idxparen <- which(stringr::str_detect(latex, "^\\("))
  if (length(idxparen)>0){
    latex[(idxparen-1)] <- stringr::str_c(latex[(idxparen-1)], "\\relax")
  }
  latex1 <- stringi::stri_replace_all(latex, regex="(?=\\d*)-{1,2}(\\d)", replacement="--$1")
  latex2 <- c(latex1[1:idxtable], caption, latex1[-c(1:idxtable)])
  latex3 <- paste(latex2, collapse = "\n")
  gt_l[1] <- latex3
  return(gt_l)
}

print_gt_book <- function(gtobj, ref){
  if ("gtsummary" %in% class(gtobj)){
    gtobj <- as_gt(gtobj)
  }
  
  if (knitr::is_latex_output()){
    gtobj %>%
      as_latex_with_caption(ref)
  } else {
    gtobj %>% 
      tab_caption(glue::glue("(ref:{ref})"))
  }
  
  
}

```

`r if (knitr:::is_html_output()) '# Welcome {-}'`

```{r}
#| label: index-printversion-text
#| echo: false
#| results: asis

printversion_text <- "This is the online version of the book published by CRC Press in November 2024.  You can purchase a copy of this book directly from [Routledge](https://www.routledge.com/Exploring-Complex-Survey-Data-Analysis-Using-R-A-Tidy-Introduction-with-srvyr-and-survey/Zimmer-Powell-Velasquez/p/book/9781032302867) or your preferred bookstore. The cover artwork was designed and created by [Allison Horst](https://allisonhorst.com/)."

if (knitr:::is_html_output()){
  cat(printversion_text)
} 

rm(printversion_text)
```


```{r}
#| label: index-printversion-coverimage
#| echo: false
#| fig.cap: ""
#| fig.alt: "Image of print book cover with author names, title, and cover image"
#| out.width: 70%
#| fig.align: center

if (knitr:::is_html_output()){
  knitr::include_graphics(path="images/cover.png")
}
```

`r if (knitr:::is_html_output()) '## Dedication {-}'`

```{r}
#| label: index-dedication-text
#| echo: false
#| results: asis

thanks <- "To Will, Tom, and Drew, thanks for all the help with additional chores and plenty of Git consulting!"

if (knitr:::is_html_output()){
  cat(thanks)
} else if(knitr:::is_latex_output()){
  bb <- readLines(here::here("latex", "before_body_temp.tex"))
  bb[which(bb=="placeholder")] <- thanks
  writeLines(bb, here::here("latex", "before_body_ded.tex"))
  rm(bb)
}

rm(thanks)
```

`r if (knitr:::is_html_output()) '## Citation {-}'`

```{r}
#| label: index-citation-text
#| echo: false
#| results: asis

citation_text <- "To cite this book, we recommend the following citation: \n 
Zimmer, S. A., Powell, R. J., & Velásquez, I. C. (2024). <i>Exploring Complex Survey Data Analysis Using R: A Tidy Introduction with {srvyr} and {survey}</i>. Chapman & Hall: CRC Press."

if (knitr:::is_html_output()){
  cat(citation_text)
} 

rm(citation_text)
```

<!--chapter:end:index.Rmd-->

\mainmatter

# (PART) Introduction {-}

# Introduction {#c01-intro}

Surveys are valuable tools for gathering information about a population. Researchers, governments, and businesses use surveys to better understand public opinion and behaviors. For example, a non-profit group may analyze societal trends to measure their impact, government agencies may study behaviors to inform policy, or companies may seek to learn customer product preferences to refine business strategy. With survey data, we can explore the world around us. 

Surveys are often conducted with a sample of the population. Therefore, to use the survey data to understand the population, we use weights to adjust the survey results for unequal probabilities of selection, nonresponse, and post-stratification. These adjustments ensure the sample accurately represents the population of interest [@gard2023weightsdef]. To account for the intricate nature of the survey design, analysts rely on statistical software such as SAS, Stata, SUDAAN, and R. 

In this book, we focus on R to introduce survey analysis. Our goal is to provide a comprehensive guide for individuals new to survey analysis but with some familiarity with statistics and R programming. We use a combination of the {survey} and {srvyr} packages and present the code following best practices from the tidyverse [@R-srvyr; @lumley2010complex; @tidyverse2019]. 

## Survey analysis in R

The {survey} package was released on the [Comprehensive R Archive Network (CRAN)](https://cran.r-project.org/src/contrib/Archive/survey/) in 2003 and has been continuously developed over time. This package, primarily authored by Thomas Lumley, offers an extensive array of features, including:

* Calculation of point estimates and estimates of their uncertainty, including means, totals, ratios, quantiles, and proportions
* Estimation of regression models, including generalized linear models, log-linear models, and survival curves
* Variances by Taylor linearization or by replicate weights, including balance repeated replication, jackknife, bootstrap, multistage bootstrap, or user-supplied methods
* Hypothesis testing for means, proportions, and other parameters

The {srvyr} package builds on the {survey} package by providing wrappers for functions that align with the tidyverse philosophy. This is our motivation for using and recommending the {srvyr} package. We find that it is user-friendly for those familiar with the tidyverse packages in R.

For example, while many functions in the {survey} package access variables through formulas, the {srvyr} package uses tidy selection to pass variable names, a common feature in the tidyverse [@R-tidyselect]. Users of the tidyverse are also likely familiar with the magrittr pipe operator (`%>%`), which seamlessly works with functions from the {srvyr} package. Moreover, several common functions from {dplyr}, such as `filter()`, `mutate()`, and `summarize()`, can be applied to survey objects [@R-dplyr]. This enables users to streamline their analysis workflow and leverage the benefits of both the {srvyr} and {tidyverse} packages.

While the {srvyr} package offers many advantages, there is one notable limitation: it doesn't fully incorporate the modeling capabilities of the {survey} package into tidy wrappers. When discussing modeling and hypothesis testing, we primarily rely on the {survey} package. However, we provide information on how to apply the pipe operator to these functions to maintain clarity and consistency in analyses.

## What to expect {#what-to-expect}

This book covers many aspects of survey design and analysis, from understanding how to create design objects to conducting descriptive analysis, statistical tests, and models. We emphasize coding best practices and effective presentation techniques while using real-world data and practical examples to help readers gain proficiency in survey analysis. 

Below is a summary of each chapter:

- **Chapter \@ref(c02-overview-surveys) - Overview of surveys**:
  - Overview of survey design processes
  - References for more in-depth knowledge
- **Chapter \@ref(c03-survey-data-documentation) - Survey data documentation**:
  - Guide to survey documentation types
  - How to read survey documentation
- **Chapter \@ref(c04-getting-started) - Getting started**:
  - Installation of packages
  - Introduction to the {srvyrexploR} package and its analytic datasets
  - Outline of the survey analysis process
  - Comparison between the {dplyr} and {srvyr} packages
- **Chapter \@ref(c05-descriptive-analysis) - Descriptive analyses**:
  - Calculation of point estimates
  - Estimation of standard errors and confidence intervals
  - Calculation of design effects
- **Chapter \@ref(c06-statistical-testing) - Statistical testing**:
  - Statistical testing methods
  - Comparison of means and proportions
  - Goodness-of-fit tests, tests of independence, and tests of homogeneity
- **Chapter \@ref(c07-modeling) - Modeling**:
  - Overview of model formula specifications
  - Linear regression, ANOVA, and logistic regression modeling
- **Chapter \@ref(c08-communicating-results) - Communication of results**:
  - Strategies for communicating survey results
  - Tools and guidance for creating publishable tables and graphs
- **Chapter \@ref(c09-reprex-data) - Reproducible research**: 
  - Tools and methods for achieving reproducibility
  - Resources for reproducible research
- **Chapter \@ref(c10-sample-designs-replicate-weights) - Sample designs and replicate weights**: 
  - Overview of common sampling designs
  - Replicate weight methods 
  - How to specify survey designs in R
- **Chapter \@ref(c11-missing-data) - Missing data**:
  - Overview of missing data in surveys
  - Approaches to dealing with missing data
- **Chapter \@ref(c12-recommendations) - Successful survey analysis recommendations**:
  - Tips for successful analysis
  - Recommendations for debugging
- **Chapter \@ref(c13-ncvs-vignette) - National Crime Victimization Survey Vignette**: 
  - Vignette on analyzing National Crime Victimization Survey (NCVS) data
  - Illustration of analysis requiring multiple files for victimization rates
- **Chapter \@ref(c14-ambarom-vignette) - AmericasBarometer Vignette**:
  - Vignette on analyzing AmericasBarometer survey data
  - Creation of choropleth maps with survey estimates

The majority of chapters contain code that readers can follow. Each of these chapters starts with a "Prerequisites" section, which includes the code needed to load the packages and datasets used in the chapter. We then provide the main idea of the chapter and examples of how to use the functions. Most chapters conclude with exercises to work through. We provide the solutions to the exercises in the [online version of the book](https://tidy-survey-r.github.io/tidy-survey-book/).

While we provide a brief overview of survey methodology and statistical theory, this book is not intended to be the sole resource for these topics. We reference other materials and encourage readers to seek them out for more information. 

## Prerequisites

To get the most out of this book, we assume a survey has already been conducted and readers have obtained a microdata file. Microdata, also known as respondent-level or row-level data, differ from summarized data typically found in tables. Microdata contain individual survey responses, along with analysis weights and design variables such as strata or clusters.

Additionally, the survey data should already include weights and design variables. These are required to accurately calculate unbiased estimates. The concepts and techniques discussed in this book help readers to extract meaningful insights from survey data, but this book does not cover how to create weights, as this is a separate complex topic. If weights are not already created for the survey data, we recommend reviewing other resources focused on weight creation such as @Valliant2018weights.

This book is tailored for analysts already familiar with R and the tidyverse, but who may be new to complex survey analysis in R. We anticipate that readers of this book can:

* Install R and their Integrated Development Environment (IDE) of choice, such as RStudio
* Install and load packages from CRAN and GitHub repositories
* Run R code
* Read data from a folder or their working directory
* Understand fundamental tidyverse concepts such as tidy/long/wide data, tibbles, the magrittr pipe (`%>%`), and tidy selection
* Use the tidyverse packages to wrangle, tidy, and visualize data

If these concepts or skills are unfamiliar, we recommend starting with introductory resources to cover these topics before reading this book. R for Data Science [@wickham2023r4ds] is a beginner-friendly guide for getting started in data science using R. It offers guidance on preliminary installation steps, basic R syntax, and tidyverse workflows and packages.

## Datasets used in this book

We work with two key datasets throughout the book: the Residential Energy Consumption Survey [RECS -- @recs-2020-tech] and the American National Election Studies [ANES -- @debell]. We introduce the loading and preparation of these datasets in Chapter \@ref(c04-getting-started).

## Conventions

Throughout the book, we use the following typographical conventions:

* Package names are surrounded by curly brackets: {srvyr}
* Function names are in constant-width text format and include parentheses: `survey_mean()`
* Object and variable names are in constant-width text format: `anes_des`

## Getting help

We recommend first trying to resolve errors and issues independently using the tips provided in Chapter \@ref(c12-recommendations). 

There are several community forums for asking questions, including:

* [Posit Community](https://forum.posit.co/)
* [R for Data Science Slack Community](https://rfordatasci.com/)
* [Stack Overflow](https://stackoverflow.com/)

Please report any bugs and issues to the book's [GitHub repository](https://github.com/tidy-survey-r/tidy-survey-book/issues).

## Acknowledgments

We would like to thank Holly Cast, Greg Freedman Ellis, Joe Murphy, and Sheila Saia for their reviews of the initial draft. Their detailed and honest feedback helped improve this book, and we are grateful for their input. Additionally, this book started with two short courses. The first was at the Annual Conference for the American Association for Public Opinion Research (AAPOR) and the second was a series of webinars for the Midwest Association of Public Opinion Research (MAPOR). We would like to also thank those who assisted us by moderating breakout rooms and answering questions from attendees: Greg Freedman Ellis, Raphael Nishimura, and Benjamin Schneider.

## Colophon

This book was written in [bookdown](http://bookdown.org/) using [RStudio](http://www.rstudio.com/ide/). The complete source is available on [GitHub](https://github.com/tidy-survey-r/tidy-survey-book).

This version of the book was built with `r R.version.string` and with the packages listed in Table \@ref(tab:intro-packages-tab).

```{r}
#| label: intro-colophon-pkgs
#| echo: false
#| warning: false
#| message: false
library(prettyunits)
library(DiagrammeR)
library(tidyverse)
library(tidycensus)
library(survey)
library(srvyr)
library(srvyrexploR)
library(broom)
library(gt)
library(gtsummary)
library(censusapi)
library(naniar)
library(haven)
library(sf)
library(rnaturalearth)
library(rnaturalearthdata)
library(ggpattern)
library(osfr)
library(janitor)
library(kableExtra)
library(knitr)
library(labelled)
library(bookdown)
library(rmarkdown)
library(tidyselect)

```

(ref:intro-packages-tab) Package versions and sources used in building this book

```{r}
#| label: intro-packages-tab
#| echo: FALSE
#| warning: FALSE
renv_in <- renv::lockfile_read()
renv_pack <- renv_in$Packages %>%
  map(as_tibble) %>%list_rbind() %>%
  distinct(Package, Version, Source, Repository, RemoteSha, RemoteUsername, RemoteRepo)

packinfo <- sessioninfo::package_info()
packinfo_attach <- packinfo %>%
  filter(attached|package %in% c("renv"))

packinfo_tib <-
  renv_pack %>%
  filter(Package %in% c(pull(packinfo_attach, package))) %>%
  rename(SourceInit=Source) %>%
  mutate(
    ShortSha=str_sub(RemoteSha, 1, 7),
    Source=case_when(
      Repository=="CRAN"~"CRAN",
      TRUE ~ glue::glue("{SourceInit} ({RemoteUsername}/{RemoteRepo}@{ShortSha})")
    )
  ) %>%
  select(Package, Version, Source)

packinfo_tib %>%
  gt() %>%
  cols_align(align="left") %>%
  cols_label(
    Package=md("**Package**"),
    Version=md("**Version**"),
    Source=md("**Source**"),
  ) %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

<!--chapter:end:01-introduction.Rmd-->

# Overview of surveys {#c02-overview-surveys}

## Introduction
 
Developing surveys to gather accurate information about populations involves an intricate and time-intensive process. Researchers can spend months, or even years, developing the study design, questions, and other methods for a single survey to ensure high-quality data is collected.

\index{Research topic|(} \index{Question of interest|see {Research topic}} \index{Research question|see {Research topic}} \index{Burden|(} \index{Respondent burden|see {Burden}} \index{Survey burden|see {Burden}} \index{Survey life cycle|(}
Before analyzing survey data, we recommend understanding the entire survey life cycle. This understanding can provide better insight into what types of analyses should be conducted on the data. The survey life cycle consists of the necessary stages to execute a survey project successfully. Each stage influences the survey's timing, costs, and feasibility, consequently impacting the data collected and how we should analyze them. Figure \@ref(fig:overview-diag) shows a high-level overview of the survey process.

```{r}
#| label: overview-diag
#| echo: false
#| fig.cap: "Overview of the survey process"
#| fig.alt: "Diagram of survey process beginning with survey concept (first level), then sampling design, questionnaire design, and data collection planning (second level), data collection (third level), post-survey processing (fourth level), analysis (fifth level), and reporting (sixth level)"
library(DiagrammeR)
mermaid("
graph TD
  A[Survey Concept]-->B[Sampling Design]
  A-->C[Questionnaire Design]
  A-->D[Data Collection Planning]
  B-->E[Data Collection]
  C-->E
  D-->E
  E-->F[Post-Survey Processing]
  F-->G[Analysis]
  G-->H[Reporting]
  
  style A fill: #bfd7ea, stroke: #0b3954
  style B fill: #bfd7ea, stroke: #0b3954
  style C fill: #bfd7ea, stroke: #0b3954
  style D fill: #bfd7ea, stroke: #0b3954
  style E fill: #bfd7ea, stroke: #0b3954
  style F fill: #bfd7ea, stroke: #0b3954
  style G fill: #bfd7ea, stroke: #0b3954
  style H fill: #bfd7ea, stroke: #0b3954
")
```


\index{Research topic|(} \index{Question of interest|see {Research topic}} \index{Research question|see {Research topic}} \index{Burden|(} \index{Respondent burden|see {Burden}} \index{Survey burden|see {Burden}} \index{Questionnaire|(}
The survey life cycle starts with a research topic or question of interest (e.g., the impact that childhood trauma has on health outcomes later in life). Drawing from available resources can result in a reduced burden on respondents, lower costs, and faster research outcomes. Therefore, we recommend reviewing existing data sources to determine if data that can address this question are already available. However, if existing data cannot answer the nuances of the research question, we can capture the exact data we need through a questionnaire, or a set of questions.\index{Research topic|)} \index{Burden|)} \index{Survey life cycle|)} \index{Questionnaire|)}

To gain a deeper understanding of survey design and implementation, we recommend reviewing several pieces of existing literature in detail [e.g., @biemer2003survqual; @Bradburn2004; @dillman2014mode; @groves2009survey; @Tourangeau2000psych;  @valliant2013practical].

## Searching for public-use survey data

Throughout this book, we use public-use datasets from different surveys, including the American National Election Studies (ANES), the Residential Energy Consumption Survey (RECS), the National Crime Victimization Survey (NCVS), and the AmericasBarometer surveys.

\index{Research topic|(}As mentioned above, we should look for existing data that can provide insights into our research questions before embarking on a new survey. One of the greatest sources of data is the government. For example, in the U.S., we can get data directly from the various statistical agencies such as the U.S. Energy Information Administration or Bureau of Justice Statistics. Other countries often have data available through official statistics offices, such as the Office for National Statistics in the United Kingdom.\index{Research topic|)}

In addition to government data, many researchers make their data publicly available through repositories such as the [Inter-university Consortium for Political and Social Research (ICPSR)](https://www.icpsr.umich.edu/web/pages/ICPSR/ssvd/) or the [Odum Institute Data Archive](https://odum.unc.edu/archive/). Searching these repositories or other compiled lists (e.g., [Analyze Survey Data for Free](https://asdfree.com)) can be an efficient way to identify surveys with questions related to our research topic.

## Pre-survey planning {#pre-survey-planning}

There are multiple things to consider when starting a survey. Errors are the differences between the true values of the variables being studied and the values obtained through the survey. Each step and decision made before the launch of the survey impact the types of errors that are introduced into the data, which in turn impact how to interpret the results.

\index{Representation|(}\index{Total survey error|(}Generally, survey researchers consider there to be seven main sources of error that fall under either Representation or \index{Measurement}Measurement [@groves2009survey]:

- Representation
    - \index{Coverage error|(}Coverage Error: A mismatch between the \index{Population of interest|(}\index{Target population|see {Population of interest}}population of interest\index{Population of interest|)} and \index{Sampling frame|(}the sampling frame, the list from which the sample is drawn.\index{Coverage error|)}\index{Sampling frame|)}
    - \index{Sampling error|(}Sampling Error: \index{Sampling frame|(}\index{Sample|(}Error produced when selecting a sample, the subset of the population, from the sampling frame.\index{Sampling frame|)} This error is due to randomization, and we discuss how to quantify this error in Chapter \@ref(c10-sample-designs-replicate-weights). There is no sampling error in a census, as there is no randomization. The sampling error measures the difference between all potential samples under the same sampling method.\index{Sampling error|)}\index{Sample|)}
    - \index{Nonresponse error|(}Nonresponse Error: Differences between those who responded and \index{Unit nonresponse|(}did not respond to the survey (unit nonresponse)\index{Unit nonresponse|)} or \index{Item nonresponse|(}a given question (item nonresponse).\index{Nonresponse error|)}\index{Item nonresponse|)}
    - \index{Adjustment error|(}Adjustment Error: Error introduced during post-survey statistical adjustments. \index{Representation|)}
- \index{Measurement|(}Measurement\index{Adjustment error|)}
    - \index{Validity|(}Validity: A mismatch between the research topic and the question(s) used to collect that information.\index{Validity|)}
    - \index{Measurement error|(}Measurement Error: A mismatch between what the researcher asked and how the respondent answered.\index{Measurement error|)}
    - \index{Processing error|(}Processing Error: Edits by the researcher to responses provided by the respondent (e.g., adjustments to data based on illogical responses).\index{Measurement|)}\index{Processing error|)}

Almost every survey has errors. Researchers attempt to conduct a survey that reduces the total survey error, or the accumulation of all errors that may arise throughout the survey life cycle. By assessing these different types of errors together, researchers can seek strategies to maximize the overall survey quality and improve the reliability and validity of results [@tse-doc]. However, attempts to reduce individual source errors (and therefore total survey error) come at the price of time and money. For example:

- \index{Coverage error|(}\index{Sampling frame|(}Coverage Error Tradeoff: Researchers can search for or create more accurate and updated sampling frames, but they can be difficult to construct or obtain.\index{Coverage error|)}\index{Sampling frame|)}
- \index{Sampling error|(}Sampling Error Tradeoff: Researchers can increase the sample size to reduce sampling error; however, larger samples can be expensive and time-consuming to field.\index{Sampling error|)}
- \index{Nonresponse error|(}Nonresponse Error Tradeoff: Researchers can increase or diversify efforts to improve survey participation, but this may be resource-intensive while not entirely removing nonresponse bias.\index{Nonresponse error|)}
- \index{Weighting|(}\index{Weights|see {Weighting}}Adjustment Error Tradeoff: Weighting is a statistical technique used to adjust the contribution of individual survey responses to the final survey estimates. It is typically done to make the sample more representative of the population of interest.  However, if researchers do not carefully execute the adjustments or base them on inaccurate information, they can introduce new biases, leading to less accurate estimates.\index{Weighting|)}
- \index{Validity|(}Validity Error Tradeoff: Researchers can increase validity through a variety of ways, such as using established scales or collaborating with a psychometrician during survey design to pilot and evaluate questions. However, doing so increases the amount of time and resources needed to complete survey design.\index{Validity|)}
- \index{Measurement error|)}\index{Questionnaire testing|(}\index{Piloting|see {Questionnaire testing}} \index{Cognitive interview|(}Measurement Error Tradeoff: Researchers can use techniques such as questionnaire testing and cognitive interviewing to ensure respondents are answering questions as expected. However, these activities require time and resources to complete.\index{Measurement error|)} \index{Questionnaire testing|(} \index{Cognitive interview|)}
- \index{Processing error|(}Processing Error Tradeoff: Researchers can impose rigorous data cleaning and validation processes. However, this requires supervision, training, and time.\index{Processing error|)}

The challenge for survey researchers is to find the optimal tradeoffs among these errors. They must carefully consider ways to reduce each error source and total survey error while balancing their study's objectives and resources.

For survey analysts, understanding the decisions that researchers took to minimize these error sources can impact how results are interpreted. The remainder of this chapter explores critical considerations for survey development. We explore how to consider each of these sources of error and how these error sources can inform the interpretations of the data.\index{Total survey error|)}

## Study design {#overview-design}

\index{Survey life cycle|(} \index{Sampling frame|(} \index{Study design|(}From formulating methodologies to choosing an appropriate sampling frame, the study design phase is where the blueprint for a successful survey takes shape. \index{Population of interest|(}Study design encompasses multiple parts of the survey life cycle, including decisions on the population of interest, \index{Population of interest|)} \index{Mode|(}\index{Survey mode|see {Mode}}survey mode (the format through which a survey is administered to respondents)\index{Mode|)}, timeline, and questionnaire design. Knowing who and how to survey individuals depends on the study's goals and the feasibility of implementation. This section explores the strategic planning that lays the foundation for a survey.\index{Sampling frame|)} \index{Survey life cycle|)}

### Sampling design {#overview-design-sampdesign}

\index{Population of interest|(}The set or group we want to survey is known as the population of interest or the target population. The population of interest could be broad, such as “all adults age 18+ living in the U.S.” or a specific population based on a particular characteristic or location. For example, we may want to know about "adults aged 18--24 who live in North Carolina" or "eligible voters living in Illinois." \index{Population of interest|)}

\index{Sampling frame|(}However, a sampling frame with contact information is needed to survey individuals in these populations of interest. If we are looking at eligible voters, the sampling frame could be the voting registry for a given state or area. If we are looking at more board populations of interest, like all adults in the United States, the sampling frame is likely imperfect. In these cases, a full list of individuals in the United States is not available for a sampling frame. Instead, we may choose to use a sampling frame of mailing addresses and send the survey to households, or we may choose to use random digit dialing (RDD) and call random phone numbers (that may or may not be assigned, connected, and working). 

\index{Coverage error|(}These imperfect sampling frames can result in coverage error where there is a mismatch between the population of interest and the list of individuals we can select. For example, if we are looking to obtain estimates for "all adults aged 18+ living in the U.S.," a sampling frame of mailing addresses will miss specific types of individuals, such as the homeless, transient populations, and incarcerated individuals. Additionally, many households have more than one adult resident, so we would need to consider how to get a specific individual to fill out the survey (called within household selection) or adjust the population of interest to report on "U.S. households" instead of "individuals."\index{Coverage error|)}

Once we have selected the sampling frame, the next step is determining how to select individuals for the survey. In rare cases, we may conduct a census and survey everyone on the sampling frame. However, the ability to implement a questionnaire at that scale is something only a few can do (e.g., government censuses). \index{Weighting|(}Instead, we typically choose to sample individuals and use weights to estimate numbers in the population of interest. They can use a variety of different sampling methods, and more information on these can be found in Chapter \@ref(c10-sample-designs-replicate-weights). \index{Sampling error|(}This decision of which sampling method to use impacts sampling error and can be accounted for in weighting.\index{Sampling error|)}\index{Sampling frame|)}\index{Weighting|)}

#### Example: Number of pets in a household {.unnumbered #overview-design-sampdesign-ex}

Let's use a simple example where we are interested in the average number of pets in a household. We need to consider the population of interest for this study. Specifically, are we interested in all households in a given country or households in a more local area (e.g., city or state)? Let's assume we are interested in the number of pets in a U.S. household with at least one adult (18 years or older). \index{Coverage error|(}\index{Sampling frame|(}In this case, a sampling frame of mailing addresses would introduce only a small amount of coverage error as the frame would closely match our population of interest.\index{Coverage error|)} Specifically, we would likely want to use the Computerized Delivery Sequence File (CDSF), which is a file of mailing addresses that the United States Postal Service (USPS) creates and covers nearly 100% of U.S. households [@harter2016address]. To sample these households, for simplicity, \index{Stratified sampling|(}we use a stratified simple random sample design (see Chapter \@ref(c10-sample-designs-replicate-weights) for more information on sample designs), where we randomly sample households within each state (i.e., we stratify by state).\index{Stratified sampling|)}\index{Sampling frame|)} 

Throughout this chapter, we build on this example research question to plan a survey. 

### Data collection planning {#overview-design-dcplanning}

\index{Mode|(} \index{Data collection|(}
With the sampling design decided, researchers can then decide how to survey these individuals. Specifically, the modes used for contacting and surveying the sample, how frequently to send reminders and follow-ups, and the overall timeline of the study are some of the major data collection determinations. Traditionally, survey researchers have considered there to be four main modes^[Other modes such as using mobile apps or text messaging can also be considered, but at the time of publication, they have smaller reach or are better for longitudinal studies (i.e., surveying the same individuals over many time periods of a single study).]:

- Computer-Assisted Personal Interview (CAPI; also known as face-to-face or in-person interviewing)
- Computer-Assisted Telephone Interview (CATI; also known as phone or telephone interviewing)
- Computer-Assisted Web Interview (CAWI; also known as web or online interviewing)
- Paper and Pencil Interview (PAPI)

We can use a single mode to collect data or multiple modes (also called mixed-modes). Using mixed-modes can allow for broader reach and increase response rates depending on the population of interest [@biemer_choiceplus; @deLeeuw2005; @DeLeeuw_2018]. For example, we could both call households to conduct a CATI survey and send mail with a PAPI survey to the household. By using both modes, we could gain participation through the mail from individuals who do not pick up the phone to unknown numbers or through the phone from individuals who do not open all of their mail. However, mode effects (where responses differ based on the mode of response) can be present in the data and may need to be considered during analysis.

\index{Sampling frame|(}When selecting which mode, or modes, to use, understanding the unique aspects of the chosen population of interest and sampling frame provides insight into how they can best be reached and engaged. For example, if we plan to survey adults aged 18--24 who live in North Carolina, asking them to complete a survey using CATI (i.e., over the phone) would likely not be as successful as other modes like the web. This age group does not talk on the phone as much as other generations and often does not answer phone calls from unknown numbers. Additionally, the mode for contacting respondents relies on what information is available in the sampling frame. For example, if our sampling frame includes an email address, we could email our selected sample members to convince them to complete a survey. Alternatively, if the sampling frame is a list of mailing addresses, we could contact sample members with a letter. 

It is important to note that there can be a difference between the contact and survey modes. For example, if we have a sampling frame with addresses, we can send a letter to our sample members and provide information on completing a web survey.\index{Sampling frame|)} Another option is using mixed-mode surveys by mailing sample members a paper and pencil survey but also including instructions to complete the survey online. \index{Nonresponse error|(}\index{Unit nonresponse|(}Combining different contact modes and different survey modes can be helpful in reducing unit nonresponse error--where the entire unit (e.g., a household) does not respond to the survey at all--as different sample members may respond better to different contact and survey modes. \index{Burden|(} However, when considering which modes to use, it is important to make access to the survey as easy as possible for sample members to reduce burden and unit nonresponse.\index{Mode|)} \index{Burden|)} 

Another way to reduce unit nonresponse error is by varying the language of the contact materials [@dillman2014mode]. People are motivated by different things, so constantly repeating the same message may not be helpful. Instead, mixing up the messaging and the type of contact material the sample member receives can increase response rates and reduce the unit nonresponse error. For example, instead of only sending standard letters, we could consider sending mailings that invoke "urgent" or "important" thoughts by sending priority letters or using other delivery services like FedEx, UPS, or DHL.\index{Nonresponse error|)}\index{Unit nonresponse|)}

A study timeline may also determine the number and types of contacts. If the timeline is long, there is plentiful time for follow-ups and diversified messages in contact materials. If the timeline is short, then fewer follow-ups can be implemented. Many studies start with the tailored design method put forth by @dillman2014mode and implement five contacts:

* Pre-notification (Pre-notice) to let sample members know the survey is coming
* Invitation to complete the survey
* Reminder to also thank the respondents who have already completed the survey
* Reminder (with a replacement paper survey if needed)
* Final reminder

This method is easily adaptable based on the study timeline and needs but provides a starting point for most studies.

#### Example: Number of pets in a household {.unnumbered #overview-design-dcplanning-ex}

Let's return to our example of the average number of pets in a household. \index{Nonresponse error|(}\index{Sampling frame|(}\index{Unit nonresponse|(}We are using a sampling frame of mailing addresses, so we recommend starting our data collection with letters mailed to households, but later in data collection, we want to send interviewers to the house to conduct an in-person (or CAPI) interview to decrease unit nonresponse error.\index{Nonresponse error|)}\index{Sampling frame|)}\index{Unit nonresponse|)} This means we have two contact modes (paper and in-person). \index{Mode|(}As mentioned above, the survey mode does not have to be the same as the contact mode, so we recommend a mixed-mode study with both web and CAPI modes. Let's assume we have 6 months for data collection, so we could recommend Table \@ref(tab:prot-examp)'s protocol:

Table: (\#tab:prot-examp) Protocol example for 6-month web and CAPI data collection 

| Week | Contact Mode | Contact Message | Survey Mode Offered |
|:----:|-----------|------------------|---------------|
|  1 | Mail: Letter | Pre-notice | --- |
|  2 | Mail: Letter | Invitation | Web |
|  3 | Mail: Postcard | Thank You/Reminder | Web |
|  6 | Mail: Letter in large envelope | Animal Welfare Discussion | Web |
| 10 | Mail: Postcard | Inform Upcoming In-Person Visit | Web |
| 14 | In-Person Visit | --- | CAPI |
| 16 | Mail: Letter | Reminder of In-Person Visit | Web, but includes a number to call to schedule CAPI | 
| 20 | In-Person Visit | --- | CAPI |
| 25 | Mail: Letter in large envelope | Survey Closing Notice | Web, but includes a number to call to schedule CAPI |

This is just one possible protocol that we can use that starts respondents with the web (typically done to reduce costs). However, we could begin in-person data collection earlier during the data collection period or ask interviewers to attempt more than two visits with a household.\index{Mode|)} \index{Data collection|)}

### Questionnaire design {#overview-design-questionnaire}

\index{Research topic|(} \index{Burden|(} When developing the questionnaire, it can be helpful to first outline the topics to be asked and include the "why" each question or topic is important to the research question(s). This can help us better tailor the questionnaire and reduce the number of questions (and thus the burden on the respondent) if topics are deemed irrelevant to the research question.\index{Burden|)} \index{Weighting|(}When making these decisions, we should also consider questions needed for weighting.\index{Research topic|)} While we would love to have everyone in our population of interest answer our survey, this rarely happens. \index{Nonresponse error|(}\index{Item nonresponse|(}Thus, including questions about demographics in the survey can assist with weighting for nonresponse errors (both unit and item nonresponse).\index{Nonresponse error|)}\index{Item nonresponse|)}\index{Weighting|)} \index{Coverage error|(}\index{Sampling error|(}Knowing the details of the sampling plan and what may impact coverage error and sampling error can help us determine what types of demographics to include. Thus questionnaire design is typically done in conjunction with sampling design.\index{Coverage error|)}\index{Sampling error|)}

We can benefit from the work of others by using questions from other surveys. Demographic sections in surveys, such as race, ethnicity, or education, often are borrowed questions from a government census or other official surveys. Question banks such as the [ICPSR variable search](https://www.icpsr.umich.edu/web/pages/ICPSR/ssvd/) can provide additional potential questions. 

\index{Research topic|(} \index{Questionnaire testing|(}
If a question does not exist in a question bank, we can craft our own. When developing survey questions, we should start with the research topic and attempt to write questions that match the concept. \index{Validity|(}The closer the question asked is to the overall concept, the better validity there is. For example, if we want to know how people consume T.V. series and movies but only ask a question about how many T.V.s are in the house, then we would be missing other ways that people watch T.V. series and movies, such as on other devices or at places outside of the home. As mentioned above, we can employ techniques to increase the validity of questionnaires. For example, questionnaire testing involves piloting the survey instrument to identify and fix potential issues before conducting the main survey. \index{Cognitive interview|(}Additionally, we could conduct cognitive interviews -- a technique where we walk through the survey with participants, encouraging them to speak their thoughts out loud to uncover how they interpret and understand survey questions. \index{Research topic|)}\index{Validity|)} \index{Questionnaire testing|)} \index{Cognitive interview|)}

\index{Mode|(}Additionally, when designing questions, we should consider the mode for the survey and adjust the language appropriately.\index{Mode|)} In self-administered surveys (e.g., web or mail), respondents can see all the questions and response options, but that is not the case in interviewer-administered surveys (e.g., CATI or CAPI). With interviewer-administered surveys, the response options must be read aloud to the respondents, so the question may need to be adjusted to create a better flow to the interview. \index{Measurement error|)}Additionally, with self-administered surveys, because the respondents are viewing the questionnaire, the formatting of the questions is even more critical to ensure accurate measurement. Incorrect formatting or wording can result in measurement error, so following best practices or using existing validated questions can reduce error. \index{Mode|(}There are multiple resources to help researchers draft questions for different modes [e.g., @Bradburn2004; @dillman2014mode; @Fowler1989; @Tourangeau2004spacing].\index{Measurement error|)}\index{Mode|)}

#### Example: Number of pets in a household {.unnumbered #overview-design-questionnaire-ex}

As part of our survey on the average number of pets in a household, we may want to know what animal most people prefer to have as a pet. Let's say we have a question in our survey as displayed in Figure \@ref(fig:overview-pet-examp1).

```{r}
#| label: overview-pet-examp1
#| echo: false
#| fig.cap: Example question asking pet preference type
#| fig.alt: Example question asking "What animal do you prefer to have as a pet?" with response options of Dogs and Cats.
#| out.width: 70%
#| fig.align: center

knitr::include_graphics(path="images/PetExample1.png")
```

\index{Validity|(}This question may have validity issues as it only provides the options of "dogs" and "cats" to respondents, and the interpretation of the data could be incorrect. For example, if we had 100 respondents who answered the question and 50 selected dogs, then the results of this question cannot be "50% of the population prefers to have a dog as a pet," as only two response options were provided.\index{Validity|)} \index{Measurement error|)}If a respondent taking our survey prefers turtles, they could either be forced to choose a response between these two (i.e., interpret the question as "between dogs and cats, which do you prefer?" and result in measurement error)\index{Measurement error|)}, or \index{Nonresponse error|(}\index{Item nonresponse|(}they may not answer the question (which results in item nonresponse error).\index{Nonresponse error|)}\index{Item nonresponse|)} Based on this, the interpretation of this question should be, "When given a choice between dogs and cats, 50% of respondents preferred to have a dog as a pet." 

To avoid this issue, we should consider these possibilities and adjust the question accordingly. One simple way could be to add an "other" response option to give respondents a chance to provide a different response. The "other" response option could then include a way for respondents to write their other preference. For example, we could rewrite this question as displayed in Figure \@ref(fig:overview-pet-examp2).

```{r}
#| label: overview-pet-examp2
#| echo: false
#| fig.cap: Example question asking pet preference type with other specify option
#| fig.alt: Example question asking "What animal do you prefer to have as a pet?" with response options of Dogs, Cats, and Other.  The other option includes an open-ended box after for write in responses.
#| out.width: 70%
#| fig.align: center

knitr::include_graphics(path="images/PetExample2.png")
```

We can then code the responses from the open-ended box and get a better understanding of the respondent's choice of preferred pet. Interpreting this question becomes easier as researchers no longer need to qualify the results with the choices provided.

This is a simple example of how the presentation of the question and options can impact the findings. For more complex topics and questions, we must thoroughly consider how to mitigate any impacts from the presentation, formatting, wording, and other aspects. For survey analysts, reviewing not only the data but also the wording of the questions is crucial to ensure the results are presented in a manner consistent with the question asked. Chapter \@ref(c03-survey-data-documentation) provides further details on how to review existing survey documentation to inform our analyses, and Chapter \@ref(c08-communicating-results) goes into more details on communicating results.
\index{Study design|)}

## Data collection {#overview-datacollection}

\index{Data collection|(}
Once the data collection starts, we try to stick to the data collection protocol designed during pre-survey planning. However, effective researchers also prepare to adjust their plans and adapt as needed to the current progress of data collection [@Schouten2018]. Some extreme examples could be natural disasters that could prevent mailings or interviewers from getting to the sample members. This could cause an in-person survey needing to quickly pivot to a self-administered survey, or the field period could be delayed, for example. Others could be smaller in that something newsworthy occurs connected to the survey, so we could choose to play this up in communication materials. In addition to these external factors, there could be factors unique to the survey, such as lower response rates for a specific subgroup, so the data collection protocol may need to find ways to improve response rates for that specific group.
\index{Data collection|)}

## Post-survey processing {#overview-post}

After data collection, various activities need to be completed before we can analyze the survey. \index{Weighting|(}Multiple decisions made during this post-survey phase can assist us in reducing different error sources, such as weighting to account for the sample selection. Knowing the decisions made in creating the final analytic data can impact how we use the data and interpret the results.\index{Weighting|)}

### Data cleaning and imputation {#overview-post-cleaning}

\index{Imputation|(}Post-survey cleaning is one of the first steps we do to get the survey responses into an analytic dataset. Data cleaning can consist of correcting inconsistent data (e.g., with skip pattern errors or multiple questions throughout the survey being consistent with each other), editing numeric entries or open-ended responses for grammar and consistency, or recoding open-ended questions into categories for analysis. There is no universal set of fixed rules that every survey must adhere to. Instead, each survey or research study should establish its own guidelines and procedures for handling various cleaning scenarios based on its specific objectives.

\index{Processing error|(}We should use our best judgment to ensure data integrity, and all decisions should be documented and available to those using the data in the analysis. Each decision we make impacts processing error, so often, multiple people review these rules or recode open-ended data and adjudicate any differences in an attempt to reduce this error. \index{Processing error|)}

\index{Nonresponse error|(}\index{Item nonresponse|(} \index{Missing data|(}Another crucial step in post-survey processing is imputation. Often, there is item nonresponse where respondents do not answer specific questions. If the questions are crucial to analysis efforts or the research question, we may implement imputation to reduce item nonresponse error. Imputation is a technique for replacing missing or incomplete data values with estimated values. \index{Processing error|(}However, as imputation is a way of assigning values to missing data based on an algorithm or model, it can also introduce processing error, so we should consider the overall implications of imputing data compared to having item nonresponse.\index{Processing error|)}\index{Item nonresponse|)} There are multiple ways to impute data. We recommend reviewing other resources like @Kim2021 for more information. \index{Imputation|)}\index{Nonresponse error|)} \index{Missing data|)}

#### Example: Number of pets in a household {.unnumbered #overview-post-cleaning-ex}

Let's return to the question we created to ask about [animal preference](#overview-design-questionnaire-ex). The "other specify" invites respondents to specify the type of animal they prefer to have as a pet. If respondents entered answers such as "puppy," "turtle," "rabit," "rabbit," "bunny," "ant farm," "snake," "Mr. Purr," then we may wish to categorize these write-in responses to help with analysis. In this example, "puppy" could be assumed to be a reference to a "Dog" and could be recoded there. The misspelling of "rabit" could be coded along with "rabbit" and "bunny" into a single category of "Bunny or Rabbit." These are relatively standard decisions that we can make. The remaining write-in responses could be categorized in a few different ways. "Mr. Purr," which may be someone's reference to their own cat, could be recoded as "Cat," or it could remain as "Other" or some category that is "Unknown." Depending on the number of responses related to each of the others, they could all be combined into a single "Other" category, or maybe categories such as "Reptiles" or "Insects" could be created. Each of these decisions may impact the interpretation of the data, so we should document the types of responses that fall into each of the new categories and any decisions made.

### Weighting {#overview-post-weighting}

\index{Weighting|(}We can address some error sources identified in the previous sections using weighting. During the weighting process, weights are created for each respondent record. These weights allow the survey responses to generalize to the population. A weight, generally, reflects how many units in the population each respondent represents. Often, the weight is constructed such that the sum of the weights is the size of the population.

\index{Coverage error|(}\index{Adjustment error|(}Weights can address coverage, sampling, and nonresponse errors.\index{Coverage error|)}\index{Adjustment error||)} Many published surveys include an "analysis weight" variable that combines these adjustments. However, weighting itself can also introduce adjustment error, so we need to balance which types of errors should be corrected with weighting. The construction of weights is outside the scope of this book, we recommend referencing other materials if interested in weight construction [@Valliant2018weights]. Instead, this book assumes the survey has been completed, weights are constructed, and data are available to users. 

#### Example: Number of pets in a household {.unnumbered #overview-post-weighting-ex}

In the simple example of our survey, we decided to obtain a random sample from each state to select our sample members. Knowing this sampling design, we can include selection weights for analysis that account for how the sample members were selected for the survey. \index{Research topic|(}\index{Sampling frame|(}Additionally, the sampling frame may have the type of building associated with each address, so we could include the building type as a potential nonresponse weighting variable, along with some interviewer observations that may be related to our research topic of the average number of pets in a household.\index{Sampling frame|)} Combining these weights, we can create an analytic weight that analysts need to use when analyzing the data.\index{Research topic|)}\index{Weighting|)}

### Disclosure {#overview-post-disclosure}

\index{Research topic|(}Before data is released publicly, we need to ensure that individual respondents cannot be identified by the data when confidentiality is required. There are a variety of different methods that can be used. Here we describe a few of the most commonly used:

- Data swapping: We may swap specific data values across different respondents so that it does not impact insights from the data but ensures that specific individuals cannot be identified.
- Top/bottom coding: We may choose top or bottom coding to mask extreme values. For example, we may top-code income values such that households with income greater than \$500,000 are coded as "\$500,000 or more" with other incomes being presented as integers between \$0 and \$499,999. This can impact analyses at the tails of the distribution.
- Coarsening: We may use coarsening to mask unique values.  For example, a survey question may ask for a precise income but the public data may include income as a categorical variable. Another example commonly used in survey practice is to coarsen geographic variables.  Data collectors likely know the precise address of sample members, but the public data may only include the state or even region of respondents.
- Perturbation: We may add random noise to outcomes. As with swapping, this is done so that it does not impact insights from the data but ensures that specific individuals cannot be identified.

There is as much art as there is science to the methods used for disclosure. Only high-level comments about the disclosure are provided in the survey documentation, not specific details. This ensures nobody can reverse the disclosure and thus identify individuals. For more information on different disclosure methods, please see @Skinner2009 and the [AAPOR Standards](https://aapor.org/standards-and-ethics/disclosure-standards/).

### Documentation {#overview-post-documentation}

Documentation is a critical step of the survey life cycle. We should systematically record all the details, decisions, procedures, and methodologies to ensure transparency, reproducibility, and the overall quality of survey research.

Proper documentation allows analysts to understand, reproduce, and evaluate the study's methods and findings. Chapter \@ref(c03-survey-data-documentation) dives into how analysts should use survey data documentation.

## Post-survey data analysis and reporting

After completing the survey life cycle, the data are ready for analysts. Chapter \@ref(c04-getting-started) continues from this point. For more information on the survey life cycle, please explore the references cited throughout this chapter.

<!--chapter:end:02-overview-surveys.Rmd-->

# Survey data documentation {#c03-survey-data-documentation}

```{r}
#| label: understand-pkgs
#| echo: FALSE
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
```

## Introduction

Survey documentation helps us prepare before we look at the actual survey data. The documentation includes technical guides, questionnaires, codebooks, errata, and other useful resources. By taking the time to review these materials, we can gain a comprehensive understanding of the survey data (including research and design decisions discussed in Chapters \@ref(c02-overview-surveys) and \@ref(c10-sample-designs-replicate-weights)) and conduct our analysis more effectively.

Survey documentation can vary in organization, type, and ease of use. The information may be stored in any format---PDFs, Excel spreadsheets, Word documents, and so on. Some surveys bundle documentation together, such as providing the codebook and questionnaire in a single document. Others keep them in separate files. Despite these variations, we can gain a general understanding of the documentation types and what aspects to focus on in each.

## Types of survey documentation

### Technical documentation

The technical documentation, also known as user guides or methodology/analysis guides, highlights the variables necessary to specify the survey design. We recommend concentrating on these key sections:

  * Introduction: The introduction orients us to the survey. This section provides the project's background, the study's purpose, and \index{Research topic|(}the main research questions.\index{Research topic|)}
  * Study design: The study design section describes how researchers prepared and administered the survey.
  * \index{Sampling error|(}\index{Sampling frame|(}\index{Sample|(}Sample: The sample section describes the sample frame, any known sampling errors, and limitations of the sample.\index{Sampling frame|)} \index{Weighting|(}This section can contain recommendations on how to use sampling weights. Look for weight information, whether the survey design contains strata, clusters/PSUs, or replicate weights. Also, look for population sizes, finite population correction, or replicate weight scaling information. Additional detail on sample designs is available in Chapter \@ref(c10-sample-designs-replicate-weights).\index{Sampling error|)}\index{Sample|)}\index{Weighting|)}
  * Notes on fielding: Any additional notes on fielding, such as response rates, may be found in the technical documentation.

The technical documentation may include other helpful resources. For example, some technical documentation includes syntax for SAS, SUDAAN, Stata, and/or R, so we do not have to create this code from scratch.

### Questionnaires

\index{Questionnaire|(}A questionnaire is a series of questions used to collect information from people in a survey. It can ask about opinions, behaviors, demographics, or even just numbers like the count of lightbulbs, square footage, or farm size. Questionnaires can employ different types of questions, such as closed-ended (e.g., select one or check all that apply), open-ended (e.g., numeric or text), Likert scales (e.g., a 5- or 7-point scale specifying a respondent's level of agreement to a statement), or ranking questions (e.g., a list of options that a respondent ranks by preference). It may randomize the display order of responses or include instructions that help respondents understand the questions. A survey may have one questionnaire or multiple, depending on its scale and scope.

The questionnaire is another important resource for understanding and interpreting the survey data (see Section \@ref(overview-design-questionnaire)), and we should use it alongside any analysis. It provides details about each of the questions asked in the survey, such as question name, question wording, response options, skip logic, randomizations, display specifications, mode differences, and the universe (the subset of respondents who were asked a question).

\index{American National Election Studies (ANES)|(}
In Figure \@ref(fig:understand-que-examp), we show an example from the American National Election Studies (ANES) 2020 questionnaire [@anes-svy]. The figure shows the question name (`POSTVOTE_RVOTE`), description (Did R Vote?), full wording of the question and responses, response order, universe, question logic (this question was only asked if `vote_pre` = 0), and other specifications. The section also includes the variable name, which we can link to the codebook.

```{r}
#| label: understand-que-examp
#| echo: false
#| fig.cap: ANES 2020 questionnaire example
#| fig.alt: Question information about the variable postvote_rvote from ANES 2020 questionnaire Survey question, Universe, Logic, Web Spec, Response Order, and Released Variable are included. 

knitr::include_graphics(path = "images/questionnaire-example.jpg")
```

\index{American National Election Studies (ANES)|)}

The content and structure of questionnaires vary depending on the specific survey. For instance, question names may be informative (like the ANES example above), sequential, or denoted by a code. In some cases, surveys may not use separate names for questions and variables. Figure \@ref(fig:understand-que-examp-2) shows an example from the Behavioral Risk Factor Surveillance System (BRFSS) questionnaire that shows a sequential question number and a coded variable name (as opposed to a question name) [@brfss-svy].

```{r}
#| label: understand-que-examp-2
#| echo: false
#| fig.cap: BRFSS 2021 questionnaire example
#| fig.alt: Question information about the variable BPHIGH6 from BRFSS 2021 questionnaire. Question number, question text, variable names, responses, skip info and CATI note, interviewer notes, and columns are included. 

knitr::include_graphics(path = "images/questionnaire-example-2.jpg")
```

\index{Mode|(}We should factor in the details of a survey when conducting our analyses. For example, surveys that use various modes (e.g., web and mail) may have differences in question wording or skip logic, as web surveys can include fills or automate skip logic. If large enough, these variations could warrant separate analyses for each mode.\index{Mode|)} \index{Questionnaire|)}

### Codebooks

\index{Missing data|(} \index{Codebook|(} \index{Data dictionary|see {Codebook}}
While a questionnaire provides information about the questions posed to respondents, the codebook explains how the survey data were coded and recorded. It lists details such as variable names, variable labels, variable meanings, codes for missing data, value labels, and value types (whether categorical, continuous, etc.). The codebook helps us understand and use the variables appropriately in our analysis. In particular, the codebook (as opposed to the questionnaire) often includes information on missing data. Note that the term data dictionary is sometimes used interchangeably with codebook, but a data dictionary may include more details on the structure and elements of the data.
\index{Missing data|)}

\index{American National Election Studies (ANES)|(}
Figure \@ref(fig:understand-codebook-examp) is a question from the ANES 2020 codebook [@anes-cb]. This section indicates a variable's name (`V202066`), question wording, value labels, universe, and associated survey question (`POSTVOTE_RVOTE`).

```{r}
#| label: understand-codebook-examp
#| echo: false
#| fig.cap: ANES 2020 codebook example
#| fig.alt: Variable information about the variable V202066 from ANES 2020 questionnaire Variable meaning, Value labels, Universe, and Survey Question(s) are included. 

knitr::include_graphics(path="images/codebook-example.jpg")
```

Reviewing the questionnaires and codebooks in parallel can clarify how to interpret the variables (Figures \@ref(fig:understand-que-examp) and \@ref(fig:understand-codebook-examp)), as questions and variables do not always correspond directly to each other in a one-to-one mapping. A single question may have multiple associated variables, or a single variable may summarize multiple questions. \index{American National Election Studies (ANES)|)}
\index{Codebook|)} 

### Errata

An erratum (singular) or errata (plural) is a document that lists errors found in a publication or dataset. The purpose of an erratum is to correct or update inaccuracies in the original document. Examples of errata include:

* Issuing a corrected data table after realizing a typo or mistake in a table cell
* Reporting incorrectly programmed skips in an electronic survey where questions are skipped by the respondent when they should not have been

For example, the 2004 ANES dataset released an erratum, notifying analysts to remove a specific row from the data file due to the inclusion of a respondent who should not have been part of the sample. Adhering to an issued erratum helps us increase the accuracy and reliability of analysis.

### Additional resources

Survey documentation may include additional material, such as interviewer instructions or "show cards" provided to respondents during interviewer-administered surveys to help respondents answer questions. Explore the survey website to find out what resources were used and in what contexts.

## Missing data coding

\index{Missing data|(}
Some observations in a dataset may have missing data. This can be due to design or nonresponse, and these concepts are detailed in Chapter \@ref(c11-missing-data). In that chapter, we also discuss how to analyze data with missing values. This chapter walks through how to understand documentation related to missing data.

\index{Codebook|(} 
The survey documentation, often the codebook, represents the missing data with a code. The codebook may list different codes depending on why certain data points are missing. In the example of variable `V202066` from the ANES (Figure \@ref(fig:understand-codebook-examp)), `-9` represents "Refused," `-7` means that the response was deleted due to an incomplete interview, `-6` means that there is no response because there was no follow-up interview, and `-1` means "Inapplicable" (due to a designed skip pattern). 

\index{National Crime Victimization Survey (NCVS)|(}
As another example, there may be a summary variable that describes the missingness of a set of variables --- particularly with "select all that apply" or "multiple response" questions. In the National Crime Victimization Survey (NCVS), respondents who are victims of a crime and saw the offender are asked if the offender had a weapon and then asked what the type of weapon was. This part of the questionnaire from 2021 is shown in Figure \@ref(fig:understand-ncvs-weapon-q) [@ncvs_survey_2020].

```{r}
#| label: understand-ncvs-weapon-q
#| echo: false
#| fig.cap: Excerpt from the NCVS 2020-2021 Crime Incident Report - Weapon Type
#| fig.alt: Questions 22 and 23a from the NCVS 2020-2021 Crime Incident Report, see https://bjs.ojp.gov/content/pub/pdf/ncvs20_cir.pdf 

knitr::include_graphics(path="images/questionnaire-ncvs-weapon.jpg")
```

For these multiple response variables (select all that apply), the NCVS codebook includes what they call a "lead-in" variable that summarizes the response. This lead-in variable provides metadata information on how a respondent answered the question. For example, question 23a on the weapon type, the lead-in variable is V4050 (shown in Figure \@ref(fig:understand-ncvs-weapon-cb)) indicates the quality and type of response [@ncvs_cb_2020]. In the codebook, this variable is then followed by a set of variables for each weapon type. An example of one of the individual variables from the codebook, the handgun (V4051), is shown in Figure \@ref(fig:understand-ncvs-weapon-cb-hg) [@ncvs_cb_2020]. We will dive into how to analyze this variable in Chapter \@ref(c11-missing-data).

```{r}
#| label: understand-ncvs-weapon-cb
#| echo: false
#| fig.cap: Excerpt from the NCVS 2021 Codebook for V4050 - LI WHAT WAS WEAPON
#| fig.alt: Codebook includes location of variable (files and columns), variable type (numeric), question (What was the weapon? Anything else?), and the coding of this lead in variable
knitr::include_graphics(path="images/codebook-ncvs-weapon-li.jpg")
```


```{r}
#| label: understand-ncvs-weapon-cb-hg
#| echo: false
#| fig.cap: "Excerpt from the NCVS 2021 Codebook for V4051 - C WEAPON: HAND GUN"
#| fig.alt: Codebook includes location of variable (files and columns), variable type (numeric), question (What was the weapon? Anything else?), and the coding of this categorical variable
knitr::include_graphics(path="images/codebook-ncvs-weapon-handgun.jpg")
```

When data are read into R, some values may be system missing, that is they are coded as `NA` even if that is not evident in a codebook. We discuss in Chapter \@ref(c11-missing-data) how to analyze data with `NA` values and review how R handles missing data in calculations.
\index{National Crime Victimization Survey (NCVS)|)} \index{Missing data|)} \index{Codebook|)} 

## Example: ANES 2020 survey documentation 

\index{American National Election Studies (ANES)|(}
Let's look at the survey documentation for the ANES 2020 and the documentation from their [website](https://electionstudies.org/data-center/2020-time-series-study/). Navigating to "User Guide and Codebook" [@anes-cb], we can download the PDF that contains the survey documentation, titled "ANES 2020 Time Series Study Full Release: User Guide and Codebook." Do not be daunted by the 796-page PDF. Below, we focus on the most critical information.

#### Introduction {-}

The first section in the User Guide explains that the ANES 2020 Times Series Study continues a series of election surveys conducted since 1948. These surveys contain data on public opinion and voting behavior in the U.S. presidential elections. \index{Mode|(}The introduction also includes information about the modes used for data collection (web, live video interviewing, or CATI).\index{Mode|)} Additionally, there is a summary of the number of pre-election interviews (8,280) and post-election re-interviews (7,449).

#### Sample design and respondent recruitment {-}

\index{Mode|(}The section "Sample Design and Respondent Recruitment" provides more detail about the survey's sequential mixed-mode design. All three modes were conducted one after another and not at the same time.\index{Mode|)} Additionally, it indicates that for the 2020 survey, they resampled all respondents who participated in the 2016 ANES, along with a newly drawn cross-section:

> The target population for the fresh cross-section was the 231 million non-institutional U.S. citizens aged 18 or older living in the 50 U.S. states or the District of Columbia.

The document continues with more details on the sample groups. 

#### Data analysis, weights, and variance estimation {-}

\index{Weighting|(}The section "Data Analysis, Weights, and Variance Estimation" includes information on weights and strata/cluster variables. Reading through, we can find the full sample weight variables:

> For analysis of the complete set of cases using pre-election data only, including all cases and representative of the 2020 electorate, use the full sample pre-election weight, **V200010a**. For analysis including post-election data for the complete set of participants (i.e., analysis of post-election data only or a combination of pre- and post-election data), use the full sample post-election weight, **V200010b**. Additional weights are provided for analysis of subsets of the data...

The document provides more information about the design variables, summarized in Table \@ref(tab:aneswgts).

Table: (\#tab:aneswgts) Weight and variance information for ANES

For weight | Variance unit/cluster | Variance stratum
:-----------:|:-----------:|:-----------:
V200010a| V200010c| V200010d
V200010b| V200010c| V200010d

### Methodology {-}

The user guide mentions a supplemental document called "How to Analyze ANES Survey Data" [@debell] as a how-to guide for analyzing the data. In this document, we learn more about the weights, and that they sum to the sample size and not the population. If our goal is to calculate estimates for the entire U.S. population instead of just the sample, we must adjust the weights to the U.S. population. To create accurate weights for the population, we need to determine the total population size at the time of the survey. Let's review the "Sample Design and Respondent Recruitment" section for more details:

> The target population for the fresh cross-section was the 231 million non-institutional U.S. citizens aged 18 or older living in the 50 U.S. states or the District of Columbia.

\index{Current Population Survey (CPS)|(}
The documentation suggests that the population should equal around 231 million, but this is a very imprecise count.  Upon further investigation of the available resources, we can find the methodology file titled "Methodology Report for the ANES 2020 Time Series Study" [@anes-2020-tech]. This file states that we can use the population total from the Current Population Survey (CPS), a monthly survey sponsored by the U.S. Census Bureau and the U.S. Bureau of Labor Statistics. The CPS provides a more accurate population estimate for a specific month. Therefore, we can use the CPS to get the total population number for March 2020, when the ANES was conducted. Chapter \@ref(c04-getting-started) goes into detailed instructions on how to calculate and adjust this value in the data.
\index{Weighting|)} \index{American National Election Studies (ANES)|)} \index{Current Population Survey (CPS)|)}

<!--chapter:end:03-survey-data-documentation.Rmd-->

# (PART) Analysis {-}

# Getting started {#c04-getting-started}

```{r}
#| label: setup-styler
#| echo: false
#| message: false
knitr::opts_chunk$set(tidy = 'styler')
library(magrittr)
library(tidyselect)
```

## Introduction

This chapter provides an overview of the packages, data, and design objects we use frequently throughout this book. As mentioned in Chapter \@ref(c02-overview-surveys), understanding how a survey was conducted helps us make sense of the results and interpret findings. Therefore, we provide background on the datasets used in examples and exercises. Next, we walk through how to create the survey design objects necessary to begin an analysis. Finally, we provide an overview of the {srvyr} package and the steps needed for analysis. Please report any bugs and issues encountered while going through the book to the book's [GitHub repository](https://github.com/tidy-survey-r/tidy-survey-book).

## Setup

This section provides details on the required packages and data, as well as the steps for preparing survey design objects. For a streamlined learning experience, we recommend taking the time to walk through the code provided here and making sure everything is properly set up.

### Packages {#setup-load-pkgs}

We use several packages throughout the book, but let's install and load specific ones for this chapter. Many functions in the examples and exercises are from three packages: {tidyverse}, {survey}, and {srvyr} [@tidyverse2019; @lumley2010complex; @R-srvyr]. The packages can be installed from the Comprehensive R Archive Network (CRAN) using the code below:

```{r}
#| label: setup-install-core1
#| eval: FALSE
install.packages(c("tidyverse", "survey", "srvyr"))
```

We bundled the datasets used in the book in an R package, {srvyrexploR} [@R-srvyrexploR]. To install it from GitHub, use the {pak} package [@R-pak]:

```{r}
#| label: setup-install-core2
#| eval: FALSE
#| warning: FALSE
install.packages("pak")
pak::pak("tidy-survey-r/srvyrexploR")
```

After installing these packages, load them using the `library()` function:

```{r}
#| label: setup-pkgs-core
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(survey)
library(srvyr)
library(srvyrexploR)
```

\index{gtsummary|(} \index{gt package|(} 
The packages {broom}, {gt}, and {gtsummary} play a role in displaying output and creating formatted tables [@R-gt; @R-broom; @gtsummarysjo]. Install them with the provided code^[Note: {broom} is already included in the tidyverse, so no separate installation is required.]:

```{r}
#| label: setup-install-extra
#| eval: FALSE
install.packages(c("gt", "gtsummary"))
```

After installing these packages, load them using the `library()` function:

```{r}
#| label: setup-pkgs-extra
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(broom)
library(gt)
library(gtsummary)
```
\index{gtsummary|)} \index{gt package|)} 

\index{Current Population Survey (CPS)|(}
Install and load the {censusapi} package to access the Current Population Survey (CPS), which we use to ensure accurate weighting of a key dataset in the book [@R-censusapi]. Run the code below to install {censusapi}:

```{r}
#| label: setup-install-census
#| eval: FALSE
install.packages("censusapi")
```

After installing this package, load it using the `library()` function:

```{r}
#| label: setup-pkgs-census
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(censusapi)
```

Note that the {censusapi} package requires a Census API key, available for free from the [U.S. Census Bureau website](https://api.census.gov/data/key_signup.html) (refer to the package documentation for more information). We recommend storing the Census API key in the R environment instead of directly in the code. To do this, run the `Sys.setenv()` script below, substituting the API key where it says `YOUR_API_KEY_HERE`.

```{r}
#| label: setup-census-api-setup
#| eval: FALSE
Sys.setenv(CENSUS_KEY = "YOUR_API_KEY_HERE")
```

Then, restart the R session. Once the Census API key is stored, we can retrieve it in our R code with `Sys.getenv("CENSUS_KEY")`.
\index{Current Population Survey (CPS)|)}

There are a few other packages used in the book in limited frequency. We list them in the Prerequisite boxes at the beginning of each chapter. As we work through the book, make sure to check the Prerequisite box and install any missing packages before proceeding.

### Data

The {srvyrexploR} package contains the datasets used in the book. Once installed and loaded, explore the documentation using the `help()` function. Read the descriptions of the datasets to understand what they contain:

```{r}
#| label: setup-datapkg-help
#| eval: FALSE
help(package = "srvyrexploR")
```

This book uses two main datasets: the American National Election Studies [ANES -- @debell] and the Residential Energy Consumption Survey [RECS -- @recs-2020-tech], which are included as `anes_2020` and `recs_2020` in the {srvyrexploR} package, respectively.

#### American National Election Studies Data {-}

\index{American National Election Studies (ANES)|(}
American National Election Studies (ANES) collect data from election surveys dating back to 1948. These surveys contain information on public opinion and voting behavior in U.S. presidential elections and some midterm elections^[In the United States, presidential elections are held in years divisible by four. In other even years, there are elections at the federal level for Congress, which are referred to as midterm elections as they occur at the middle of the term of a president.]. They cover topics such as party affiliation, voting choice, and level of trust in the government. The 2020 survey (data used in this book) was fielded online, through live video interviews, or via computer-assisted telephone interviews (CATI). 

When working with new survey data, we should review the survey documentation (see Chapter \@ref(c03-survey-data-documentation)) to understand the data collection methods. The original ANES data contains variables starting with `V20` [@debell], so to assist with our analysis throughout the book, we created descriptive variable names. For example, the respondent's age is now in a variable called `Age`, and gender is in a variable called `Gender`. These descriptive variables are included in the {srvyrexploR} package. A complete overview of all variables can be found in `r if (!knitr:::is_html_output()) 'the online Appendix ('`Appendix \@ref(anes-cb)`r if (!knitr:::is_html_output()) ')'`.

Before beginning an analysis, it is useful to view the data to understand the available variables. The `dplyr::glimpse()` function produces a list of all variables, their types (e.g., function, double), and a few example values. Below, we remove variables containing a "V" followed by numbers with `select(-matches("^V\\d"))` before using `glimpse()` to get a quick overview of the data with descriptive variable names:

```{r}
#| label: setup-anes-glimpse
anes_2020 %>%
  select(-matches("^V\\d")) %>%
  glimpse()
```

From the output, we can see there are `r nrow(anes_2020 %>% select(-matches("^V\\d"))) %>% formatC(big.mark = ",")` rows and `r ncol(anes_2020 %>% select(-matches("^V\\d"))) %>% formatC(big.mark = ",")` variables in the ANES data. This output also indicates that most of the variables are factors (e.g., `InterviewMode`), while a few variables are in double (numeric) format (e.g., `Age`).
\index{American National Election Studies (ANES)|)}

#### Residential Energy Consumption Survey Data {-}

\index{Residential Energy Consumption Survey (RECS)|(}
Residential Energy Consumption Survey (RECS) is a study that measures energy consumption and expenditure in American households. Funded by the Energy Information Administration, RECS data are collected through interviews with household members and energy suppliers. These interviews take place in person, over the phone, via mail, and on the web, with modes changing over time. The survey has been fielded 14 times between 1950 and 2020. It includes questions about appliances, electronics, heating, air conditioning (A/C), temperatures, water heating, lighting, energy bills, respondent demographics, and energy assistance. 

We should read the survey documentation (see Chapter \@ref(c03-survey-data-documentation)) to understand how the data were collected and implemented. An overview of all variables can be found in `r if (!knitr:::is_html_output()) 'the online Appendix ('`Appendix \@ref(recs-cb)`r if (!knitr:::is_html_output()) ')'`. 

Before starting an analysis, we recommend viewing the data to understand the types of data and variables that are included. The `dplyr::glimpse()` function produces a list of all variables, the type of the variable (e.g., function, double), and a few example values. Below, we remove the weight variables with `select(-matches("^NWEIGHT"))` before using `glimpse()` to get a quick overview of the data:

```{r}
#| label: setup-recs-glimpse
recs_2020 %>% 
  select(-matches("^NWEIGHT")) %>% 
  glimpse()
```

From the output, we can see that the RECS data has `r nrow(recs_2020 %>% select(-matches("^NWEIGHT"))) %>% formatC(big.mark = ",")` rows and `r ncol(recs_2020 %>% select(-matches("^NWEIGHT"))) %>% formatC(big.mark = ",")` non-weight variables. This output also indicates that most of the variables are in double (numeric) format (e.g., `TOTSQFT_EN`), with some factor (e.g., `Region`), Boolean (e.g., `ACUsed`), character (e.g., `REGIONC`), and ordinal (e.g., `YearMade`) variables. \index{Residential Energy Consumption Survey (RECS)|)}

### Design objects {#setup-des-obj}

\index{Design object|(}The design object is the backbone for survey analysis. It is where we specify the sampling design, weights, and other necessary information to ensure we account for errors in the data. Before creating the design object, we should carefully review the survey documentation to understand how to create the design object for accurate analysis.

In this section, we provide details on how to code the design object for the ANES and RECS data used in the book. However, we only provide a high-level overview to get readers started. For a deeper understanding of creating design objects for a variety of sampling designs, see Chapter \@ref(c10-sample-designs-replicate-weights).

While we recommend conducting exploratory data analysis on the original data before diving into complex survey analysis (see Chapter \@ref(c12-recommendations)), the actual survey analysis and inference should be performed with the survey design objects instead of the original survey data. For example, the ANES data is called `anes_2020`. If we create a survey design object called `anes_des`, our survey analyses should begin with `anes_des` and not `anes_2020`.  Using the survey design object ensures that our calculations appropriately account for the details of the survey design.

#### American National Election Studies Design Object {-}

\index{American National Election Studies (ANES)|(} \index{Current Population Survey (CPS)|(}
The ANES documentation [@debell] details the sampling and weighting implications for analyzing the survey data. From this documentation and as noted in Chapter \@ref(c03-survey-data-documentation), the 2020 ANES data are weighted to the sample, not the population. To make generalizations about the population, we need to weigh the data against the full population count. The ANES methodology recommends using the Current Population Survey (CPS) to determine the number of non-institutional U.S. citizens aged 18 or older living in the 50 U.S. states or D.C. in March 2020.

We can use the {censusapi} package to obtain the information needed for the survey design object. The `getCensus()` function allows us to retrieve the CPS data for March (`cps/basic/mar`) in 2020 (`vintage = 2020`). Additionally, we extract several variables from the CPS:

- month (`HRMONTH`) and year (`HRYEAR4`) of the interview: to confirm the correct time period
- age (`PRTAGE`) of the respondent: to narrow the population to 18 and older (eligible age to vote) 
- citizenship status (`PRCITSHP`) of the respondent: to narrow the population to only those eligible to vote
- final person-level weight (`PWSSWGT`)

Detailed information for these variables can be found in the [CPS data dictionary](https://www2.census.gov/programs-surveys/cps/datasets/2020/basic/2020_Basic_CPS_Public_Use_Record_Layout_plus_IO_Code_list.txt).

```{r}
#| label: setup-anes-cps-get
#| message: false

cps_state_in <- getCensus(name = "cps/basic/mar",
                          vintage = 2020,
                          region = "state",
                          vars = c("HRMONTH", "HRYEAR4", 
                                   "PRTAGE", "PRCITSHP", "PWSSWGT"), 
                          key = Sys.getenv("CENSUS_KEY"))

cps_state <- cps_state_in %>%
  as_tibble() %>%
  mutate(across(.cols = everything(),
                .fns = as.numeric))
```

In the code above, we include `region = "state"`. The default region type for the CPS data is at the state level. While not required, including the region can be helpful for understanding the geographical context of the data.

In `getCensus()`, we filtered the dataset by specifying the month (`HRMONTH == 3`) and year (`HRYEAR4 == 2020`) of our request. Therefore, we expect that all interviews within our output were conducted during that particular month and year. We can confirm that the data are from March 2020 by running the code below:

```{r}
#| label: setup-anes-cps-date
cps_state %>%
  distinct(HRMONTH, HRYEAR4)
```

We can narrow down the dataset using the age and citizenship variables to include only individuals who are 18 years or older (`PRTAGE >= 18`) and have U.S. citizenship (`PRCITSHIP %in% c(1:4)`):

```{r}
#| label: setup-anes-cps-narrowresp
cps_narrow_resp <- cps_state %>%
  filter(PRTAGE >= 18,
         PRCITSHP %in% c(1:4))
```

To calculate the U.S. population from the filtered data, we sum the person weights (`PWSSWGT`):

```{r}
#| label: setup-anes-cps-targetpop
targetpop <- cps_narrow_resp %>%
  pull(PWSSWGT) %>%
  sum()

scales::comma(targetpop)
```


The population of interest in 2020 is `r scales::comma(targetpop)`. This result gives us what we need to create the survey design object for estimating population statistics. Using the `anes_2020` data, we adjust the weighting variable (`V200010b`) using the population of interest we just calculated (`targetpop`). We determine the proportion of the total weight for each individual weight (`V200010b / sum(V200010b)`) and then multiply that proportion by the calculated population of interest.
\index{Current Population Survey (CPS)|)}

```{r}
#| label: setup-anes-adjust
anes_adjwgt <- anes_2020 %>%
  mutate(Weight = V200010b / sum(V200010b) * targetpop) 
```
\index{Stratified sampling|(} \index{Functions in srvyr!as\_survey\_design|(} \index{as\_survey\_design|see {Functions in srvyr}} \index{Clustered sampling|(} \index{Primary sampling unit|(} \index{PSU|see {Primary sampling unit}} \index{Cluster|see {Primary sampling unit}}
Once we have the adjusted weights, we can refer to the rest of the documentation to create the survey design. The documentation indicates that the study uses a stratified cluster sampling design. Therefore, we need to specify variables for `strata` and `ids` (cluster) and fill in the `nest` argument. The documentation provides guidance on which strata and cluster variables to use depending on whether we are analyzing pre- or post-election data. In this book, we analyze post-election data, so we need to use the post-election weight `V200010b`, strata variable `V200010d`, and Primary Sampling Unit (PSU)/cluster variable `V200010c`. Additionally, we set `nest=TRUE` to ensure the clusters are nested within the strata. \index{Weighting|)}

```{r}
#| label: setup-anes-des
anes_des <- anes_adjwgt %>%
  as_survey_design(weights = Weight,
                   strata = V200010d,
                   ids = V200010c,
                   nest = TRUE)

anes_des
```

We can examine this new object to learn more about the survey design, such that the ANES is a "Stratified 1 - level Cluster Sampling design (with replacement) With (101) clusters." Additionally, the output displays the sampling variables and then lists the remaining variables in the dataset. This design object is used throughout this book to conduct survey analysis. \index{Stratified sampling|)} \index{Functions in srvyr!as\_survey\_design|)} \index{American National Election Studies (ANES)|)} \index{Clustered sampling|)} \index{Primary sampling unit|)}

#### Residential Energy Consumption Survey Design Object {-}

\index{Replicate weights|(} \index{Replicate weights!Jackknife} \index{Jackknife|see {Replicate weights}} \index{Residential Energy Consumption Survey (RECS)|(}
The RECS documentation [@recs-2020-tech] provides information on the survey's sampling and weighting implications for analysis. The documentation shows the 2020 RECS uses Jackknife weights, where the main analytic weight is `NWEIGHT`, and the Jackknife weights are `NWEIGHT1`-`NWEIGHT60`. We can specify these in the ``weights`` and ``repweights`` arguments in the survey design object code, respectively.

With Jackknife weights, additional information is required: `type`, `scale`, and `mse`.  Chapter \@ref(c10-sample-designs-replicate-weights) discusses in depth each of these arguments; but to quickly get started, the RECS documentation lets us know that `type=JK1`, `scale=59/60`, and `mse = TRUE`. \index{Functions in srvyr!as\_survey\_rep|(}We can use the following code to create the survey design object: \index{as\_survey\_rep|see {Functions in srvyr}} \index{Replicate weights!Jackknife}

```{r}
#| label: setup-recs-des

recs_des <- recs_2020 %>%
  as_survey_rep(
    weights = NWEIGHT,
    repweights = NWEIGHT1:NWEIGHT60,
    type = "JK1",
    scale = 59 / 60,
    mse = TRUE
  )

recs_des
```

Viewing this new object provides information about the survey design, such that RECS is an "Unstratified cluster jacknife (JK1) with 60 replicates and MSE variances."  Additionally, the output shows the sampling variables (`NWEIGHT1`-`NWEIGHT60`) and then lists the remaining variables in the dataset. This design object is used throughout this book to conduct survey analysis. \index{Functions in srvyr!as\_survey\_rep|)} \index{Replicate weights|)} \index{Residential Energy Consumption Survey (RECS)|)}

## Survey analysis process {#survey-analysis-process}

\index{Survey analysis process|(}

There is a general process for analyzing data to create estimates with {srvyr} package:

1. Create a `tbl_svy` object (a survey object) using: `as_survey_design()` or `as_survey_rep()`

2. Subset data (if needed) using `filter()` (to create subpopulations)

3. Specify domains of analysis using `group_by()` 

4. Within `summarize()`, specify variables to calculate, including means, totals, proportions, quantiles, and more

In Section \@ref(setup-des-obj), we follow Step 1 to create the survey design objects for the ANES and RECS data featured in this book. Additional details on how to create design objects can be found in Chapter \@ref(c10-sample-designs-replicate-weights). Then, once we have the design object, we can filter the data to any subpopulation of interest (if needed). It is important to filter the data after creating the design object. This ensures that we are accurately accounting for the survey design in our calculations. Finally, we can use `group_by()`, `summarize()`, and other functions from the {survey} and {srvyr} packages to analyze the survey data by estimating means, totals, and so on.

\index{Survey analysis process|)}\index{Design object|)}

## Similarities between {dplyr} and {srvyr} functions {#similarities-dplyr-srvyr}

The {dplyr} package from the tidyverse offers flexible and intuitive functions for data wrangling [@R-dplyr]. One of the major advantages of using {srvyr} is that it applies {dplyr}-like syntax to the {survey} package [@R-srvyr]. We can use pipes, such as `%>%` from the {magrittr} package, to specify a survey design object, apply a function, and then feed that output into the next function's first argument [@R-magrittr]. Functions follow the 'tidy' convention of snake_case function names.

To help explain the similarities between {dplyr} functions and {srvyr} functions, we use the `towny` dataset from the {gt} package and `apistrat` data that comes in the {survey} package. The `towny` dataset provides population data for municipalities in Ontario, Canada on census years between 1996 and 2021. Taking a look at `towny` with `dplyr::glimpse()`, we can see the dataset has `r ncol(towny)` columns with a mix of character and numeric data.

```{r}
#| label: setup-towny-surveydata
towny %>% 
  glimpse()
```

Let's examine the `towny` object's class. We verify that it is a tibble, as indicated by `"tbl_df"`, by running the code below:

```{r}
#| label: setup-towny-class
class(towny)
```

All tibbles are data.frames, but not all data.frames are tibbles. Compared to data.frames, tibbles have some advantages, with the printing behavior being a noticeable advantage. When working with tidyverse style code, we recommend making all your datasets tibbles for ease of analysis.

The {survey} package contains datasets related to the California Academic Performance Index, which measures student performance in schools with at least 100 students in California. We can access these datasets by loading the {survey} package and running `data(api)`. 

\index{Stratified sampling|(} \index{Functions in srvyr!as\_survey\_design|(}
Let's work with the `apistrat` dataset, which is a stratified random sample, stratified by school type (`stype`) with three levels: `E` for elementary school, `M` for middle school, and `H` for high school. We first create the survey design object (see Chapter \@ref(c10-sample-designs-replicate-weights) for more information). The sample is stratified by the `stype` variable and the sampling weights are found in the `pw` variable. We can use this information to construct the design object, `apistrat_des`.  \index{Stratified sampling|)}

```{r}
#| label: setup-api-surveydata
data(api)

apistrat_des <- apistrat %>%
  as_survey_design(strata = stype, 
                   weights = pw)
```

When we check the class of `apistrat_des`, it is not a typical `data.frame`. Applying the `as_survey_design()` function transforms the data into a `tbl_svy`, a special class specifically for survey design objects. The {srvyr} package is designed to work with the `tbl_svy` class of objects. \index{Functions in srvyr!as\_survey\_design|)}

```{r}
#| label: setup-api-class
class(apistrat_des)
```

Let's look at how {dplyr} works with regular data frames. The example below calculates the mean and median for the `land_area_km2` variable in the `towny` dataset.

```{r}
#| label: setup-dplyr-examp
towny %>%
  summarize(area_mean = mean(land_area_km2),
            area_median = median(land_area_km2))
```

In the code below, we calculate the mean and median of the variable `api00` using `apistrat_des`. Note the similarity in the syntax. However, the standard error of the statistic is also calculated in addition to the statistic itself. \index{Functions in srvyr!survey\_mean|(} \index{Functions in srvyr!summarize|(} \index{summarize|see {Functions in srvyr}} \index{survey\_mean|see {Functions in srvyr}}

```{r}
#| label: setup-srvyr-examp
apistrat_des %>%
  summarize(api00_mean = survey_mean(api00),
            api00_med = survey_median(api00))
```

The functions in {srvyr} also play nicely with other tidyverse functions. For example, if we wanted to select columns with shared characteristics, we can use {tidyselect} functions such as `starts_with()`, `num_range()`, etc. [@R-tidyselect]. In the examples below, we use a combination of `across()` and `starts_with()` to calculate the mean of variables starting with "population" in the `towny` data frame and those beginning with `api` in the `apistrat_des` survey object.  \index{Functions in srvyr!summarize|)}

```{r}
#| label: setup-dplyr-select
towny %>%
  summarize(across(starts_with("population"), 
                   ~mean(.x, na.rm = TRUE)))
```

```{r}
#| label: setup-srvyr-select
apistrat_des %>%
  summarize(across(starts_with("api"), 
                   survey_mean))
```
\index{Functions in srvyr!survey\_mean|)}

We have the flexibility to use {dplyr} verbs such as `mutate()`, `filter()`, and `select()` on our survey design object. As mentioned in Section \@ref(survey-analysis-process), these steps should be performed on the survey design object. This ensures our survey design is properly considered in all our calculations.

```{r}
#| label: setup-srvyr-mutate
apistrat_des_mod <- apistrat_des %>%
  mutate(api_diff = api00 - api99) %>%
  filter(stype == "E") %>%
  select(stype, api99, api00, api_diff, api_students = api.stu)

apistrat_des_mod

apistrat_des
```

Several functions in {srvyr} must be called within `srvyr::summarize()`, with the exception of \index{Functions in srvyr!survey\_count}`srvyr::survey_count()` and \index{Functions in srvyr!survey\_tally}`srvyr::survey_tally()`. This is similar to how `dplyr::count()` and `dplyr::tally()` are not called within `dplyr::summarize()`. The `summarize()` function can be used in conjunction with the `group_by()` function or `by/.by` arguments, which applies the functions on a group-by-group basis to create grouped summaries. \index{survey\_count|see {Functions in srvyr}}

```{r}
#| label: setup-dplyr-groupby
towny %>%
  group_by(csd_type) %>%
  dplyr::summarize(area_mean = mean(land_area_km2),
                   area_median = median(land_area_km2))
```

We use a similar setup to summarize data in {srvyr}: \index{Functions in srvyr!survey\_mean|(} \index{Functions in srvyr!survey\_median|(} \index{Functions in srvyr!summarize|(} \index{survey\_median|see {Functions in srvyr}}

```{r}
#| label: setup-srvyr-groupby
apistrat_des %>%
  group_by(stype) %>%
  summarize(api00_mean = survey_mean(api00),
            api00_median = survey_median(api00))
```

An alternative way to do grouped analysis on the `towny` data would be with the `.by` argument:

```{r}
#| label: setup-dplyr-by-alt
towny %>%
  dplyr::summarize(area_mean = mean(land_area_km2),
                   area_median = median(land_area_km2), 
                   .by=csd_type)
```

The `.by` syntax is similarly implemented in {srvyr} for grouped analysis: 

```{r}
#| label: setup-srvyr-by-alt
apistrat_des %>%
  summarize(api00_mean = survey_mean(api00),
            api00_median = survey_median(api00),
            .by = stype)
```
\index{Functions in srvyr!survey\_median|)}

As mentioned above, {srvyr} functions are meant for `tbl_svy` objects. Attempting to manipulate data on non-`tbl_svy` objects, like the `towny` example shown below, results in an error. Running the code lets us know what the issue is: `Survey context not set`.

```{r}
#| label: setup-nsobj-error
#| error: true
towny %>%
  summarize(area_mean = survey_mean(land_area_km2))
```
 \index{Functions in srvyr!survey\_mean|)}
 
A few functions in {srvyr} have counterparts in {dplyr}, such as `srvyr::summarize()` and `srvyr::group_by()`. Unlike {srvyr}-specific verbs, {srvyr} recognizes these parallel functions if applied to a non-survey object. Instead of causing an error, the package provides the equivalent output from {dplyr}: 

```{r}
#| label: setup-nsobj-noerr
towny %>%
  srvyr::summarize(area_mean = mean(land_area_km2))
```

Because this book focuses on survey analysis, most of our pipes stem from a survey object. When we load the {dplyr} and {srvyr} packages, the functions automatically figure out the class of data and use the appropriate one from {dplyr} or {srvyr}. Therefore, we do not need to include the namespace for each function (e.g., `srvyr::summarize()`). \index{Functions in srvyr!summarize|)}

<!--chapter:end:04-set-up.Rmd-->

# Descriptive analyses {#c05-descriptive-analysis}

```{r}
#| label: desc-styler
#| include: false
knitr::opts_chunk$set(tidy = 'styler')
options(pillar.max_dec_width = 19)
```

::: {.prereqbox-header}
`r if (knitr:::is_html_output()) '### Prerequisites {- #prereq5}'`
:::

::: {.prereqbox data-latex="{Prerequisites}"}
For this chapter, load the following packages:
```{r}
#| label: desc-setup
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(srvyr)
library(srvyrexploR)
library(broom)
```

We are using data from ANES and RECS described in Chapter \@ref(c04-getting-started). As a reminder, here is the code to create the design objects for each to use throughout this chapter. For ANES, we need to adjust the weight so it sums to the population instead of the sample (see the ANES documentation and Chapter \@ref(c04-getting-started) for more information).

```{r}
#| label: desc-anes-des
#| eval: FALSE
targetpop <- 231592693

anes_adjwgt <- anes_2020 %>%
  mutate(Weight = Weight / sum(Weight) * targetpop)

anes_des <- anes_adjwgt %>%
  as_survey_design(
    weights = Weight,
    strata = Stratum,
    ids = VarUnit,
    nest = TRUE
  )
```

For RECS, details are included in the RECS documentation and Chapters \@ref(c04-getting-started) and \@ref(c10-sample-designs-replicate-weights).

```{r}
#| label: desc-recs-des
#| eval: FALSE

recs_des <- recs_2020 %>%
  as_survey_rep(
    weights = NWEIGHT,
    repweights = NWEIGHT1:NWEIGHT60,
    type = "JK1",
    scale = 59/60,
    mse = TRUE
  )
```
:::

## Introduction

\index{Point estimates|(}\index{Uncertainty estimates|(}Descriptive analyses, such as basic counts, cross-tabulations, or means, are among the first steps in making sense of our survey results. During descriptive analyses, we calculate point estimates of unknown population parameters, such as population mean, and uncertainty estimates, such as confidence intervals. By reviewing the findings, we can glean insight into the data, the underlying population, and any unique aspects of the data or population. For example, if only 10% of survey respondents are male, it could indicate a unique population, a potential error or bias, an intentional survey sampling method, or other factors. Additionally, descriptive analyses provide summaries of distribution and other measures. These analyses lay the groundwork for the next steps of running statistical tests or developing models.\index{Point estimates|)}\index{Uncertainty estimates|)}

We discuss many different types of descriptive analyses in this chapter. However, it is important to know what type of data we are working with and which statistics are appropriate. In survey data, we typically consider data as one of four main types:

  * \index{Categorical data|(}\index{Nominal data|see {Categorical data}}Categorical/nominal data: variables with levels or descriptions that cannot be ordered, such as the region of the country (North, South, East, and West)\index{Categorical data|)}
  * \index{Ordinal data|(}Ordinal data: variables that can be ordered, such as those from a Likert scale (strongly disagree, disagree, agree, and strongly agree)\index{Ordinal data|)}
  * \index{Discrete data|(}Discrete data: variables that are counted or measured, such as number of children\index{Discrete data|)}
  * \index{Continuous data|(}Continuous data: variables that are measured and whose values can lie anywhere on an interval, such as income\index{Continuous data|)}

This chapter discusses how to analyze measures of distribution (e.g., cross-tabulations), central tendency (e.g., means), relationship (e.g., ratios), and dispersion (e.g., standard deviation) using functions from the {srvyr} package [@R-srvyr].

\index{Measures of distribution|(}

Measures of distribution describe how often an event or response occurs. These measures include counts and totals. We cover the following functions:

  * Count of observations (`survey_count()` and `survey_tally()`)
  * Summation of variables (`survey_total()`)

\index{Measures of distribution|)}

\index{Central tendency|(}

Measures of central tendency find the central (or average) responses. These measures include means and medians. We cover the following functions:

  * Means and proportions (`survey_mean()` and `survey_prop()`) 
  * Quantiles and medians (`survey_quantile()` and `survey_median()`)

\index{Central tendency|)}

\index{Relationship|(}

Measures of relationship describe how variables relate to each other. These measures include correlations and ratios. We cover the following functions:

  * Correlations (`survey_corr()`)
  * Ratios (`survey_ratio()`)

\index{Relationship|)}

\index{Measures of dispersion|(}

Measures of dispersion describe how data spread around the central tendency for continuous variables. These measures include standard deviations and variances. We cover the following functions:

  * Variances and standard deviations (`survey_var()` and `survey_sd()`)

\index{Measures of dispersion|(}

To incorporate each of these survey functions, recall the general process for survey estimation from Chapter \@ref(c04-getting-started):  

\index{Survey analysis process|(}

1. Create a `tbl_svy` object using `srvyr::as_survey_design()` or `srvyr::as_survey_rep()`.
2. Subset the data for subpopulations using `srvyr::filter()`, if needed.
3. Specify domains of analysis using `srvyr::group_by()`, if needed.
4. Analyze the data with survey-specific functions.

\index{Survey analysis process|)}

This chapter walks through how to apply the survey functions in Step 4. Note that unless otherwise specified, our estimates are weighted as a result of setting up the survey design object. 

To look at the data by different subgroups, we can choose to filter and/or group the data. It is very important that we filter and group the data only after creating the design object. This ensures that the results accurately reflect the survey design. If we filter or group data before creating the survey design object, the data for those cases are not included in the survey design information and estimations of the variance, leading to inaccurate results.

For the sake of simplicity, we've removed cases with missing values in the examples below. For a more detailed explanation of how to handle missing data, please refer to Chapter \@ref(c11-missing-data).

## Counts and cross-tabulations

\index{Functions in srvyr!survey\_tally|(} \index{Functions in srvyr!survey\_count|(} \index{survey\_tally|see {Functions in srvyr}} \index{Categorical data|(} \index{Cross-tabulation|(} \index{Measures of distribution|(} 
Using `survey_count()` and `survey_tally()`, we can calculate the estimated population counts for a given variable or combination of variables. These summaries, often referred to as cross-tabulations or cross-tabs, are applied to categorical data. They help in estimating counts of the population size for different groups based on the survey data. 
\index{Categorical data|)}

### Syntax {#desc-count-syntax} 


The syntax for `survey_count()` is similar to the `dplyr::count()` syntax, as mentioned in Chapter \@ref(c04-getting-started). However, as noted above, this function can only be called on `tbl_svy` objects. Let's explore the syntax: 

```r
survey_count(
  x,
  ...,
  wt = NULL,
  sort = FALSE,
  name = "n",
  .drop = dplyr::group_by_drop_default(x),
  vartype = c("se", "ci", "var", "cv")
  )
```

The arguments are:

* `x`: a `tbl_svy` object created by `as_survey`
* `...`: variables to group by, passed to `group_by`
* `wt`: a variable to weight on in addition to the survey weights, defaults to `NULL`
* `sort`: how to sort the variables, defaults to `FALSE`
* `name`: the name of the count variable, defaults to `n`
* `.drop`: whether to drop empty groups
* `vartype`: type(s) of variation estimate to calculate including any of `c("se", "ci", "var", "cv")`, defaults to `se` (standard error) (see Section \@ref(desc-count-syntax) for more information)

To generate a count or cross-tabs by different variables, we include them in the (`...`) argument. This argument can take any number of variables and breaks down the counts by all combinations of the provided variables. This is similar to `dplyr::count()`. To obtain an estimate of the overall population, we can exclude any variables from the (`...`) argument or use the `survey_tally()` function. While the `survey_tally()` function has a similar syntax to the `survey_count()` function, it does not include the (`...`) or the `.drop` arguments:

```r
survey_tally(
  x,
  wt,
  sort = FALSE,
  name = "n",
  vartype = c("se", "ci", "var", "cv")
)
```

Both functions include the `vartype` argument with four different values:

* `se`: standard error
    * The estimated standard deviation of the estimate
    * Output has a column with the variable name specified in the `name` argument with a suffix of "_se"
* `ci`: confidence interval
    * The lower and upper limits of a confidence interval
    * Output has two columns with the variable name specified in the `name` argument with a suffix of "_low" and "_upp"
    * By default, this is a 95% confidence interval but can be changed by using the argument level and specifying a number between 0 and 1. For example, `level=0.8` would produce an 80% confidence interval.
* `var`: variance
    * The estimated variance of the estimate
    * Output has a column with the variable name specified in the `name` argument with a suffix of "_var"
* `cv`: coefficient of variation
    * A ratio of the standard error and the estimate
    * Output has a column with the variable name specified in the `name` argument with a suffix of "_cv"

The confidence intervals are always calculated using a symmetric t-distribution based method, given by the formula:

$$ \text{estimate} \pm t^*_{df}\times SE$$

\index{Degrees of freedom|(} \index{Primary sampling unit|(} \index{Strata|(}
where $t^*_{df}$ is the critical value from a t-distribution based on the confidence level and the degrees of freedom. By default, the degrees of freedom are based on the design or number of \index{Replicate weights}replicates, but they can be specified using the `df` argument. For survey design objects, the degrees of freedom are calculated as the number of primary sampling units (PSUs or clusters) minus the number of strata (see Chapter \@ref(c10-sample-designs-replicate-weights) for more information on PSUs, strata, and sample designs). For replicate-based objects, the degrees of freedom are calculated as one less than the rank of the matrix of replicate weight, where the number of replicates is typically the rank. Note that specifying `df = Inf` is equivalent to using a normal (z-based) confidence interval -- this is the default in {survey}. These variability types are the same for most of the survey functions, and we provide examples using different variability types throughout this chapter. \index{Degrees of freedom|)} \index{Primary sampling unit|)} \index{Strata|)}

### Examples

#### Example 1: Estimated population count {.unnumbered}

If we want to obtain the estimated number of households in the U.S. (the population of interest) using the Residential Energy Consumption Survey (RECS) data, we can use `survey_count()`. If we do not specify any variables in the `survey_count()` function, it outputs the estimated population count (`n`) and its corresponding standard error (`n_se`). \index{Residential Energy Consumption Survey (RECS)|(}

```{r}
#| label: desc-count-overall
recs_des %>%
  survey_count() 
```

```{r}
#| label: desc-count-oa-save
#| echo: FALSE
.est_pop <- recs_des %>%
  survey_count() %>%
  pull(n) %>%
  prettyNum(big.mark = ",", digits = 20)
```

Based on this calculation, the estimated number of households in the U.S. is `r sub("\\..*", "", .est_pop)`.

Alternatively, we could also use the `survey_tally()` function. The example below yields the same results as `survey_count()`. 

```{r}
#| label: desc-tally-oa
recs_des %>%
  survey_tally() 
```

#### Example 2: Estimated counts by subgroups (cross-tabs) {.unnumbered}

To calculate the estimated number of observations for specific subgroups, such as Region and Division, we can include the variables of interest in the `survey_count()` function. In the example below, we calculate the estimated number of housing units by region and division. The argument `name =` in `survey_count()` allows us to change the name of the count variable in the output from the default `n` to `N`. 

```{r}
#| label: desc-count-group
recs_des %>%
  survey_count(Region, Division, name = "N") 
```

```{r}
#| label: desc-count-group-save
#| echo: FALSE
.est_pop_div <- recs_des %>%
  survey_count(Region, Division, name = "N")  %>%
  mutate(N = formatC(
    N,
    big.mark = ",",
    format = "f",
    digits = 0
  ))
```

When we run the cross-tab, we see that there are an estimated `r .est_pop_div %>% filter(Division=="New England") %>% pull(N)` housing units in the New England Division. 

The code results in an error if we try to use the `survey_count()` syntax with `survey_tally()`:

```{r}
#| label: desc-tally-group-bad
#| error: TRUE
recs_des %>%
  survey_tally(Region, Division, name = "N") 
```

Use a `group_by()` function prior to using `survey_tally()` to successfully run the cross-tab:

```{r}
#| label: desc-tally-group-good
recs_des %>%
  group_by(Region, Division) %>%
  survey_tally(name = "N") 
```
\index{Functions in srvyr!survey\_count|)} \index{Cross-tabulation|)}

## Totals and sums \index{Functions in srvyr!survey\_total|(} \index{survey\_total|see {Functions in srvyr}}

\index{Continuous data|(}
The `survey_total()` function is analogous to `sum`. It can be applied to continuous variables to obtain the estimated total quantity in a population. Starting from this point in the chapter, all the introduced functions must be called within `summarize()`.  \index{Functions in srvyr!summarize|(} \index{Continuous data|)}

### Syntax

Here is the syntax:

```r
survey_total(
  x,
  na.rm = FALSE,
  vartype = c("se", "ci", "var", "cv"),
  level = 0.95,
  deff = FALSE,
  df = NULL
)
```

The arguments are:

* `x`: a variable, expression, or empty
* `na.rm`: an indicator of whether missing values should be dropped, defaults to `FALSE`
* `vartype`: type(s) of variation estimate to calculate including any of `c("se", "ci", "var", "cv")`, defaults to `se` (standard error) (see Section \@ref(desc-count-syntax) for more information)
* `level`: a number or a vector indicating the confidence level, defaults to 0.95
* `deff`: a logical value stating whether the design effect should be returned, defaults to FALSE (this is described in more detail in Section \@ref(desc-deff))
* \index{Degrees of freedom|(}`df`: (for `vartype = 'ci'`), a numeric value indicating degrees of freedom for the t-distribution\index{Degrees of freedom|)}

### Examples

#### Example 1: Estimated population count {.unnumbered}

To calculate a population count estimate with `survey_total()`, we leave the argument `x` empty, as shown in the example below:

```{r}
#| label: desc-tot-nox
recs_des %>%
  summarize(Tot = survey_total())  
```

The estimated number of households in the U.S. is `r scales::comma(recs_des %>% summarize(Tot = survey_total()) %>% pull(Tot))`. Note that this result obtained from `survey_total()` is equivalent to the ones from the `survey_count()` and `survey_tally()` functions. However, the `survey_total()` function is called within `summarize()`, whereas \index{Functions in srvyr!survey\_count}`survey_count()` and `survey_tally()` are not.  \index{Functions in srvyr!survey\_tally|)} 

#### Example 2: Overall summation of continuous variables {.unnumbered}

\index{Continuous data|(}
The distinction between `survey_total()` and `survey_count()` becomes more evident when working with continuous variables. Let's compute the total cost of electricity in whole dollars from variable `DOLLAREL`^[RECS has two components: a household survey and an energy supplier survey. For each household that responds, their energy providers are contacted to obtain their energy consumption and expenditure. This value reflects the dollars spent on electricity in 2020, according to the energy supplier. See @recs-2020-meth for more details.].
\index{Continuous data|)}

```{r}
#| label: desc-tot-dollarel
recs_des %>%
  summarize(elec_bill = survey_total(DOLLAREL))
```

```{r}
#| label: desc-tot-dollarel-save
#| echo: FALSE
.elbill <- recs_des %>%
  summarize(elec_bill = survey_total(DOLLAREL)) %>%
  mutate(across(starts_with("elec"), \(x) str_c(
    "$", formatC(
      x,
      big.mark = ",",
      format = "f",
      digits = 0
    )
  )))
```

It is estimated that American residential households spent a total of `r .elbill %>% pull(elec_bill)` on electricity in 2020, and the estimate has a standard error of `r .elbill %>% pull(elec_bill_se)`.

#### Example 3: Summation by groups {.unnumbered}

Since we are using the {srvyr} package, we can use `group_by()` to calculate the cost of electricity for different groups. Let's examine the variations in the cost of electricity in whole dollars across regions and display the confidence interval instead of the default standard error. 

```{r}
#| label: desc-tot-group
recs_des %>%
  group_by(Region) %>%
  summarize(elec_bill = survey_total(DOLLAREL,
                                     vartype = "ci"))
```

```{r}
#| label: desc-tot-group-save
#| echo: FALSE
.elbil_reg <- recs_des %>%
  group_by(Region) %>%
  summarize(elec_bill = survey_total(DOLLAREL,
                                     vartype = "ci")) %>%
  mutate(across(starts_with("elec"), \(x) str_c(
    "$", prettyNum(round(x, 0), big.mark = ",", digits = 20)
  )))
```

The survey results estimate that households in the Northeast spent `r .elbil_reg %>% filter(Region=="Northeast") %>% pull(elec_bill)` with a confidence interval of (`r .elbil_reg %>% filter(Region=="Northeast") %>% pull(elec_bill_low)`, `r .elbil_reg %>% filter(Region=="Northeast") %>% pull(elec_bill_upp)`) on electricity in 2020, while households in the South spent an estimated `r .elbil_reg %>% filter(Region=="South") %>% pull(elec_bill)` with a confidence interval of (`r .elbil_reg %>% filter(Region=="South") %>% pull(elec_bill_low)`, `r .elbil_reg %>% filter(Region=="South") %>% pull(elec_bill_upp)`).

As we calculate these numbers, we may notice that the confidence interval of the South is larger than those of other regions. This implies that we have less certainty about the true value of electricity spending in the South. A larger confidence interval could be due to a variety of factors, such as a wider range of electricity spending in the South. We could try to analyze smaller regions within the South to identify areas that are contributing to more variability. Descriptive analyses serve as a valuable starting point for more in-depth exploration and analysis. \index{Functions in srvyr!survey\_total|)} \index{Measures of distribution|)}

## Means and proportions {#desc-meanprop} 

\index{Functions in srvyr!survey\_mean|(} \index{Functions in srvyr!survey\_prop|(} \index{survey\_prop|see {Functions in srvyr}} \index{Categorical data|(} \index{Continuous data|(} \index{Central tendency|(}
Means and proportions form the foundation of many research studies. These estimates are often the first things we look for when reviewing research on a given topic. The `survey_mean()` and `survey_prop()` functions calculate means and proportions while taking into account the survey design elements. The `survey_mean()` function should be used on continuous variables of survey data, while the `survey_prop()` function should be used on categorical variables.  
\index{Categorical data|)} \index{Continuous data|)}

### Syntax {#desc-meanprop-syntax}

The syntax for both means and proportions is very similar: 

```r
survey_mean(
  x,
  na.rm = FALSE,
  vartype = c("se", "ci", "var", "cv"),
  level = 0.95,
  proportion = FALSE,
  prop_method = c("logit", "likelihood", "asin", "beta", "mean"),
  deff = FALSE,
  df = NULL
)

survey_prop(
  na.rm = FALSE,
  vartype = c("se", "ci", "var", "cv"),
  level = 0.95,
  proportion = TRUE,
  prop_method = 
    c("logit", "likelihood", "asin", "beta", "mean", "xlogit"),
  deff = FALSE,
  df = NULL
)
```

Both functions have the following arguments and defaults:

  * `na.rm`: an indicator of whether missing values should be dropped, defaults to `FALSE`
  * `vartype`: type(s) of variation estimate to calculate including any of `c("se", "ci", "var", "cv")`, defaults to `se` (standard error) (see Section \@ref(desc-count-syntax) for more information)
  * `level`: a number or a vector indicating the confidence level, defaults to 0.95
  * `prop_method`: Method to calculate the confidence interval for confidence intervals
  * `deff`: a logical value stating whether the design effect should be returned, defaults to FALSE (this is described in more detail in Section \@ref(desc-deff))
  * \index{Degrees of freedom|(}`df`: (for `vartype = 'ci'`), a numeric value indicating degrees of freedom for the t-distribution\index{Degrees of freedom|)}

There are two main differences in the syntax. The `survey_mean()` function includes the first argument `x`, representing the variable or expression on which the mean should be calculated. The `survey_prop()` does not have an argument to include the variables directly. Instead, prior to `summarize()`, we must use the `group_by()` function to specify the variables of interest for `survey_prop()`. For `survey_mean()`, including a `group_by()` function allows us to obtain the means by different groups. 

The other main difference is with the `proportion` argument. The `survey_mean()` function can be used to calculate both means and proportions. Its `proportion` argument defaults to `FALSE`, indicating it is used for calculating means. If we wish to calculate a proportion using `survey_mean()`, we need to set the `proportion` argument to `TRUE`. In the `survey_prop()` function, the `proportion` argument defaults to `TRUE` because the function is specifically designed for calculating proportions.

In Section \@ref(desc-count-syntax), we provide an overview of different variability types. The confidence interval used for most measures, such as means and counts, is referred to as a Wald-type interval. However, for proportions, a Wald-type interval with a symmetric t-based confidence interval may not provide accurate coverage, especially when dealing with small sample sizes or proportions "near" 0 or 1. We can use other methods to calculate confidence intervals, which we specify using the `prop_method` option in `survey_prop()`. The options include:

  * `logit`: fits a logistic regression model and computes a Wald-type interval on the log-odds scale, which is then transformed to the probability scale. This is the default method.
  * `likelihood`: uses the (Rao-Scott) scaled chi-squared distribution for the log-likelihood from a binomial distribution.
  * `asin`: uses the variance-stabilizing transformation for the binomial distribution, the arcsine square root, and then back-transforms the interval to the probability scale.
  * `beta`: uses the incomplete beta function with an effective sample size based on the estimated variance of the proportion.
  * `mean`: the Wald-type interval ($\pm t_{df}^*\times SE$).
  * `xlogit`: uses a logit transformation of the proportion, calculates a Wald-type interval, and then back-transforms to the probability scale. This method is the same as those used by default in SUDAAN and SPSS.

Each option yields slightly different confidence interval bounds when dealing with proportions. Please note that when working with `survey_mean()`, we do not need to specify a method unless the `proportion` argument is `TRUE`. If `proportion` is `FALSE`, it calculates a symmetric `mean` type of confidence interval.

### Examples

#### Example 1: One variable proportion {.unnumbered}

If we are interested in obtaining the proportion of people in each region in the RECS data, we can use `group_by()` and `survey_prop()` as shown below: 

```{r}
#| label: desc-p-ex1
#| message: false
recs_des %>%
  group_by(Region) %>%
  summarize(p = survey_prop()) 
```

```{r}
#| label: desc-p-ex1-save
#| echo: FALSE
.preg <- recs_des %>%
  group_by(Region) %>%
  summarize(p = survey_prop()) %>%
  mutate(p = p * 100)
```

`r .preg %>% filter(Region=="Northeast") %>% pull(p) %>% signif(3)`% of the households are in the Northeast, `r .preg %>% filter(Region=="Midwest") %>% pull(p) %>% signif(3)`% are in the Midwest, and so on. Note that the proportions in column `p` add up to one.

\index{Categorical data|(}
The `survey_prop()` function is essentially the same as using `survey_mean()` with a categorical variable and without specifying a numeric variable in the `x` argument. The following code gives us the same results as above:
\index{Categorical data|)}

```{r}
#| label: desc-p-ex2
recs_des %>%
  group_by(Region) %>%
  summarize(p = survey_mean())
```

#### Example 2: Conditional proportions {.unnumbered}

We can also obtain proportions by more than one variable. In the following example, we look at the proportion of housing units by Region and whether air conditioning (A/C) is used (`ACUsed`)^[Question text: "Is any air conditioning equipment used in your home?" [@recs-svy]].

```{r}
#| label: desc-pmulti-ex1
recs_des %>%
  group_by(Region, ACUsed) %>%
  summarize(p = survey_prop())
```

When specifying multiple variables, the proportions are conditional. In the results above, notice that the proportions sum to 1 within each region. This can be interpreted as the proportion of housing units with A/C within each region. For example, in the Northeast region, approximately `r scales::percent(recs_des %>% group_by(Region, ACUsed) %>% summarize(p = survey_prop()) %>% filter(Region == "Northeast", ACUsed == "FALSE") %>% pull(p), accuracy = 0.1)` of housing units don't have A/C, while around `r scales::percent(recs_des %>% group_by(Region, ACUsed) %>% summarize(p = survey_prop()) %>% filter(Region == "Northeast", ACUsed == "TRUE") %>% pull(p), accuracy = 0.1)` have A/C.

#### Example 3: Joint proportions {.unnumbered} 
\index{Functions in srvyr!interact|(} \index{interact|see {Functions in srvyr}}

If we're interested in a joint proportion, we use the `interact()` function. In the example below, we apply the `interact()` function to `Region` and `ACUsed`:  

```{r}
#| label: desc-pmulti-ex2
recs_des %>%
  group_by(interact(Region, ACUsed)) %>%
  summarize(p = survey_prop())
```

In this case, all proportions sum to 1, not just within regions.  This means that `r scales::percent(recs_des %>% group_by(interact(Region, ACUsed)) %>% summarize(p = survey_prop()) %>% filter(Region == "Northeast", ACUsed == "TRUE") %>% pull(p), accuracy = 0.1)` of the population lives in the Northeast and has A/C. As noted earlier, we can use both the `survey_prop()` and `survey_mean()` functions, and they produce the same results. \index{Functions in srvyr!interact|)} \index{Functions in srvyr!survey\_prop|)}

#### Example 4: Overall mean {.unnumbered}

Below, we calculate the estimated average cost of electricity in the U.S. using `survey_mean()`. To include both the standard error and the confidence interval, we can include them in the `vartype` argument: 

```{r}
#| label: desc-mn-oa
recs_des %>%
  summarize(elec_bill = survey_mean(DOLLAREL,
                                    vartype = c("se", "ci")))
```

```{r}
#| label: desc-mn-oa-save
#| echo: FALSE
.elbill_mn <- recs_des %>%
  summarize(elec_bill = survey_mean(DOLLAREL,
                                    vartype = c("se", "ci"))) %>%
  mutate(across(starts_with("elec"), \(x) str_c(
    "$", prettyNum(round(x, 0), big.mark = ",", digits = 6)
  )))
```

Nationally, the average household spent `r pull(.elbill_mn, elec_bill)` in 2020.

#### Example 5: Means by subgroup {.unnumbered}

We can also calculate the estimated average cost of electricity in the U.S. by each region.  To do this, we include a `group_by()` function with the variable of interest before the `summarize()` function: 

```{r}
#| label: desc-mn-group
recs_des %>%
  group_by(Region) %>%
  summarize(elec_bill = survey_mean(DOLLAREL))
```

```{r}
#| label: desc-mn-group-save
#| echo: FALSE
.elbill_mn_reg <- recs_des %>%
  group_by(Region) %>%
  summarize(elec_bill = survey_mean(DOLLAREL)) %>%
  mutate(across(starts_with("elec"), \(x) str_c(
    "$", prettyNum(round(x, 0), big.mark = ",", digits = 6)
  )))
```

Households from the West spent approximately `r .elbill_mn_reg %>% filter(Region=="West") %>% pull(elec_bill)`, while in the South, the average spending was `r .elbill_mn_reg %>% filter(Region=="South") %>% pull(elec_bill)`. \index{Functions in srvyr!survey\_mean|)} 

## Quantiles and medians 

\index{Functions in srvyr!survey\_median} \index{Functions in srvyr!survey\_quantile|(} \index{survey\_quantile|see {Functions in srvyr}} \index{Continuous data|(}
To better understand the distribution of a continuous variable like income, we can calculate quantiles at specific points. For example, computing estimates of the quartiles (25%, 50%, 75%) helps us understand how income is spread across the population. We use the `survey_quantile()` function to calculate quantiles in survey data. 

Medians are useful for finding the midpoint of a continuous distribution when the data are skewed, as medians are less affected by outliers compared to means. The median is the same as the 50th percentile, meaning the value where 50% of the data are higher and 50% are lower. Because medians are a special, common case of quantiles, we have a dedicated function called `survey_median()` for calculating the median in survey data. Alternatively, we can use the `survey_quantile()` function with the `quantiles` argument set to `0.5` to achieve the same result. \index{Continuous data|)}

### Syntax

The syntax for `survey_quantile()` and `survey_median()` are nearly identical: 

```r
survey_quantile(
  x,
  quantiles,
  na.rm = FALSE,
  vartype = c("se", "ci", "var", "cv"),
  level = 0.95,
  interval_type = 
    c("mean", "beta", "xlogit", "asin", "score", "quantile"),
  qrule = c("math", "school", "shahvaish", "hf1", "hf2", "hf3", 
            "hf4", "hf5", "hf6", "hf7", "hf8", "hf9"),
  df = NULL
)

survey_median(
  x,
  na.rm = FALSE,
  vartype = c("se", "ci", "var", "cv"),
  level = 0.95,
  interval_type = 
    c("mean", "beta", "xlogit", "asin", "score", "quantile"),
  qrule = c("math", "school", "shahvaish", "hf1", "hf2", "hf3", 
            "hf4", "hf5", "hf6", "hf7", "hf8", "hf9"),
  df = NULL
)
```

The arguments available in both functions are:

  * `x`: a variable, expression, or empty
  * `na.rm`: an indicator of whether missing values should be dropped, defaults to `FALSE`
  * `vartype`: type(s) of variation estimate to calculate, defaults to `se` (standard error)
  * `level`: a number or a vector indicating the confidence level, defaults to 0.95
  * `interval_type`: method for calculating a confidence interval
  * `qrule`: rule for defining quantiles. The default is the lower end of the quantile interval ("math"). The midpoint of the quantile interval is the "school" rule. "hf1" to "hf9" are weighted analogs to type=1 to 9 in `quantile()`. "shahvaish" corresponds to a rule proposed by @shahvaish. See `vignette("qrule", package="survey")` for more information.
  * \index{Degrees of freedom|(}`df`: (for `vartype = 'ci'`), a numeric value indicating degrees of freedom for the t-distribution\index{Degrees of freedom|)}

The only difference between `survey_quantile()` and `survey_median()` is the inclusion of the `quantiles` argument in the `survey_quantile()` function. This argument takes a vector with values between 0 and 1 to indicate which quantiles to calculate. For example, if we wanted the quartiles of a variable, we would provide `quantiles = c(0.25, 0.5, 0.75)`. While we can specify quantiles of 0 and 1, which represent the minimum and maximum, this is not recommended. It only returns the minimum and maximum of the respondents and cannot be extrapolated to the population, as there is no valid definition of standard error. 

In Section \@ref(desc-count-syntax), we provide an overview of the different variability types. The interval used in confidence intervals for most measures, such as means and counts, is referred to as a Wald-type interval. However, this is not always the most accurate interval for quantiles. Similar to confidence intervals for proportions, quantiles have various interval types, including asin, beta, mean, and xlogit (see Section \@ref(desc-meanprop-syntax)). Quantiles also have two more methods available:

  * `score`: the Francisco and Fuller confidence interval based on inverting a score test (only available for design-based survey objects and not replicate-based objects)
  * `quantile`: \index{Replicate weights|(} \index{Replicate weights!Jackknife|(}\index{Replicate weights!Bootstrap|(}\index{Bootstrap|see {Replicate weights}}\index{Replicate weights!Balanced repeated replication (BRR)|(}\index{Balanced repeated replication (BRR)|see {Replicate weights}} based on the replicates of the quantile. This is not valid for jackknife-type replicates but is available for bootstrap and BRR replicates.\index{Replicate weights|)}\index{Replicate weights!Jackknife|)}\index{Replicate weights!Bootstrap|)}\index{Replicate weights!Balanced repeated replication (BRR)|)}

One note with the `score` method is that when there are numerous ties in the data, this method may produce confidence intervals that do not contain the estimate. When dealing with a high propensity for ties (e.g., many respondents are the same age), it is recommended to use another method. SUDAAN, for example, uses the `score` method but adds noise to the values to prevent issues. The documentation in the {survey} package indicates, in general, that the `score` method may have poorer performance compared to the beta and logit intervals [@lumley2010complex].

### Examples

#### Example 1: Overall quartiles {.unnumbered}

Quantiles provide insights into the distribution of a variable. Let's look into the quartiles, specifically, the first quartile (p=0.25), the median (p=0.5), and the third quartile (p=0.75) of electric bills. 

```{r}
#| label: desc-quantile-oa
#| eval: FALSE
recs_des %>%
  summarize(elec_bill = survey_quantile(DOLLAREL,
                                        quantiles = c(0.25, .5, 0.75)))
```

```{r}
#| label: desc-quantile-oa-print
#| echo: FALSE
recs_des %>%
  summarize(elec_bill = survey_quantile(DOLLAREL,
                                        quantiles = c(0.25, .5, 0.75))) %>%
  print(width=Inf)
```

```{r}
#| label: desc-quantile-oa-save
#| echo: FALSE
.elbill_quant <- recs_des %>%
  summarize(elec_bill = survey_quantile(DOLLAREL,
                                        quantiles = c(0.25, .5, 0.75))) %>%
  mutate(
    across(c(!ends_with("se")), \(x) str_c("$", prettyNum(
    round(x, 0), big.mark = ","))),
    across(c(ends_with("se")), \(x) str_c("$", prettyNum(
    round(x, 2), big.mark = ",")))
  )
```

The output above shows the values for the three quartiles of electric bill costs and their respective standard errors: the 25th percentile is `r .elbill_quant %>% pull(elec_bill_q25)` with a standard error of `r .elbill_quant %>% pull(elec_bill_q25_se)`, the 50th percentile (median) is `r .elbill_quant %>% pull(elec_bill_q50)` with a standard error of `r .elbill_quant %>% pull(elec_bill_q50_se)`, and the 75th percentile is `r .elbill_quant %>% pull(elec_bill_q75)` with a standard error of `r .elbill_quant %>% pull(elec_bill_q75_se)`.

#### Example 2: Quartiles by subgroup {.unnumbered}

We can estimate the quantiles of electric bills by region by using the `group_by()` function: 

```{r}
#| label: desc-quantile-reg
#| eval: false
recs_des %>%
  group_by(Region) %>%
  summarize(elec_bill = survey_quantile(DOLLAREL,
                                        quantiles = c(0.25, .5, 0.75))) 
```

```{r}
#| label: desc-quantile-reg-print
#| echo: false
recs_des %>%
  group_by(Region) %>%
  summarize(elec_bill = survey_quantile(DOLLAREL,
                                        quantiles = c(0.25, .5, 0.75))) %>%
  print(width = Inf)
```

```{r}
#| label: desc-quantile-save
#| echo: FALSE
.elbill_quant_gp <- recs_des %>%
  group_by(Region) %>%
  summarize(elec_bill = survey_quantile(DOLLAREL,
                                        quantiles = c(0.25, .5, 0.75))) %>%
  mutate(
    across(c(!ends_with("se") & where(is.numeric)), \(x) str_c("$", prettyNum(
    round(x, 0), big.mark = ","))),
    across(c(ends_with("se")), \(x) str_c("$", prettyNum(
    round(x, 1), big.mark = ",")))
  )
```


The 25th percentile for the Northeast region is `r .elbill_quant_gp %>% filter(Region=="Northeast") %>% pull(elec_bill_q25)`, while it is `r .elbill_quant_gp %>% filter(Region=="South") %>% pull(elec_bill_q25)` for the South.

#### Example 3: Minimum and maximum {.unnumbered}

As mentioned in the syntax section, we can specify quantiles of `0` (minimum) and `1` (maximum), and R calculates these values.  However, these are only the minimum and maximum values in the data, and there is not enough information to determine their standard errors: 

```{r}
#| label: desc-quantile-minmax
recs_des %>%
  summarize(elec_bill = survey_quantile(DOLLAREL,
                                        quantiles = c(0, 1))) 

```

```{r}
#| label: desc-quantile-minmax-save
#| echo: FALSE
.elbill_minmax <- recs_des %>%
  summarize(elec_bill = survey_quantile(DOLLAREL,
                                        quantiles = c(0, 1))) %>%
    mutate(
        across(!ends_with("se"), \(x) scales::dollar(round(x), format="d"))

  )
```

The minimum cost of electricity in the dataset is -`r .elbill_minmax %>% pull(elec_bill_q00)`, while the maximum is `r .elbill_minmax %>% pull(elec_bill_q100)`, but the standard error is shown as `NaN` and `0`, respectively. Notice that the minimum cost is a negative number. This may be surprising, but some housing units with solar power sell their energy back to the grid and earn money, which is recorded as a negative expenditure.

#### Example 4: Overall median {.unnumbered}

We can calculate the estimated median cost of electricity in the U.S. using the `survey_median()` function: 

```{r}
#| label: desc-med-oa
recs_des %>%
  summarize(elec_bill = survey_median(DOLLAREL))
```

```{r}
#| label: desc-med-oa-save
#| echo: FALSE
.elbill_med <- recs_des %>%
  summarize(elec_bill = survey_median(DOLLAREL)) %>%
  mutate(across(starts_with("elec"), \(x) str_c(
    "$", prettyNum(round(x, 0), big.mark = ",", digits = 6)
  )))
```

Nationally, the median household spent `r pull(.elbill_med, elec_bill)` in 2020. This is the same result as we obtained using the `survey_quantile()` function. Interestingly, the average electric bill for households that we calculated in Section \@ref(desc-meanprop) is `r pull(.elbill_mn, elec_bill)`, but the estimated median electric bill is `r pull(.elbill_med, elec_bill)`,  indicating the distribution is likely right-skewed. \index{Functions in srvyr!survey\_quantile|)}

#### Example 5: Medians by subgroup {.unnumbered}

We can calculate the estimated median cost of electricity in the U.S. by region using the `group_by()` function with the variable(s) of interest before the `summarize()` function, similar to when we found the mean by region. 

```{r}
#| label: desc-med-group
recs_des %>%
  group_by(Region) %>%
  summarize(elec_bill = survey_median(DOLLAREL))
```

```{r}
#| label: desc-med-group-save
#| echo: FALSE
.elbill_med_reg <- recs_des %>%
  group_by(Region) %>%
  summarize(elec_bill = survey_median(DOLLAREL)) %>%
  mutate(across(starts_with("elec"), \(x) str_c(
    "$", prettyNum(round(x, 0), big.mark = ",", digits = 6)
  )))
```

We estimate that households in the Northeast spent a median of `r .elbill_med_reg %>% filter(Region=="Northeast") %>% pull(elec_bill)` on electricity, and in the South, they spent a median of `r .elbill_med_reg %>% filter(Region=="South") %>% pull(elec_bill)`. \index{Functions in srvyr!survey\_median|)} \index{Central tendency|)}

## Ratios \index{Functions in srvyr!survey\_ratio|(} \index{survey\_ratio|see {Functions in srvyr}} \index{Relationship|(}

A ratio is a measure of the ratio of the sum of two variables, specifically in the form of:

$$ \frac{\sum x_i}{\sum y_i}.$$
Note that the ratio is not the same as calculating the following:

$$ \frac{1}{N} \sum \frac{x_i}{y_i} $$
which can be calculated with \index{Functions in srvyr!survey\_mean|(}`survey_mean()` by creating a derived variable $z=x/y$ and then calculating the mean of $z$. 

Say we wanted to assess the energy efficiency of homes in a standardized way, where we can compare homes of different sizes. We can calculate the ratio of energy consumption to the square footage of a home. This helps us meaningfully compare homes of different sizes by identifying how much energy is being used per unit of space. To calculate this ratio, we would run `survey_ratio(Energy Consumption in BTUs, Square Footage of Home)`. If, instead, we used `survey_mean(Energy Consumption in BTUs/Square Footage of Home)`, we would estimate the average energy consumption per square foot of all surveyed homes. While helpful in understanding general energy use, this statistic does not account for differences in home sizes.  

### Syntax

The syntax for `survey_ratio()` is as follows:

```r
survey_ratio(
  numerator,
  denominator,
  na.rm = FALSE,
  vartype = c("se", "ci", "var", "cv"),
  level = 0.95,
  deff = FALSE,
  df = NULL
)
```

The arguments are:

  * `numerator`: The numerator of the ratio
  * `denominator`: The denominator of the ratio
  * `na.rm`: A logical value to indicate whether missing values should be dropped
  * `vartype`: type(s) of variation estimate to calculate including any of `c("se", "ci", "var", "cv")`, defaults to `se` (standard error) (see Section \@ref(desc-count-syntax) for more information)
  * `level`: A single number or vector of numbers indicating the confidence level
  * `deff`: A logical value to indicate whether the design effect should be returned (this is described in more detail in Section \@ref(desc-deff))
  * \index{Degrees of freedom|(}`df`: (For vartype = "ci" only) A numeric value indicating the degrees of freedom for t-distribution\index{Degrees of freedom|)}

### Examples

#### Example 1: Overall ratios {.unnumbered}

Suppose we wanted to find the ratio of dollars spent on liquid propane per unit (in British thermal unit [Btu]) nationally^[The value of `DOLLARLP` reflects the annualized amount spent on liquid propane and `BTULP` reflects the annualized consumption in Btu of liquid propane [@recs-svy].]. To find the average cost to a household, we can use `survey_mean()`. However, to find the national unit rate, we can use `survey_ratio()`. In the following example, we show both methods and discuss the interpretation of each: 

```{r}
#| label: desc-ratio-1
#| eval: false
recs_des %>%
  summarize(
    DOLLARLP_Tot = survey_total(DOLLARLP, vartype = NULL),
    BTULP_Tot = survey_total(BTULP, vartype = NULL),
    DOL_BTU_Rat = survey_ratio(DOLLARLP, BTULP),
    DOL_BTU_Avg = survey_mean(DOLLARLP / BTULP, na.rm = TRUE)
  )
```

```{r}
#| label: desc-ratio-1-print
#| echo: false
recs_des %>%
  summarize(
    DOLLARLP_Tot = survey_total(DOLLARLP, vartype = NULL),
    BTULP_Tot = survey_total(BTULP, vartype = NULL),
    DOL_BTU_Rat = survey_ratio(DOLLARLP, BTULP),
    DOL_BTU_Avg = survey_mean(DOLLARLP / BTULP, na.rm = TRUE)
  ) %>%
  print(width = Inf)
```
\index{Functions in srvyr!survey\_mean|)}

```{r}
#| label: desc-ratio-1-save
#| echo: FALSE
.rat_out <- recs_des %>%
  summarize(
    DOLLARLP_Tot = survey_total(DOLLARLP),
    BTULP_Tot = survey_total(BTULP),
    DOL_BTU_Rat = survey_ratio(DOLLARLP, BTULP),
    DOL_BTU_Avg = survey_mean(DOLLARLP / BTULP, na.rm = TRUE),
  )

num <-
  pull(.rat_out, DOLLARLP_Tot) %>% formatC(big.mark = ",",
                                           digits = 0,
                                           format = "f")

den <-
  pull(.rat_out, BTULP_Tot) %>% formatC(big.mark = ",",
                                        digits = 0,
                                        format = "f")

rat <- pull(.rat_out, DOL_BTU_Rat) %>% signif(3)

avg <- pull(.rat_out, DOL_BTU_Avg) %>% signif(3)
```

The ratio of the total spent on liquid propane to the total consumption was `r rat`, but the average rate was `r avg`. With a bit of calculation, we can show that the ratio is the ratio of the totals `DOLLARLP_Tot`/`BTULP_Tot`=`r num`/`r den`=`r rat`. Although the estimated ratio can be calculated manually in this manner, the standard error requires the use of the `survey_ratio()` function. The average can be interpreted as the average rate paid by a household.

#### Example 2: Ratios by subgroup {.unnumbered}

As previously done with other estimates, we can use `group_by()` to examine whether this ratio varies by region. 

```{r}
#| label: desc-ratio-2
recs_des %>%
  group_by(Region) %>%
  summarize(DOL_BTU_Rat = survey_ratio(DOLLARLP, BTULP)) %>%
  arrange(DOL_BTU_Rat)
```

Although not a formal statistical test, it appears that the cost ratios for liquid propane are the lowest in the Midwest (`r round(recs_des %>% group_by(Region) %>% summarize(DOL_BTU_Rat = survey_ratio(DOLLARLP, BTULP)) %>% filter(Region == "Midwest") %>% pull(DOL_BTU_Rat), 4)`). \index{Functions in srvyr!survey\_ratio|)}

## Correlations \index{Functions in srvyr!survey\_corr|(} \index{survey\_corr|see {Functions in srvyr}}

\index{Continuous data|(}
The correlation is a measure of the linear relationship between two continuous variables, which ranges between --1 and 1. The most commonly used method is Pearson's correlation (referred to as correlation henceforth). A sample correlation for a simple random sample is calculated as follows:

$$\frac{\sum (x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum (x_i-\bar{x})^2} \sqrt{\sum(y_i-\bar{y})^2}} $$

When using `survey_corr()` for designs other than a simple random sample, the weights are applied when estimating the correlation. 
\index{Continuous data|)}

### Syntax

The syntax for `survey_corr()` is as follows:

```r
survey_corr(
  x,
  y,
  na.rm = FALSE,
  vartype = c("se", "ci", "var", "cv"),
  level = 0.95,
  df = NULL
)
```

The arguments are:

  * `x`: A variable or expression
  * `y`: A variable or expression
  * `na.rm`: A logical value to indicate whether missing values should be dropped
  * `vartype`: Type(s) of variation estimate to calculate including any of `c("se", "ci", "var", "cv")`, defaults to `se` (standard error) (see Section \@ref(desc-count-syntax) for more information)
  * `level`: (For vartype = "ci" only) A single number or vector of numbers indicating the confidence level
  * \index{Degrees of freedom|(}`df`: (For vartype = "ci" only) A numeric value indicating the degrees of freedom for t-distribution\index{Degrees of freedom|)}

### Examples

#### Example 1: Overall correlation {.unnumbered}

We can calculate the correlation between the total square footage of homes (`TOTSQFT_EN`)^[Question text: "What is the square footage of your home?" [@recs-svy]] and electricity consumption (`BTUEL`)^[`BTUEL` is derived from the supplier side component of the survey where `BTUEL` represents the electricity consumption in British thermal units (Btus) converted from kilowatt hours (kWh) in a year [@recs-svy].].

```{r}
#| label: desc-corr-1
#| warning: FALSE
recs_des %>%
  summarize(SQFT_Elec_Corr = survey_corr(TOTSQFT_EN, BTUEL))
```

The correlation between the total square footage of homes and electricity consumption is `r recs_des %>% summarize(SQFT_Elec_Corr = survey_corr(TOTSQFT_EN, BTUEL)) %>% pull(SQFT_Elec_Corr) %>% round(3)`, indicating a moderate positive relationship.

#### Example 2: Correlations by subgroup {.unnumbered}

We can explore the correlation between total square footage and electricity consumption based on subgroups, such as whether A/C is used (`ACUsed`).

```{r}
#| label: desc-corr-2
#| warning: FALSE
recs_des %>%
  group_by(ACUsed) %>%
  summarize(SQFT_Elec_Corr = survey_corr(TOTSQFT_EN, DOLLAREL))
```

For homes without A/C, there is a small positive correlation between total square footage with electricity consumption (`r recs_des %>% group_by(ACUsed) %>% summarize(SQFT_Elec_Corr = survey_corr(TOTSQFT_EN, DOLLAREL)) %>% filter(ACUsed == FALSE) %>% pull(SQFT_Elec_Corr) %>% round(3)`). For homes with A/C, the correlation of `r recs_des %>% group_by(ACUsed) %>% summarize(SQFT_Elec_Corr = survey_corr(TOTSQFT_EN, DOLLAREL)) %>% filter(ACUsed == TRUE) %>% pull(SQFT_Elec_Corr) %>% round(3)` indicates a stronger positive correlation between total square footage and electricity consumption. \index{Functions in srvyr!survey\_corr|)} \index{Relationship|)}

## Standard deviation and variance \index{Functions in srvyr!survey\_sd|(} \index{Functions in srvyr!survey\_var|(} \index{survey\_sd|see {Functions in srvyr}} \index{survey\_var|see {Functions in srvyr}}

\index{Measures of dispersion|(}
All survey functions produce an estimate of the variability of a given estimate.  No additional function is needed when dealing with variable estimates. However, if we are specifically interested in population variance and standard deviation, we can use the `survey_var()` and `survey_sd()` functions. In our experience, it is not common practice to use these functions. They can be used when designing a future study to gauge population variability and inform sampling precision. 

### Syntax

As with non-survey data, the standard deviation estimate is the square root of the variance estimate. Therefore, the `survey_var()` and `survey_sd()` functions share the same arguments, except the standard deviation does not allow the usage of `vartype`. 

```r
survey_var(
  x,
  na.rm = FALSE,
  vartype = c("se", "ci", "var"),
  level = 0.95,
  df = NULL
)

survey_sd(
  x, 
  na.rm = FALSE
)
```

The arguments are:

  * `x`: A variable or expression, or empty
  * `na.rm`: A logical value to indicate whether missing values should be dropped
  * `vartype`: Type(s) of variation estimate to calculate including any of `c("se", "ci", "var")`, defaults to `se` (standard error) (see Section \@ref(desc-count-syntax) for more information)
  * `level`: (For vartype = "ci" only) A single number or vector of numbers indicating the confidence level
  * \index{Degrees of freedom|(}`df`: (For vartype = "ci" only) A numeric value indicating the degrees of freedom for t-distribution\index{Degrees of freedom|)}

### Examples

#### Example 1: Overall variability {.unnumbered}

Let's return to electricity bills and explore the variability in electricity expenditure. 

```{r}
#| label: desc-sdvar-ex1
#| warning: FALSE
recs_des %>%
  summarize(var_elbill = survey_var(DOLLAREL),
            sd_elbill = survey_sd(DOLLAREL))
```

We may encounter a warning related to deprecated underlying calculations performed by the `survey_var()` function. This warning is a result of changes in the way R handles recycling in vectorized operations. The results are still valid. They give an estimate of the population variance of electricity bills (`var_elbill`), the standard error of that variance (`var_elbill_se`), and the estimated population standard deviation of electricity bills (`sd_elbill`). Note that no standard error is associated with the standard deviation; this is the only estimate that does not include a standard error. 

#### Example 2: Variability by subgroup {.unnumbered}

To find out if the variability in electricity expenditure is similar across regions, we can calculate the variance by region using `group_by()`:  

```{r}
#| label: desc-sdvar-ex2
#| warning: false
recs_des %>%
  group_by(Region) %>%
  summarize(var_elbill = survey_var(DOLLAREL),
            sd_elbill = survey_sd(DOLLAREL))
```
\index{Functions in srvyr!survey\_sd|)} \index{Functions in srvyr!survey\_var|)} \index{Measures of dispersion|)}

## Additional topics

### Unweighted analysis \index{Functions in srvyr!unweighted|(} \index{unweighted|see {Functions in srvyr}}

Sometimes, it is helpful to calculate an unweighted estimate of a given variable. For this, we use the `unweighted()` function in the `summarize()` function. The `unweighted()` function calculates unweighted summaries from a `tbl_svy` object, providing the summary among the respondents without extrapolating to a population estimate. The `unweighted()` function can be used in conjunction with any {dplyr} functions. Here is an example looking at the average household electricity cost: \index{Functions in srvyr!survey\_mean|(} 

```{r}
#| label: desc-mn-unwgt
#| warning: false
recs_des %>%
  summarize(elec_bill = survey_mean(DOLLAREL),
            elec_unweight = unweighted(mean(DOLLAREL)))
```
\index{Functions in srvyr!survey\_mean|)}

```{r}
#| label: desc-mn-unwgt-save
#| echo: FALSE
.elbill_mn_unwgt <- recs_des %>%
  summarize(elec_bill = survey_mean(DOLLAREL),
            elec_unweight = unweighted(mean(DOLLAREL))) %>%
  mutate(
    across(c(elec_bill, elec_unweight), \(x) str_c(
    "$", formatC(
      x,
      big.mark = ",",
      format = "f",
      digits = 0
    ))),
    across(c(elec_bill_se), \(x) str_c(
    "$", formatC(
      x,
      big.mark = ",",
      format = "f",
      digits = 2
    )))
  )
```

It is estimated that American residential households spent an average of `r .elbill_mn_unwgt %>% pull(elec_bill)` on electricity in 2020, and the estimate has a standard error of `r .elbill_mn_unwgt %>% pull(elec_bill_se)`. The `unweighted()` function calculates the unweighted average and represents the average amount of money spent on electricity in 2020 by the respondents, which was `r .elbill_mn_unwgt %>% pull(elec_unweight)`.  \index{Functions in srvyr!unweighted|)} 

### Subpopulation analysis \index{Functions in srvyr!filter|(} \index{filter|see {Functions in srvyr}}

\index{Subpopulation|(}\index{Domain|see {Subpopulation}}
We mentioned using `filter()` to subset a survey object for analysis. This operation should be done after creating the survey design object. \index{Primary sampling unit|(}Subsetting data before creating the object can lead to incorrect variability estimates, if subsetting removes an entire Primary Sampling Unit (PSU; see Chapter \@ref(c10-sample-designs-replicate-weights) for more information on PSUs and sample designs). \index{Primary sampling unit|)}

Suppose we want estimates of the average amount spent on natural gas among housing units using natural gas (based on the variable `BTUNG`)^[`BTUNG` is derived from the supplier side component of the survey where `BTUNG` represents the natural gas consumption in British thermal units (Btus) in a year [@recs-svy].]. We first filter records to only include records where `BTUNG > 0` and then find the average amount spent.

```{r}
#| label: desc-subpop
recs_des %>%
  filter(BTUNG > 0) %>%
  summarize(NG_mean = survey_mean(DOLLARNG,
                                  vartype = c("se", "ci")))
```

The estimated average amount spent on natural gas among households that use natural gas is `r recs_des %>% filter(BTUNG > 0) %>% summarize(NG_mean = survey_mean(DOLLARNG, vartype = c("se", "ci"))) %>% mutate(NG_mean =round(NG_mean )) %>% pull(NG_mean) %>% scales::dollar()`. Let's compare this to the mean when we do not filter. 

```{r}
#| label: desc-subpop-2
recs_des %>%
  summarize(NG_mean = survey_mean(DOLLARNG,
                                  vartype = c("se", "ci")))
```

Based on this calculation, the estimated average amount spent on natural gas is `r recs_des  %>% summarize(NG_mean = survey_mean(DOLLARNG, vartype = c("se", "ci"))) %>% mutate(NG_mean =round(NG_mean )) %>% pull(NG_mean) %>% scales::dollar()`. Note that applying the filter to include only housing units that use natural gas yields a higher mean than when not applying the filter. This is because including housing units that do not use natural gas introduces many $0 amounts, impacting the mean calculation.

### Design effects {#desc-deff}

The design effect measures how the precision of an estimate is influenced by the sampling design. In other words, it measures how much more or less statistically efficient the survey design is compared to a simple random sample (SRS). It is computed by taking the ratio of the estimate's variance under the design at hand to the estimate's variance under a simple random sample without replacement. \index{Stratified sampling|(}A design effect less than 1 indicates that the design is more statistically efficient than an SRS design, which is rare but possible in a stratified sampling design where the outcome correlates with the stratification variable(s).\index{Stratified sampling|)} A design effect greater than 1 indicates that the design is less statistically efficient than an SRS design. From a design effect, we can calculate the effective sample size as follows:

$$n_{eff}=\frac{n}{D_{eff}} $$

where $n$ is the nominal sample size (the number of survey responses) and $D_{eff}$ is the estimated design effect. We can interpret the effective sample size $n_{eff}$ as the hypothetical sample size that a survey using an SRS design would need to achieve the same precision as the design at hand. Design effects specific to each outcome --- outcomes that are less clustered in the population have smaller design effects than outcomes that are clustered.

In the {srvyr} package, design effects can be calculated for totals, proportions, means, and ratio estimates by setting the `deff` argument to `TRUE` in the corresponding functions. In the example below, we calculate the design effects for the average consumption of electricity (`BTUEL`), natural gas (`BTUNG`), liquid propane (`BTULP`), fuel oil (`BTUFO`), and wood (`BTUWOOD`) by setting `deff = TRUE`: 

```{r}
#| label: desc-deff
recs_des %>%
  summarize(across(
    c(BTUEL, BTUNG, BTULP, BTUFO, BTUWOOD),
    ~ survey_mean(.x, deff = TRUE, vartype = NULL)
  )) %>%
  select(ends_with("deff"))
```

For the values less than 1 (`BTUEL_deff` and `BTUFO_deff`), the results suggest that the survey design is more efficient than a simple random sample. For the values greater than 1 (`BTUNG_deff`, `BTULP_deff`, and `BTUWOOD_deff`), the results indicate that the survey design is less efficient than a simple random sample.

\index{Design effect|)}

### Creating summary rows 

\index{Functions in srvyr!cascade|(} \index{cascade|see {Functions in srvyr}}

When using `group_by()` in analysis, the results are returned with a row for each group or combination of groups. Often, we want both breakdowns by group and a summary row for the estimate representing the entire population. For example, we may want the average electricity consumption by region and nationally. The {srvyr} package has the convenient `cascade()` function, which adds summary rows for the total of a group. It is used instead of `summarize()` and has similar functionalities along with some additional features. 

#### Syntax {.unnumbered}

The syntax is as follows:

```
cascade(
  .data, 
  ..., 
  .fill = NA, 
  .fill_level_top = FALSE, 
  .groupings = NULL
)
```

where the arguments are:

* `.data`: A `tbl_svy` object
* `...`: Name-value pairs of summary functions (same as the `summarize()` function)
* `.fill`: Value to fill in for group summaries (defaults to `NA`)
* `.fill_level_top`: When filling factor variables, whether to put the value '.fill' in the first position (defaults to FALSE, placing it in the bottom)

#### Example {.unnumbered}

First, let's look at an example where we calculate the average household electricity cost. Then, we build on it to examine the features of the `cascade()` function. In the first example below, we calculate the average household energy cost `DOLLAREL_mn` using `survey_mean()` without modifying any of the argument defaults in the function: \index{Functions in srvyr!survey\_mean|(} 

```{r}
#| label: desc-casc-ex1
recs_des %>%
  cascade(DOLLAREL_mn = survey_mean(DOLLAREL))
```

Next, let's group the results by region by adding `group_by()` before the `cascade()` function: 

```{r}
#| label: desc-casc-ex2
recs_des %>%
  group_by(Region) %>%
  cascade(DOLLAREL_mn = survey_mean(DOLLAREL))
```

```{r}
#| label: desc-casc-ex2-save
#| echo: false
.ebill_reg_cascade <- recs_des %>%
  group_by(Region) %>%
  cascade(DOLLAREL_mn = survey_mean(DOLLAREL)) %>%
  mutate(
        across(c(DOLLAREL_mn), \(x) str_c(
    "$", formatC(
      x,
      big.mark = ",",
      format = "f",
      digits = 0
    )))
  )
```

We can see the estimated average electricity bills by region: `r .ebill_reg_cascade %>% filter(Region=="Northeast") %>% pull(DOLLAREL_mn)` for the Northeast, `r .ebill_reg_cascade %>% filter(Region=="South") %>% pull(DOLLAREL_mn)` for the South, and so on. The last row, where `Region = NA`, is the national average electricity bill, `r .ebill_reg_cascade %>% filter(is.na(Region)) %>% pull(DOLLAREL_mn)`. However, naming the national "region" as `NA` is not very informative. We can give it a better name using the `.fill` argument. 

```{r}
#| label: desc-casc-ex3
recs_des %>%
  group_by(Region) %>%
  cascade(DOLLAREL_mn = survey_mean(DOLLAREL),
          .fill = "National")
```

We can move the summary row to the first row by adding `.fill_level_top = TRUE` to `cascade()`: 

```{r}
#| label: desc-casc-ex4
recs_des %>%
  group_by(Region) %>%
  cascade(
    DOLLAREL_mn = survey_mean(DOLLAREL),
    .fill = "National",
    .fill_level_top = TRUE
  )
```

While the results remain the same, the table is now easier to interpret. \index{Functions in srvyr!cascade|)} \index{Functions in srvyr!survey\_mean|)}

### Calculating estimates for many outcomes

Often, we are interested in a summary statistic across many variables. Useful tools include the `across()` function in {dplyr}, shown a few times above, and the `map()` function in {purrr}.

The `across()` function applies the same function to multiple columns within `summarize()`. This works well with all functions shown above, except for `survey_prop()`. In a later example, we tackle summarizing multiple proportions.

#### Example 1: `across()` {.unnumbered}

Suppose we want to calculate the total and average consumption, along with coefficients of variation (CV), for each fuel type. These include the reported consumption of electricity (`BTUEL`), natural gas (`BTUNG`), liquid propane (`BTULP`), fuel oil (`BTUFO`), and wood (`BTUWOOD`), as mentioned in the section on design effects. We can take advantage of the fact that these are the only variables that start with "BTU" by selecting them with `starts_with("BTU")` in the `across()` function. For each selected column (`.x`), `across()` creates a list of two functions to be applied: `survey_total()` to calculate the total and \index{Functions in srvyr!survey\_mean|(}`survey_mean()` to calculate the mean, along with their CV (`vartype = "cv"`). Finally, `.unpack = "{outer}.{inner}"` specifies that the resulting column names are a concatenation of the variable name, followed by Total or Mean, and then "coef" or "cv."  

```{r}
#| label: desc-multi-1
consumption_ests <- recs_des %>%
  summarize(across(
    starts_with("BTU"),
    list(
      Total =  ~ survey_total(.x, vartype = "cv"),
      Mean =  ~ survey_mean(.x, vartype = "cv")
    ),
    .unpack = "{outer}.{inner}"
  ))

consumption_ests 
```
\index{Functions in srvyr!survey\_mean|)}

The estimated total consumption of electricity (`BTUEL`) is `r scales::comma(consumption_ests %>% pull(BTUEL_Total.coef))` (`BTUEL_Total.coef`), the estimated average consumption is `r scales::comma(consumption_ests %>% pull(BTUEL_Mean.coef))` (`BTUEL_Mean.coef`), and the CV is `r round(consumption_ests %>% pull(BTUEL_Total._cv), 5)`.

In the example above, the table was quite wide. We may prefer a row for each fuel type. Using the `pivot_longer()` and `pivot_wider()` functions from {tidyr} can help us achieve this. First, we use `pivot_longer()` to make each variable a column, changing the data to a "long" format. We use the `names_to` argument to specify new column names: `FuelType`, `Stat`, and `Type`. Then, the `names_pattern` argument extracts the names in the original column names based on the regular expression pattern `BTU(.*)_(.*)\\.(.*)`. They are saved in the column names defined in `names_to`.

```{r}
#| label: desc-multi-2
consumption_ests_long <- consumption_ests %>%
  pivot_longer(
    cols = everything(),
    names_to = c("FuelType", "Stat", "Type"),
    names_pattern = "BTU(.*)_(.*)\\.(.*)"
  )

consumption_ests_long
```

Then, we use `pivot_wider()` to create a table that is nearly ready for publication. Within the function, we can make the names for each element more descriptive and informative by gluing the `Stat` and `Type` together with `names_glue`. Further details on creating publication-ready tables are covered in Chapter \@ref(c08-communicating-results).

```{r}
#| label: desc-multi-4
consumption_ests_long %>%
  mutate(Type = case_when(Type == "coef" ~ "",
                          Type == "_cv" ~ " (CV)")) %>%
  pivot_wider(
    id_cols = FuelType,
    names_from = c(Stat, Type),
    names_glue = "{Stat}{Type}",
    values_from = value
  )
```



#### Example 2: Proportions with `across()` {.unnumbered}

As mentioned earlier, proportions do not work as well directly with the `across()` method. If we want the proportion of houses with A/C and the proportion of houses with heating, we require two separate `group_by()` statements as shown below:

```{r}
#| label: desc-multip-1
recs_des %>%
  group_by(ACUsed) %>%
  summarize(p = survey_prop())

recs_des %>%
  group_by(SpaceHeatingUsed) %>%
  summarize(p = survey_prop())
```

We estimate `r scales::percent(recs_des %>% group_by(ACUsed) %>% summarize(p = survey_prop()) %>% filter(ACUsed == TRUE) %>% pull(p), accuracy = 0.1)` of households have A/C and `r scales::percent(recs_des %>% group_by(SpaceHeatingUsed) %>% summarize(p = survey_prop()) %>% filter(SpaceHeatingUsed == TRUE) %>% pull(p), accuracy = 0.1)` have heating.

If we are only interested in the `TRUE` outcomes, that is, the proportion of households that have A/C and the proportion that have heating, we can simplify the code. \index{Functions in srvyr!survey\_mean|(} Applying `survey_mean()` to a logical variable is the same as using `survey_prop()`, as shown below: 

```{r}
#| label: desc-multip-2
cool_heat_tab <- recs_des %>%
  summarize(across(c(ACUsed, SpaceHeatingUsed), ~ survey_mean(.x),
                   .unpack = "{outer}.{inner}"))

cool_heat_tab
```
\index{Functions in srvyr!survey\_mean|)} 

Note that the estimates are the same as those obtained using the separate `group_by()` statements. As before, we can use `pivot_longer()` to structure the table in a more suitable format for distribution.

```{r}
#| label: desc-multip-3
cool_heat_tab %>%
  pivot_longer(everything(),
               names_to = c("Comfort", ".value"),
               names_pattern = "(.*)\\.(.*)") %>%
  rename(p = coef,
         se = `_se`)
```
\index{Residential Energy Consumption Survey (RECS)|)}

#### Example 3: `purrr::map()` {.unnumbered}

Loops are a common tool when dealing with repetitive calculations. The {purrr} package provides the `map()` functions, which, like a loop, allow us to perform the same task across different elements [@R-purrr]. In our case, we may want to calculate proportions from the same design multiple times. A straightforward approach is to design the calculation for one variable, build a function based on that, and then apply it iteratively for the rest of the variables.

\index{American National Election Studies (ANES)|(}
Suppose we want to create a table that shows the proportion of people who express trust in their government (`TrustGovernment`)^[Question text: "How often can you trust the federal government in Washington to do what is right? (Always, most of the time, about half the time, some of the time, or never)" [@anes-svy]] as well as those that trust in people (`TrustPeople`)^[Question text: "Generally speaking, how often can you trust other people? (Always, most of the time, about half the time, some of the time, or never)" [@anes-svy]] using data from the 2020 ANES.

First, we create a table for a single variable. The table includes the variable name as a column, the response, and the corresponding percentage with its standard error.  \index{Functions in srvyr!drop\_na|(} \index{drop\_na|see {Functions in srvyr}}

```{r}
#| label: desc-map-1
anes_des %>%
  drop_na(TrustGovernment) %>%
  group_by(TrustGovernment) %>%
  summarize(p = survey_prop() * 100) %>%
  mutate(Variable = "TrustGovernment") %>%
  rename(Answer = TrustGovernment) %>%
  select(Variable, everything())
```

We estimate that `r scales::percent(anes_des %>% drop_na(TrustGovernment) %>%  group_by(TrustGovernment) %>% summarize(p = survey_prop()) %>% mutate(Variable = "TrustGovernment") %>% rename(Answer = TrustGovernment) %>% select(Variable, everything()) %>% filter(Answer == "Always") %>% pull(p), accuracy = 0.01)` of people always trust the government, `r scales::percent(anes_des %>% drop_na(TrustGovernment) %>%  group_by(TrustGovernment) %>% summarize(p = survey_prop()) %>% mutate(Variable = "TrustGovernment") %>% rename(Answer = TrustGovernment) %>% select(Variable, everything()) %>% filter(Answer == "Most of the time") %>% pull(p), accuracy = 0.01)` trust the government most of the time, and so on.

Now, we want to use the original series of steps as a template to create a general function `calcps()` that can apply the same steps to other variables. We replace `TrustGovernment` with an argument for a generic variable, `var`. Referring to `var` involves a bit of tidy evaluation, an advanced skill. To learn more, we recommend @wickham2019advanced. 

```{r}
#| label: desc-map-2
calcps <- function(var) {
  anes_des %>%
    drop_na(!!sym(var)) %>%
    group_by(!!sym(var)) %>%
    summarize(p = survey_prop() * 100) %>%
    mutate(Variable = var) %>%
    rename(Answer := !!sym(var)) %>%
    select(Variable, everything())
}
```
\index{Functions in srvyr!drop\_na|)} \index{Functions in srvyr!summarize|)} 

We then apply this function to the two variables of interest, `TrustGovernment` and `TrustPeople`:

```{r}
#| label: desc-map-3
calcps("TrustGovernment")
calcps("TrustPeople")
```

Finally, we use `map()` to iterate over as many variables as needed. We feed our desired variables into `map()` along with our custom function, `calcps`. The output is a tibble with the variable names in the "Variable" column, the responses in the "Answer" column, along with the percentage and standard error. The `list_rbind()` function combines the rows into a single tibble. This example extends nicely when dealing with numerous variables for which we want percentage estimates.

```{r}
#| label: desc-map-4
c("TrustGovernment", "TrustPeople") %>%
  map(calcps) %>%
  list_rbind()
```

In addition to our results above, we can also see the output for `TrustPeople`. While we estimate that `r scales::percent(anes_des %>% drop_na(TrustGovernment) %>%  group_by(TrustGovernment) %>% summarize(p = survey_prop()) %>% mutate(Variable = "TrustGovernment") %>% rename(Answer = TrustGovernment) %>% select(Variable, everything()) %>% filter(Answer == "Always") %>% pull(p), accuracy = 0.01)` of people always trust the government,  `r scales::percent(anes_des %>% drop_na(TrustPeople) %>%  group_by(TrustPeople) %>% summarize(p = survey_prop()) %>% mutate(Variable = "TrustPeople") %>% rename(Answer = TrustPeople) %>% select(Variable, everything()) %>% filter(Answer == "Always") %>% pull(p), accuracy = 0.01)` always trust people.
\index{American National Election Studies (ANES)|)}

## Exercises

The exercises use the design objects `anes_des` and `recs_des` provided in the Prerequisites box at the beginning of the chapter.

1. How many females have a graduate degree? Hint: The variables `Gender` and `Education` will be useful.

2. What percentage of people identify as "Strong Democrat"? Hint: The variable `PartyID` indicates someone's party affiliation.

3. What percentage of people who voted in the 2020 election identify as "Strong Republican"? Hint: The variable `VotedPres2020` indicates whether someone voted in 2020.

4. What percentage of people voted in both the 2016 election and the 2020 election?  Include the logit confidence interval. Hint: The variable `VotedPres2016` indicates whether someone voted in 2016.

5. What is the design effect for the proportion of people who voted early? Hint: The variable `EarlyVote2020` indicates whether someone voted early in 2020.

6. What is the median temperature people set their thermostats to at night during the winter? Hint: The variable `WinterTempNight` indicates the temperature that people set their thermostat to in the winter at night.

7. People sometimes set their temperature differently over different seasons and during the day. What median temperatures do people set their thermostats to in the summer and winter, both during the day and at night? Include confidence intervals. Hint: Use the variables `WinterTempDay`, `WinterTempNight`, `SummerTempDay`, and `SummerTempNight`.

8. What is the correlation between the temperature that people set their temperature at during the night and during the day in the summer?

9. What is the 1st, 2nd, and 3rd quartile of money spent on energy by Building America (BA) climate zone? Hint: `TOTALDOL` indicates the total amount spent on all fuel, and `ClimateRegion_BA` indicates the BA climate zones.

<!--chapter:end:05-descriptive-analysis.Rmd-->

# Statistical testing {#c06-statistical-testing}

\index{Statistical testing|(} \index{Hypothesis testing|see {Statistical testing}}

```{r}
#| label: stattest-styler
#| include: false
knitr::opts_chunk$set(tidy = 'styler')
```


::: {.prereqbox-header}
`r if (knitr:::is_html_output()) '### Prerequisites {- #prereq6}'`
:::

::: {.prereqbox data-latex="{Prerequisites}"}
For this chapter, load the following packages:
```{r}
#| label: stattest-setup
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(survey) 
library(srvyr) 
library(srvyrexploR)
library(broom)
library(gt)
library(prettyunits)
```

We are using data from ANES and RECS described in Chapter \@ref(c04-getting-started). As a reminder, here is the code to create the design objects for each to use throughout this chapter. For ANES, we need to adjust the weight so it sums to the population instead of the sample (see the ANES documentation and Chapter \@ref(c04-getting-started) for more information).

```{r}
#| label: stattest-anes-des
#| eval: FALSE
targetpop <- 231592693

anes_adjwgt <- anes_2020 %>%
  mutate(Weight = Weight / sum(Weight) * targetpop)

anes_des <- anes_adjwgt %>%
  as_survey_design(
    weights = Weight,
    strata = Stratum,
    ids = VarUnit,
    nest = TRUE
  )
```

For RECS, details are included in the RECS documentation and Chapters \@ref(c04-getting-started) and \@ref(c10-sample-designs-replicate-weights).

```{r}
#| label: stattest-recs-des
#| eval: FALSE

recs_des <- recs_2020 %>%
  as_survey_rep(
    weights = NWEIGHT,
    repweights = NWEIGHT1:NWEIGHT60,
    type = "JK1",
    scale = 59/60,
    mse = TRUE
  )
```
:::

## Introduction

When analyzing survey results, the point estimates described in Chapter \@ref(c05-descriptive-analysis) help us understand the data at a high level. Still, we often want to make comparisons between different groups. These comparisons are calculated through statistical testing. 

The general idea of statistical testing is the same for data obtained through surveys and data obtained through other methods, where we compare the point estimates and uncertainty estimates of each statistic to see if statistically significant differences exist. However, statistical testing for complex surveys involves additional considerations due to the need to account for the sampling design in order to obtain accurate uncertainty estimates.

Statistical testing, also called hypothesis testing, involves declaring a null and alternative hypothesis. A null hypothesis is denoted as $H_0$ and the alternative hypothesis is denoted as $H_A$. The null hypothesis is the default assumption in that there are no differences in the data, or that the data are operating under "standard" behaviors. On the other hand, the alternative hypothesis is the break from the "standard," and we are trying to determine if the data support this alternative hypothesis.

Let's review an example outside of survey data. If we are flipping a coin, a null hypothesis would be that the coin is fair and that each side has an equal chance of being flipped.  In other words, the probability of the coin landing on each side is 1/2, whereas an alternative hypothesis could be that the coin is unfair and that one side has a higher probability of being flipped (e.g., a probability of 1/4 to get heads but a probability of 3/4 to get tails). We write this set of hypotheses as:

- $H_0: \rho_{heads} = \rho_{tails}$, where $\rho_{x}$ is the probability of flipping the coin and having it land on heads ($\rho_{heads}$) or tails ($\rho_{tails}$)
- $H_A: \rho_{heads} \neq \rho_{tails}$

\index{p-value|(} 
When we conduct hypothesis testing, the statistical models calculate a p-value, which shows how likely we are to observe the data if the null hypothesis is true. If the p-value (a probability between 0 and 1) is small, we have strong evidence to reject the null hypothesis, as it is unlikely to see the data we observe if the null hypothesis is true. However, if the p-value is large, we say we do not have evidence to reject the null hypothesis. The size of the p-value for this cut-off is determined by Type 1 error known as $\alpha$.  A common Type 1 error value for statistical testing is to use $\alpha = 0.05$^[For more information on statistical testing, we recommend reviewing introduction to statistics textbooks.]. Explanations of statistical testing often refer to confidence level.  The confidence level is the inverse of the Type 1 error.  Thus, if $\alpha = 0.05$, the confidence level would be 95%.
\index{p-value|)} 

The functions in the {survey} package allow for the correct estimation of the uncertainty estimates (e.g., standard deviations and confidence intervals). This chapter covers the following statistical tests with survey data and the following functions from the {survey} package [@lumley2010complex]:

* Comparison of proportions (`svyttest()`)
* Comparison of means (`svyttest()`)
* Goodness-of-fit tests (`svygofchisq()`)
* Tests of independence (`svychisq()`)
* Tests of homogeneity (`svychisq()`)

## Dot notation {#dot-notation}

\index{Dot notation|(}
Up to this point, we have shown functions that use wrappers from the {srvyr} package. This means that the functions work with tidyverse syntax. However, the functions in this chapter do not have wrappers in the {srvyr} package and are instead used directly from the {survey} package. Therefore, the design object is not the first argument, and to use these functions with the magrittr pipe (`%>%`) and tidyverse syntax, we need to use dot (`.`) notation^[This could change in the future if another package is built or {srvyr} is expanded to work with {tidymodels} packages, but no such plans are known at this time.].

Functions that work with the magrittr pipe (`%>%`) have the dataset as the first argument. When we run a function with the pipe, it automatically places anything to the left of the pipe into the first argument of the function to the right of the pipe. For example, if we wanted to take the `towny` data from the {gt} package and filter to municipalities with the Census Subdivision Type of "city," we can write the code in at least four different ways:

1. `filter(towny, csd_type == "city")`
2. `towny %>% filter(csd_type == "city")`
3. `towny %>% filter(., csd_type == "city")`
4. `towny %>% filter(.data = ., csd_type == "city")`

Each of these lines of code produces the same output since the argument that takes the dataset is in the first spot in `filter()`. The first two are probably familiar to those who have worked with the tidyverse. The third option functions the same way as the second one but is explicit that `towny` goes into the first argument, and the fourth option indicates that `towny` is going into the named argument of `.data`. Here, we are telling R to take what is on the left side of the pipe (`towny`) and pipe it into the spot with the dot (`.`) --- the first argument.

In functions that are not part of the tidyverse, the data argument may not be in the first spot. For example, in \index{Functions in survey!svyttest|(}`svyttest()`, the data argument is in the second spot, which means we need to place the dot (`.`) in the second spot and not the first. For example: \index{svyttest|see {Functions in survey}}

```r
svydata_des %>%
 svyttest(x ~ y, .)
```

By default, the pipe places the left-hand object in the first argument spot. Placing the dot (`.`) in the second argument spot indicates that the survey design object `svydata_des` should be used in the second argument and not the first.

Alternatively, named arguments could be used to place the dot first, as named arguments can appear at any location as in the following: 

```r
svydata_des %>%
 svyttest(design = ., x ~ y)
```

However, the following code does not work as the `svyttest()` function expects the formula as the first argument when arguments are not named:

```r
svydata_des %>%
 svyttest(., x ~ y)
```
\index{Functions in survey!svyttest|)} \index{Dot notation|)}

## Comparison of proportions and means {#stattest-ttest}

\index{t-test|(}
We use t-tests to compare two proportions or means. T-tests allow us to determine if one proportion or mean is statistically different from another. \index{t-test!one-sample t-test|(}They are commonly used to determine if a single estimate differs from a known value (e.g., 0 or 50%) or to compare two group means (e.g., North versus South). Comparing a single estimate to a known value is called a one-sample t-test, and we can set up the hypothesis test as follows:  

  - $H_0: \mu = 0$ where $\mu$ is the mean outcome and $0$ is the value we are comparing it to
  - $H_A: \mu \neq 0$

\index{t-test!one-sample t-test|)}

\index{t-test!two-sample t-test|(}
For comparing two estimates, this is called a two-sample t-test. We can set up the hypothesis test as follows:

  - $H_0: \mu_1 = \mu_2$ where $\mu_i$ is the mean outcome for group $i$
  - $H_A: \mu_1 \neq \mu_2$

\index{t-test!unpaired two-sample t-test|(} \index{t-test!paired two-sample t-test|(}
Two-sample t-tests can also be paired or unpaired. If the data come from two different populations (e.g., North versus South), the t-test run is an unpaired or independent samples t-test. Paired t-tests occur when the data come from the same population. This is commonly seen with data from the same population in two different time periods (e.g., before and after an intervention).
\index{t-test!two-sample t-test|)} \index{t-test!unpaired two-sample t-test|)} \index{t-test!paired two-sample t-test|)}

The difference between t-tests with non-survey data and survey data is based on the underlying variance estimation difference.  Chapter \@ref(c10-sample-designs-replicate-weights) provides a detailed overview of the math behind the mean and sampling error calculations for various sample designs. The functions in the {survey} package account for these nuances, provided the design object is correctly defined.

### Syntax {#stattest-ttest-syntax}

\index{Functions in survey!svyttest|(}
When we do not have survey data, we can use the `t.test()` function from the {stats} package to run t-tests. This function does not allow for weights or the variance structure that need to be accounted for with survey data. Therefore, we need to use the `svyttest()` function from {survey} when using survey data. Many of the arguments are the same between the two functions, but there are a few key differences:

  - We need to use the survey design object instead of the original data frame
  - We can only use a formula and not separate x and y data
  - The confidence level cannot be specified and is always set to 95%. However, we show examples of how the confidence level can be changed after running the `svyttest()` function by using the `confint()` function.

Here is the syntax for the `svyttest()` function: 

```r
svyttest(formula,
         design,
         ...)
```

The arguments are:

* `formula`: Formula, `outcome~group` for two-sample, `outcome~0` or `outcome~1` for one-sample. The group variable must be a factor or character with two levels, or be coded 0/1 or 1/2. We give more details on formula set-up below for different types of tests.
* `design`: survey design object
* `...`: This passes options on for one-sided tests only, and thus, we can specify `na.rm=TRUE`

\index{Dot notation|(}Notice that the first argument here is the `formula` and not the `design`. This means we must use the dot `(.)` if we pipe in the survey design object (as described in Section \@ref(dot-notation)).\index{Dot notation|)}

The `formula` argument can take several different forms depending on what we are measuring. Here are a few common scenarios: \index{Formula notation|(}

1. \index{t-test!one-sample t-test|(}One-sample t-test:
    a. Comparison to 0: `var ~ 0`, where `var` is the measure of interest, and we compare it to the value `0`. For example, we could test if the population mean of household debt is different from `0` given the sample data collected.
    b. Comparison to a different value: `var - value ~ 0`, where `var` is the measure of interest and `value` is what we are comparing to. For example, we could test if the proportion of the population that has blue eyes is different from `25%` by using `var - 0.25 ~ 0`. Note that specifying the formula as `var ~ 0.25` is not equivalent and results in a syntax error.\index{t-test!one-sample t-test|)}
2. \index{t-test!two-sample t-test|(}Two-sample t-test:
    a. \index{t-test!unpaired two-sample t-test|(}Unpaired:
        - 2 level grouping variable: `var ~ groupVar`, where `var` is the measure of interest and `groupVar` is a variable with two categories. For example, we could test if the average age of the population who voted for president in 2020 differed from the age of people who did not vote. In this case, age would be used for `var`, and a binary variable indicating voting activity would be the `groupVar`.
        - 3+ level grouping variable: `var ~ groupVar == level`, where `var` is the measure of interest, `groupVar` is the categorical variable, and `level` is the category level to isolate. For example, we could test if the test scores in one classroom differed from all other classrooms where `groupVar` would be the variable holding the values for classroom IDs and `level` is the classroom ID we want to compare to the others.\index{t-test!unpaired two-sample t-test|)}
    b. \index{t-test!paired two-sample t-test|(}Paired: `var_1 - var_2 ~ 0`, where `var_1` is the first variable of interest and `var_2` is the second variable of interest. For example, we could test if test scores on a subject differed between the start and the end of a course, so `var_1` would be the test score at the beginning of the course, and `var_2` would be the score at the end of the course.\index{t-test!two-sample t-test|)}\index{t-test!paired two-sample t-test|)}
    
\index{Formula notation|)}
The `na.rm` argument defaults to `FALSE`, which means if any data values are missing, the t-test does not compute. Throughout this chapter, we always set `na.rm = TRUE`, but before analyzing the survey data, review the notes provided in Chapter \@ref(c11-missing-data) to better understand how to handle missing data.

Let's walk through a few examples using the RECS data.

### Examples {#stattest-ttest-examples}

#### Example 1: One-sample t-test for mean {.unnumbered #stattest-ttest-ex1}  

\index{Residential Energy Consumption Survey (RECS)|(} \index{t-test!one-sample t-test|(}
RECS asks respondents to indicate what temperature they set their house to during the summer at night^[Question text: "During the summer, what is your home’s typical indoor temperature inside your home at night?" [@recs-svy]]. In our data, we have called this variable `SummerTempNight`. If we want to see if the average U.S. household sets its temperature at a value different from 68$^\circ$F^[This is the temperature that Stephanie prefers at night during the summer, and she wanted to see if she was different from the population.], we could set up the hypothesis as follows: 

- $H_0: \mu = 68$ where $\mu$ is the average temperature U.S. households set their thermostat to in the summer at night
- $H_A: \mu \neq 68$

\index{Formula notation|(}
To conduct this in R, we use `svyttest()` and subtract the temperature on the left-hand side of the formula: 

```{r}
#| label: stattest-ttest-ex1
ttest_ex1 <- recs_des %>%
  svyttest(
    formula = SummerTempNight - 68 ~ 0,
    design = .,
    na.rm = TRUE
  )

ttest_ex1
```

\index{Formula notation|)}
To pull out specific output, we can use R's built-in `$` operator. For instance, to obtain the estimate $\mu - 68$, we run `ttest_ex1$estimate`.

If we want the average, we take our t-test estimate and add it to 68: 

```{r}
#| label: stattest-ttest-ex1-add
ttest_ex1$estimate + 68
```

Or, we can use the \index{Functions in srvyr!survey\_mean|(} `survey_mean()` function described in Chapter \@ref(c05-descriptive-analysis):  \index{Functions in srvyr!summarize|(}

```{r}
#| label: stattest-ttest-ex1-svymean
recs_des %>%
  summarize(mu = survey_mean(SummerTempNight, na.rm = TRUE))
```
\index{Functions in srvyr!summarize|)} \index{Functions in srvyr!survey\_mean|)} 

The result is the same in both methods, so we see that the average temperature U.S. households set their thermostat to in the summer at night is `r signif(ttest_ex1$estimate + 68,3)`$^\circ$F. Looking at the output from `svyttest()`, the t-statistic is `r signif(ttest_ex1$statistic, 3)`, and \index{p-value|(} the p-value is `r pretty_p_value(ttest_ex1[["p.value"]])`, indicating that the average is statistically different from 68$^\circ$F at an $\alpha$ level of $0.05$. \index{p-value|)} 

If we want an 80% confidence interval for the test statistic, we can use the function `confint()` to change the confidence level. Below, we print the default confidence interval (95%), the confidence interval explicitly specifying the level as 95%, and the 80% confidence interval. When the confidence level is 95% either by default or explicitly, R returns a vector with both row and column names. However, when we specify any other confidence level, an unnamed vector is returned, with the first element being the lower bound and the second element being the upper bound of the confidence interval.

```{r}
#| label: stattest-ttest-ex1-ci80
confint(ttest_ex1)
confint(ttest_ex1, level = 0.95) 
confint(ttest_ex1, level = 0.8) 
```

In this case, neither confidence interval contains 0, and we draw the same conclusion from either that the average temperature households set their thermostat in the summer at night is significantly higher than 68$^\circ$F.

#### Example 2: One-sample t-test for proportion {.unnumbered #stattest-ttest-ex2}

\index{Categorical data|(}
RECS asked respondents if they use air conditioning (A/C) in their home^[Question text: "Is any air conditioning equipment used in your home?" [@recs-svy]]. In our data, we call this variable `ACUsed`. Let's look at the proportion of U.S. households that use A/C in their homes using the `survey_prop()` function we learned in Chapter \@ref(c05-descriptive-analysis). \index{Functions in srvyr!survey\_prop|(} \index{Functions in srvyr!summarize|(}

```{r}
#| label: stattest-ttest-acused
acprop <- recs_des %>%
  group_by(ACUsed) %>%
  summarize(p = survey_prop())

acprop
```
\index{Functions in srvyr!summarize|)} \index{Functions in srvyr!survey\_prop|)}

Based on this, `r signif((acprop %>% filter(ACUsed==TRUE) %>% pull(p))*100,3)`% of U.S. households use A/C in their homes. If we wanted to know if this differs from 90%, we could set up our hypothesis as follows:

- $H_0: p = 0.90$ where $p$ is the proportion of U.S. households that use A/C in their homes
- $H_A: p \neq 0.90$

To conduct this in R, we use the `svyttest()` function as follows: 

```{r}
#| label: stattest-ttest-ex2
ttest_ex2 <- recs_des %>%
  svyttest(
    formula = (ACUsed == TRUE) - 0.90 ~ 0,
    design = .,
    na.rm = TRUE
  )

ttest_ex2
```

The output from the `svyttest()` function can be a bit hard to read. Using the `tidy()` function from the {broom} package, we can clean up the output into a tibble to more easily understand what the test tells us [@R-broom].

```{r}
#| label: stattest-ttest-ex2-broom
tidy(ttest_ex2)
```


\index{gt package|(} \index{p-value|(} 
The 'tidied' output can also be piped into the {gt} package to create a table ready for publication (see Table \@ref(tab:stattest-ttest-ex2-gt-tab)). We go over the {gt} package in Chapter \@ref(c08-communicating-results). The function `pretty_p_value()` comes from the {prettyunits} package and converts numeric p-values to characters and, by default, prints four decimal places and displays any p-value less than 0.0001 as `"<0.0001"`, though another minimum display p-value can be specified [@R-prettyunits].

```{r}
#| label: stattest-ttest-ex2-gt
#| eval: FALSE
tidy(ttest_ex2) %>%
  mutate(p.value = pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number()
```

(ref:stattest-ttest-ex2-gt-tab)  One-sample t-test output for estimates of U.S. households use A/C in their homes differing from 90%, RECS 2020

```{r}
#| label: stattest-ttest-ex2-gt-tab
#| echo: FALSE
#| warning: FALSE

tidy(ttest_ex2) %>%
  mutate(p.value = pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number() %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

\index{gt package|)} \index{p-value|)} 

The estimate differs from Example 1 in that it does not display \(p - 0.90\) but rather \(p\), or the difference between the U.S. households that use A/C and our comparison proportion. We can see that there is a difference of --`r signif(ttest_ex2$estimate*100,3)` percentage points. Additionally, the t-statistic value in the `statistic` column is --`r signif(ttest_ex2$statistic,3)`, and the p-value is `r pretty_p_value(ttest_ex2$p.value)`. These results indicate that fewer than 90% of U.S. households use A/C in their homes.
\index{Categorical data|)} \index{t-test!one-sample t-test|)}

#### Example 3: Unpaired two-sample t-test {.unnumbered #stattest-ttest-ex3}

\index{t-test!two-sample t-test|(} \index{t-test!unpaired two-sample t-test|(}
In addition to `ACUsed`, another variable in the RECS data is a household's total electric cost in dollars (`DOLLAREL`).To see if U.S. households with A/C had higher electrical bills than those without, we can set up the hypothesis as follows:

- $H_0: \mu_{AC} = \mu_{noAC}$ where $\mu_{AC}$ is the electrical bill cost for U.S. households that used A/C, and $\mu_{noAC}$ is the electrical bill cost for U.S. households that did not use A/C
- $H_A: \mu_{AC} \neq \mu_{noAC}$
 
Let's take a quick look at the data to see how they are formatted: \index{Functions in srvyr!survey\_mean|(} \index{Functions in srvyr!summarize|(}

```{r}
#| label: stattest-ttest-ex3-desc
recs_des %>%
  group_by(ACUsed) %>%
  summarize(mean = survey_mean(DOLLAREL, na.rm = TRUE))
```
\index{Functions in srvyr!summarize|)} \index{Functions in srvyr!survey\_mean|)} 

To conduct this in R, we use `svyttest()`:

```{r stattest-ttest-ex3}
#| label: stattest-ttest-ex3
ttest_ex3 <- recs_des %>%
  svyttest(formula = DOLLAREL ~ ACUsed,
           design = .,
           na.rm = TRUE)
```

```{r}
#| label: stattest-ttest-ex3-gt
#| eval: FALSE
tidy(ttest_ex3) %>%
  mutate(p.value = pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number()
```

(ref:stattest-ttest-ex3-gt-tab)  Unpaired two-sample t-test output for estimates of U.S. households electrical bills by A/C use, RECS 2020

```{r}
#| label: stattest-ttest-ex3-gt-tab
#| echo: FALSE
#| warning: FALSE

tidy(ttest_ex3) %>%
  mutate(p.value = pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number() %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

The results in Table \@ref(tab:stattest-ttest-ex3-gt-tab) indicate that the difference in electrical bills for those who used A/C and those who did not is, on average, \$`r round(ttest_ex3$estimate,2)`. The difference appears to be statistically significant as the t-statistic is `r signif(ttest_ex3$statistic, 3)` and the p-value is `r pretty_p_value(ttest_ex3[["p.value"]])`. Households that used A/C spent, on average, $`r round(ttest_ex3[["estimate"]], 2) %>% unname()` more in 2020 on electricity than households without A/C.
\index{t-test!unpaired two-sample t-test|)}

#### Example 4: Paired two-sample t-test {.unnumbered #stattest-ttest-ex4}

\index{t-test!paired two-sample t-test|(}
Let's say we want to test whether the temperature at which U.S. households set their thermostat at night differs depending on the season (comparing summer and winter^[Question text: "During the winter, what is your home’s typical indoor temperature inside your home at night?" [@recs-svy]] temperatures). We could set up the hypothesis as follows:

- $H_0: \mu_{summer} = \mu_{winter}$ where $\mu_{summer}$ is the temperature that U.S. households set their thermostat to during summer nights, and $\mu_{winter}$ is the temperature that U.S. households set their thermostat to during winter nights
- $H_A: \mu_{summer} \neq \mu_{winter}$

To conduct this in R, we use `svyttest()` by calculating the temperature difference on the left-hand side as follows:

```{r}
#| label: stattest-ttest-ex4
ttest_ex4 <- recs_des %>%
  svyttest(
    design = .,
    formula = SummerTempNight - WinterTempNight ~ 0,
    na.rm = TRUE
  )
```

```{r}
#| label: stattest-ttest-ex4-gt
#| eval: FALSE
tidy(ttest_ex4) %>%
  mutate(p.value = pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number()
```

(ref:stattest-ttest-ex4-gt-tab)  Paired two-sample t-test output for estimates of U.S. households thermostat temperature by season, RECS 2020

```{r}
#| label: stattest-ttest-ex4-gt-tab
#| echo: FALSE
#| warning: FALSE

tidy(ttest_ex4) %>%
  mutate(p.value = pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number() %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

\index{p-value|(} 
The results displayed in Table \@ref(tab:stattest-ttest-ex4-gt-tab) indicate that U.S. households set their thermostat on average `r signif(ttest_ex4$estimate,2)`$^\circ$F warmer in summer nights than winter nights, which is statistically significant (t = `r signif(ttest_ex4$statistic, 3)`, p-value is `r pretty_p_value(ttest_ex4[["p.value"]])`).  \index{Functions in survey!svyttest|)} \index{Residential Energy Consumption Survey (RECS)|(} \index{p-value|)} \index{t-test|)} \index{t-test!two-sample t-test|(} \index{t-test!paired two-sample t-test|(}

## Chi-squared tests {#stattest-chi}

\index{Chi-squared test|(} \index{Categorical data|(}
Chi-squared tests ($\chi^2$) allow us to examine multiple proportions using a goodness-of-fit test, a test of independence, or a test of homogeneity. These three tests have the same $\chi^2$ distributions but with slightly different underlying assumptions.
\index{Categorical data|)}

\index{Chi-squared test!Goodness-of-fit test|(}
First, goodness-of-fit tests are used when comparing observed data to expected data. For example, this could be used to determine if respondent demographics (the observed data in the sample) match known population information (the expected data). In this case, we can set up the hypothesis test as follows:

  - $H_0: p_1 = \pi_1, ~ p_2 = \pi_2, ~ ..., ~ p_k = \pi_k$ where $p_i$ is the observed proportion for category $i$, $\pi_i$ is the expected proportion for category $i$, and $k$ is the number of categories
  - $H_A:$ at least one level of $p_i$ does not match $\pi_i$

\index{Chi-squared test!Goodness-of-fit test|)}

\index{Chi-squared test!Test of independence|(}
Second, tests of independence are used when comparing two types of observed data to see if there is a relationship. For example, this could be used to determine if the proportion of respondents who voted for each political party in the presidential election matches the proportion of respondents who voted for each political party in a local election. In this case, we can set up the hypothesis test as follows:

  - $H_0:$ The two variables/factors are independent
  - $H_A:$ The two variables/factors are not independent 

\index{Chi-squared test!Test of independence|)}

\index{Chi-squared test!Test of homogeneity|(}
Third, tests of homogeneity are used to compare two distributions to see if they match. For example, this could be used to determine if the highest education achieved is the same for both men and women. In this case, we can set up the hypothesis test as follows:

  - $H_0: p_{1a} = p_{1b}, ~ p_{2a} = p_{2b}, ~ ..., ~ p_{ka} = p_{kb}$ where $p_{ia}$ is the observed proportion of category $i$ for subgroup $a$, $p_{ib}$ is the observed proportion of category $i$ for subgroup $a$, and $k$ is the number of categories
  - $H_A:$ at least one category of $p_{ia}$ does not match $p_{ib}$

\index{Chi-squared test!Test of homogeneity|)}

As with t-tests, the difference between using $\chi^2$ tests with non-survey data and survey data is based on the underlying variance estimation. The functions in the {survey} package account for these nuances, provided the design object is correctly defined. For basic variance estimation formulas for different survey design types, refer to Chapter \@ref(c10-sample-designs-replicate-weights).

### Syntax {#stattest-chi-syntax}

When we do not have survey data, we may be able to use the `chisq.test()` function from the {stats} package in base R to run chi-squared tests [@R-base]. However, this function does not allow for weights or the variance structure to be accounted for with survey data. Therefore, when using survey data, we need to use one of two functions: \index{Functions in survey!svygofchisq|(} \index{Functions in survey!svychisq|(} \index{svygofchisq|see {Functions in survey}} \index{svychisq|see {Functions in survey}}

- \index{Chi-squared test!Goodness-of-fit test|(}`svygofchisq()`: For goodness-of-fit tests\index{Chi-squared test!Goodness-of-fit test|)}
- \index{Chi-squared test!Test of homogeneity|(}\index{Chi-squared test!Test of independence|(}`svychisq()`: For tests of independence and homogeneity\index{Chi-squared test!Test of homogeneity|)}\index{Chi-squared test!Test of independence|)}

The non-survey data function of `chisq.test()` requires either a single set of counts and given proportions (for goodness-of-fit tests) or two sets of counts for tests of independence and homogeneity. \index{Formula notation|(}The functions we use with survey data require respondent-level data and formulas instead of counts. This ensures that the variances are correctly calculated.\index{Formula notation|)}

\index{Chi-squared test!Goodness-of-fit test|(}
First, the function for the goodness-of-fit tests is `svygofchisq()`: 

```r
svygofchisq(formula,
            p,
            design,
            na.rm = TRUE,
            ...)
```

The arguments are:

* `formula`: Formula specifying a single factor variable
* `p`: Vector of probabilities for the categories of the factor in the correct order. If the probabilities do not sum to 1, they are rescaled to sum to 1.
* `design`: Survey design object
* ...: Other arguments to pass on, such as `na.rm`

\index{Dot notation|(} \index{Formula notation|(}
Based on the order of the arguments, we again must use the dot `(.)` notation if we pipe in the survey design object or explicitly name the arguments as described in Section \@ref(dot-notation).\index{Dot notation|)} For the goodness-of-fit tests, the formula is a single variable `formula = ~var` as we compare the observed data from this variable to the expected data. The expected probabilities are then entered in the `p` argument and need to be a vector of the same length as the number of categories in the variable. For example, if we want to know if the proportion of males and females matches a distribution of 30/70, then the sex variable (with two categories) would be used `formula = ~SEX`, and the proportions would be included as `p = c(.3, .7)`. \index{Factor|(}It is important to note that the variable entered into the formula should be formatted as either a factor or a character. The examples below provide more detail and tips on how to make sure the levels match up correctly.  \index{Functions in srvyr!drop\_na|)} \index{Factor|)} \index{Chi-squared test!Goodness-of-fit test|)}\index{Formula notation|)}

\index{Chi-squared test!Test of homogeneity|(} \index{Chi-squared test!Test of independence|(}
For tests of homogeneity and independence, the `svychisq()` function should be used. The syntax is as follows: 

```r
svychisq(
  formula,
  design,
  statistic = c("F", "Chisq", "Wald", "adjWald",
                "lincom", "saddlepoint"),
  na.rm = TRUE
)
```

The arguments are:

* `formula`: Model formula specifying the table (shown in examples)
* `design`: Survey design object
* `statistic`: Type of test statistic to use in test (details below)
* `na.rm`: Remove missing values

\index{Cross-tabulation|(} 
There are six statistics that are accepted in this formula. For tests of homogeneity (when comparing cross-tabulations), the `F` or `Chisq` statistics should be used^[These two statistics can also be used for goodness-of-fit tests if the `svygofchisq()` function is not used.]. The `F` statistic is the default and uses the Rao-Scott second-order correction. This correction is designed to assist with complicated sampling designs (i.e., those other than a simple random sample) [@Scott2007]. The `Chisq` statistic is an adjusted version of the Pearson $\chi^2$ statistic. The version of this statistic in the `svychisq()` function compares the design effect \index{Design effect} estimate from the provided survey data to what the $\chi^2$ distribution would have been if the data came from a simple random sampling. 
\index{Cross-tabulation|)} \index{Chi-squared test!Test of homogeneity|)} 

\index{Primary sampling unit|(}
For tests of independence, the `Wald` and `adjWald` are recommended as they provide a better adjustment for variable comparisons [@lumley2010complex]. \index{Degrees of freedom|(}If the data have a small number of primary sampling units (PSUs) compared to the degrees of freedom, then the `adjWald` statistic should be used to account for this.\index{Degrees of freedom|)} The `lincom` and `saddlepoint` statistics are available for more complicated data structures.
\index{Primary sampling unit|)} \index{Chi-squared test!Test of independence|)}

\index{Formula notation|(}
The formula argument is always one-sided, unlike the `svyttest()` function. The two variables of interest should be included with a plus sign: `formula = ~ var_1 + var_2`. As with the `svygofchisq()` function, the variables entered into the formula should be formatted as either a factor or a character. 
\index{Formula notation|)}

Additionally, as with the t-test function, both `svygofchisq()` and `svychisq()` have the `na.rm` argument. If any data values are missing, the $\chi^2$ tests assume that `NA` is a category and include it in the calculation. Throughout this chapter, we always set `na.rm = TRUE`, but before analyzing the survey data, review the notes provided in Chapter \@ref(c11-missing-data) to better understand how to handle missing data. \index{Functions in survey!svychisq|)}

### Examples {#stattest-chi-examples}

\index{American National Election Studies (ANES)|(}
Let's walk through a few examples using the ANES data.

#### Example 1: Goodness-of-fit test {.unnumbered #stattest-chi-ex1}

\index{American Community Survey (ACS)|(} \index{Chi-squared test!Goodness-of-fit test|(}
ANES asked respondents about their highest education level^[Question text: "What is the highest level of school you have completed or the highest degree you have received?" [@anes-svy]]. Based on the data from the 2020 American Community Survey (ACS) 5-year estimates^[Data was pulled from data.census.gov using the S1501 Education Attainment 2020: ACS 5-Year Estimates Subject Tables.], the education distribution of those aged 18+ in the United States (among the 50 states and the District of Columbia) is as follows:

  - 11% had less than a high school degree
  - 27% had a high school degree
  - 29% had some college or an associate's degree
  - 33% had a bachelor's degree or higher

If we want to see if the weighted distribution from the ANES 2020 data matches this distribution, we could set up the hypothesis as follows:

  - $H_0: p_1 = 0.11, ~ p_2 = 0.27, ~ p_3 = 0.29, ~ p_4 = 0.33$
  - $H_A:$ at least one of the education levels does not match between the ANES and the ACS

To conduct this in R, let's first look at the education variable (`Education`) we have on the ANES data. Using the \index{Functions in srvyr!survey\_mean|(} `survey_mean()` function discussed in Chapter \@ref(c05-descriptive-analysis), we can see the education levels and estimated proportions.  \index{Functions in srvyr!summarize|(} \index{Functions in srvyr!drop\_na|(}

```{r}
#| label: stattest-chi-ex1-educmean
anes_des %>%
  drop_na(Education) %>%
  group_by(Education) %>%
  summarize(p = survey_mean())
```
\index{Functions in srvyr!summarize|)} \index{Functions in srvyr!survey\_mean|)}

\index{Formula notation|(}
Based on this output, we can see that we have different levels from the ACS data. Specifically, the education data from ANES include two levels for bachelor's degree or higher (bachelor's and graduate), so these two categories need to be collapsed into a single category to match the ACS data. For this, among other methods, we can use the {forcats} package from the tidyverse [@R-forcats]. The package's `fct_collapse()` function helps us create a new variable by collapsing categories into a single one. Then, we use the `svygofchisq()` function to compare the ANES data to the ACS data, where we specify the updated design object, the formula using the collapsed education variable, the ACS estimates for education levels as p, and removing `NA` values.

```{r}
#| label: stattest-chi-ex1
anes_des_educ <- anes_des %>%
  mutate(Education2 =
           fct_collapse(Education,
                        "Bachelor or Higher" = c("Bachelor's",
                                                 "Graduate")))

anes_des_educ %>%
  drop_na(Education2) %>%
  group_by(Education2) %>%
  summarize(p = survey_mean())

chi_ex1 <- anes_des_educ %>%
  svygofchisq(
    formula =  ~ Education2,
    p = c(0.11, 0.27, 0.29, 0.33),
    design = .,
    na.rm = TRUE
  )

chi_ex1
```

\index{Formula notation|)}

The output from the `svygofchisq()` indicates that at least one proportion from ANES does not match the ACS data ($\chi^2 =$ `r prettyNum(chi_ex1$statistic, big.mark=",")`; p-value is `r pretty_p_value(chi_ex1[["p.value"]])`). To get a better idea of the differences, we can use the `expected` output along with `survey_mean()` to create a comparison table:  \index{Functions in srvyr!survey\_mean|(} \index{Functions in srvyr!summarize|(} \index{Functions in srvyr!drop\_na|(}

```{r}
#| label: stattest-chi-ex1-table
ex1_table <- anes_des_educ %>%
  drop_na(Education2) %>%
  group_by(Education2) %>%
  summarize(Observed = survey_mean(vartype = "ci")) %>%
  rename(Education = Education2) %>%
  mutate(Expected=c(0.11, 0.27, 0.29, 0.33)) %>%
  select(Education, Expected, everything())

ex1_table
```
\index{Functions in srvyr!drop\_na|)} \index{Functions in srvyr!summarize|)} \index{Functions in srvyr!survey\_mean|)}

This output includes our expected proportions from the ACS that we provided the `svygofchisq()` function along with the output of the observed proportions and their confidence intervals. This table shows that the "high school" and "post HS" categories have nearly identical proportions, but that the other two categories are slightly different. Looking at the confidence intervals, we can see that the ANES data skew to include fewer people in the "less than HS" category and more people in the "bachelor or higher" category. This may be easier to see if we plot this. The code below uses the tabular output to create Figure \@ref(fig:stattest-chi-ex1-graph).

```{r}
#| label: stattest-chi-ex1-graph
#| fig.cap: Expected and observed proportions of education with confidence intervals 
#| fig.alt: Expected and observed proportions of education, showing the confidence intervals for the expected proportions and whether the observed proportions lie within them. The x-axis has labels 'Less than HS', 'High school', 'Post HS', and 'Bachelor or Higher'. The only ones where expected proportion is outside of the intervals is 'Less than HS' and 'Bachelor or Higher'.

ex1_table %>%
  pivot_longer(
    cols = c("Expected", "Observed"),
    names_to = "Names",
    values_to = "Proportion"
  ) %>%
  mutate(
    Observed_low = if_else(Names == "Observed", Observed_low, NA_real_),
    Observed_upp = if_else(Names == "Observed", Observed_upp, NA_real_),
    Names = if_else(Names == "Observed",
                    "ANES (observed)", "ACS (expected)")
  ) %>%
  ggplot(aes(x = Education, y = Proportion, color = Names)) +
  geom_point(alpha = 0.75, size = 2) +
  geom_errorbar(aes(ymin = Observed_low, ymax = Observed_upp), 
                width = 0.25) +
  theme_bw() +
  scale_color_manual(name = "Type", values = book_colors[c(4, 1)]) +
  theme(legend.position = "bottom", legend.title = element_blank())
```
\index{Functions in survey!svygofchisq|)} \index{American Community Survey (ACS)|)} \index{Chi-squared test!Goodness-of-fit test|)}

#### Example 2: Test of independence {.unnumbered #stattest-chi-ex2}

\index{Functions in survey!svychisq|(} \index{Chi-squared test!Test of independence|(}
ANES asked respondents two questions about trust:

  - Question text: "How often can you trust the federal government to do what is right?" [@anes-svy]
  - Question text: "How often can you trust other people?" [@anes-svy]

If we want to see if the distributions of these two questions are similar or not, we can conduct a test of independence. Here is how the hypothesis could be set up:

  - $H_0:$ People's trust in the federal government and their trust in other people are independent (i.e., not related)
  - $H_A:$ People's trust in the federal government and their trust in other people are not independent (i.e., they are related)

To conduct this in R, we use the `svychisq()` function to compare the two variables: 

```{r}
#| label: stattest-chi-ex2
chi_ex2 <- anes_des %>%
  svychisq(
    formula =  ~ TrustGovernment + TrustPeople,
    design = .,
    statistic = "Wald",
    na.rm = TRUE
  )

chi_ex2
```

\index{Cross-tabulation|(}
The output from `svychisq()` indicates that the distribution of people's trust in the federal government and their trust in other people are not independent, meaning that they are related. Let's output the distributions in a table to see the relationship. The `observed` output from the test provides a cross-tabulation of the counts for each category: 

```{r}
#| label: stattest-chi-ex2-counts
chi_ex2$observed
```

\index{gt package|(} 
However, we often want to know about the proportions, not just the respondent counts from the survey. There are a couple of different ways that we can do this. The first is using the counts from `chi_ex2$observed` to calculate the proportion. We can then pivot the table to create a cross-tabulation similar to the counts table above. Adding `group_by()` to the code means that we obtain the proportions within each variable level. In this case, we are looking at the distribution of `TrustGovernment` for each level of `TrustPeople`. The resulting table is shown in Table \@ref(tab:stattest-chi-ex2-prop1-tab).

```{r}
#| label: stattest-chi-ex2-prop1
#| warning: false
chi_ex2_table<-chi_ex2$observed %>% 
  as_tibble() %>%
  group_by(TrustPeople) %>%
  mutate(prop = round(n / sum(n), 3)) %>%
  select(-n) %>%
  pivot_wider(names_from = TrustPeople, values_from = prop) %>% 
  gt(rowname_col = "TrustGovernment") %>%
  tab_stubhead(label = "Trust in Government") %>%
  tab_spanner(label = "Trust in People",
              columns = everything()) %>%
  cols_label(`Most of the time` = md("Most of<br />the time"),
             `About half the time` = md("About half<br />the time"),
             `Some of the time` = md("Some of<br />the time"))
```

```{r}
#| label: stattest-chi-ex2-prop1-noeval
#| eval: false
chi_ex2_table
```

(ref:stattest-chi-ex2-prop1-tab) Proportion of adults in the U.S. by levels of trust in people and government, ANES 2020

```{r}
#| label: stattest-chi-ex2-prop1-tab
#| echo: FALSE
#| warning: FALSE

chi_ex2_table %>%
    print_gt_book(knitr::opts_current$get()[["label"]])
```

In Table \@ref(tab:stattest-chi-ex2-prop1-tab), each column sums to 1. For example, we can say that it is estimated that of people who always trust in people, `r round(chi_ex2$observed[1,1]/sum(chi_ex2$observed[,1])*100, 1)`% also always trust in the government based on the top-left cell, but `r round(chi_ex2$observed[5,1]/sum(chi_ex2$observed[,1])*100, 1)`% never trust in the government. \index{Cross-tabulation|)} 

The second option is to use the `group_by()` and `survey_mean()` functions to calculate the proportions from the ANES design object. Remember that with more than one variable listed in the `group_by()` statement, the proportions are within the first variable listed. As mentioned above, we are looking at the distribution of `TrustGovernment` for each level of `TrustPeople`. \index{Functions in srvyr!survey\_mean|(} \index{Functions in srvyr!summarize|(} \index{Functions in srvyr!drop\_na|(}

```{r}
#| label: stattest-chi-ex2-prop2
chi_ex2_obs <- anes_des %>%
  drop_na(TrustPeople, TrustGovernment) %>%
  group_by(TrustPeople, TrustGovernment) %>%
  summarize(Observed = round(survey_mean(vartype = "ci"), 3),
            .groups="drop") 

chi_ex2_obs_table<-chi_ex2_obs %>%
  mutate(prop = paste0(Observed, " (", Observed_low, ", ",
                       Observed_upp, ")")) %>%
  select(TrustGovernment, TrustPeople, prop) %>%
  pivot_wider(names_from = TrustPeople, values_from = prop) %>%
  gt(rowname_col = "TrustGovernment") %>%
  tab_stubhead(label = "Trust in Government") %>%
  tab_spanner(label = "Trust in People",
              columns = everything()) %>% 
  tab_options(page.orientation = "landscape")
```
\index{Functions in srvyr!summarize|)} \index{Functions in srvyr!survey\_mean|)}

```{r}
#| label: stattest-chi-ex2-prop2-noeval
#| eval: false
chi_ex2_obs_table
```
\index{Functions in srvyr!drop\_na|)}

(ref:stattest-chi-ex2-prop2-tab) Proportion of adults in the U.S. by levels of trust in people and government with confidence intervals, ANES 2020

```{r}
#| label: stattest-chi-ex2-prop2-tab
#| echo: FALSE
#| warning: FALSE

chi_ex2_obs_table %>%
    print_gt_book(knitr::opts_current$get()[["label"]])
```

Both methods produce the same output as the `svychisq()` function. However, calculating the proportions directly from the design object allows us to obtain the variance information. In this case, the output in Table \@ref(tab:stattest-chi-ex2-prop2-tab) displays the survey estimate followed by the confidence intervals. Based on the output, we can see that of those who never trust people, `r round(chi_ex2$observed[5,5]/sum(chi_ex2$observed[,5])*100, 1)`% also never trust the government, while the proportions of never trusting the government are much lower for each of the other levels of trusting people. \index{gt package|)} 

We may find it easier to look at these proportions graphically. We can use `ggplot()` and facets to provide an overview to create Figure \@ref(fig:stattest-chi-ex2-graph) below:

```{r}
#| label: stattest-chi-ex2-graph
#| fig.cap: Proportion of adults in the U.S. by levels of trust in people and government with confidence intervals, ANES 2020
#| fig.alt: Proportion of adults in the U.S. by levels of trust in people and government with confidence intervals, ANES 2020. This presents the same information as the previous table in graphical form.

chi_ex2_obs %>%
  mutate(TrustPeople=
           fct_reorder(str_c("Trust in People:\n", TrustPeople), 
                       order(TrustPeople))) %>%
  ggplot(
    aes(x = TrustGovernment, y = Observed, color = TrustGovernment)) +
  facet_wrap( ~ TrustPeople, ncol = 5) +
  geom_point() +
  geom_errorbar(aes(ymin = Observed_low, ymax = Observed_upp)) +
  ylab("Proportion") +
  xlab("") +
  theme_bw() +
  scale_color_manual(name="Trust in Government", 
                     values=book_colors) +
  theme(axis.text.x = element_blank(), 
        axis.ticks.x = element_blank(),
        legend.position = "bottom") +
  guides(col = guide_legend(nrow=2))
```

\index{Chi-squared test!Test of independence|)}

#### Example 3: Test of homogeneity {.unnumbered #stattest-chi-ex3}

\index{Chi-squared test!Test of homogeneity|(}
Researchers and politicians often look at specific demographics each election cycle to understand how each group is leaning or voting toward candidates. The ANES data are collected post-election, but we can still see if there are differences in how specific demographic groups voted.

If we want to see if there is a difference in how each age group voted for the 2020 candidates, this would be a test of homogeneity, and we can set up the hypothesis as follows:

\begin{align*}
H_0: p_{1_{Biden}} &= p_{1_{Trump}} = p_{1_{Other}},\\
    p_{2_{Biden}} &= p_{2_{Trump}} = p_{2_{Other}},\\
    p_{3_{Biden}} &= p_{3_{Trump}} = p_{3_{Other}},\\
    p_{4_{Biden}} &= p_{4_{Trump}} = p_{4_{Other}},\\
    p_{5_{Biden}} &= p_{5_{Trump}} = p_{5_{Other}},\\
    p_{6_{Biden}} &= p_{6_{Trump}} = p_{6_{Other}}
  \end{align*}
    where $p_{i_{Biden}}$ is the observed proportion of each age group ($i$) that voted for Joseph Biden, $p_{i_{Trump}}$ is the observed proportion of each age group ($i$) that voted for Donald Trump, and $p_{i_{Other}}$ is the observed proportion of each age group ($i$) that voted for another candidate.
    
- $H_A:$ at least one category of $p_{i_{Biden}}$ does not match $p_{i_{Trump}}$ or $p_{i_{Other}}$

To conduct this in R, we use the `svychisq()` function to compare the two variables: \index{Functions in srvyr!drop\_na|(} 

```{r}
#| label: stattest-chi-ex3
chi_ex3 <- anes_des %>%
  drop_na(VotedPres2020_selection, AgeGroup) %>%
  svychisq(
    formula =  ~ AgeGroup + VotedPres2020_selection,
    design = .,
    statistic = "Chisq",
    na.rm = TRUE
  )

chi_ex3
```
\index{Functions in srvyr!drop\_na|)}

The output from `svychisq()` indicates a difference in how each age group voted in the 2020 election. To get a better idea of the different distributions, let's output proportions to see the relationship. As we learned in Example 2 above, we can use `chi_ex3$observed`, or if we want to get the variance information (which is crucial with survey data), we can use `survey_mean()`. Remember, when we have two variables in `group_by()`, we obtain the proportions within each level of the variable listed. In this case, we are looking at the distribution of `AgeGroup` for each level of `VotedPres2020_selection`. \index{Functions in srvyr!survey\_mean|(} \index{Functions in srvyr!filter|(} \index{Functions in srvyr!summarize|(} \index{Functions in srvyr!drop\_na|(}

```{r}
#| label: stattest-chi-ex3-table
chi_ex3_obs <- anes_des %>%
  filter(VotedPres2020 == "Yes") %>%
  drop_na(VotedPres2020_selection, AgeGroup) %>%
  group_by(VotedPres2020_selection, AgeGroup) %>%
  summarize(Observed = round(survey_mean(vartype = "ci"), 3)) 

chi_ex3_obs_table<-chi_ex3_obs %>%
  mutate(prop = paste0(Observed, " (", Observed_low, ", ",
                       Observed_upp, ")")) %>%
  select(AgeGroup, VotedPres2020_selection, prop) %>%
  pivot_wider(names_from = VotedPres2020_selection, 
              values_from = prop) %>%
  gt(rowname_col = "AgeGroup") %>%
  tab_stubhead(label = "Age Group")
```
\index{Functions in srvyr!drop\_na|)} \index{Functions in srvyr!filter|)} \index{Functions in srvyr!summarize|)} \index{Functions in srvyr!survey\_mean|)}

```{r}
#| label: stattest-chi-ex3-table-noeval
#| eval: false
chi_ex3_obs_table
```

(ref:stattest-chi-ex3-tab) Distribution of age group by presidential candidate selection with confidence intervals

```{r}
#| label: stattest-chi-ex3-tab
#| echo: FALSE
#| warning: FALSE

chi_ex3_obs_table %>%
    print_gt_book(knitr::opts_current$get()[["label"]])
```

In Table \@ref(tab:stattest-chi-ex3-tab) we can see that the age group distribution that voted for Biden and other candidates was younger than those that voted for Trump. For example, of those who voted for Biden, 20.4% were in the 18--29 age group, compared to only 11.4% of those who voted for Trump were in that age group. Conversely, 23.4% of those who voted for Trump were in the 50--59 age group compared to only 15.4% of those who voted for Biden. \index{Functions in survey!svychisq|)} \index{Chi-squared test|)} \index{American National Election Studies (ANES)|)} \index{Statistical testing|)}

\index{Chi-squared test!Test of homogeneity|)}

## Exercises {#stattest-exercises}

The exercises use the design objects `anes_des` and `recs_des` as provided in the Prerequisites box at the [beginning of the chapter](#c06-statistical-testing). Here are some exercises for practicing conducting t-tests using `svyttest()`:

1. Using the RECS data, do more than 50% of U.S. households use A/C (`ACUsed`)?

2. Using the RECS data, does the average temperature at which U.S. households set their thermostats differ between the day and night in the winter (`WinterTempDay` and `WinterTempNight`)?

3. Using the ANES data, does the average age (`Age`) of those who voted for Joseph Biden in 2020 (`VotedPres2020_selection`) differ from those who voted for another candidate?

4. If we wanted to determine if the political party affiliation differed for males and females, what test would we use?

  a. Goodness-of-fit test (`svygofchisq()`)
  b. Test of independence (`svychisq()`)
  c. Test of homogeneity (`svychisq()`)

5. In the RECS data, is there a relationship between the type of housing unit (`HousingUnitType`) and the year the house was built (`YearMade`)?

6. In the ANES data, is there a difference in the distribution of gender (`Gender`) across early voting status in 2020 (`EarlyVote2020`)?

<!--chapter:end:06-statistical-testing.Rmd-->

# Modeling {#c07-modeling}

```{r}
#| label: model-styler
#| include: false
knitr::opts_chunk$set(tidy = 'styler')
```

::: {.prereqbox-header}
`r if (knitr:::is_html_output()) '### Prerequisites {- #prereq7}'`
:::

::: {.prereqbox data-latex="{Prerequisites}"}
For this chapter, load the following packages:
```{r}
#| label: model-setup
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(survey) 
library(srvyr) 
library(srvyrexploR)
library(broom)
library(gt)
library(prettyunits)
```

We are using data from ANES and RECS described in Chapter \@ref(c04-getting-started). As a reminder, here is the code to create the design objects for each to use throughout this chapter. For ANES, we need to adjust the weight so it sums to the population instead of the sample (see the ANES documentation and Chapter \@ref(c04-getting-started) for more information).

```{r}
#| label: model-anes-des
#| eval: FALSE
targetpop <- 231592693

anes_adjwgt <- anes_2020 %>%
  mutate(Weight = Weight / sum(Weight) * targetpop)

anes_des <- anes_adjwgt %>%
  as_survey_design(
    weights = Weight,
    strata = Stratum,
    ids = VarUnit,
    nest = TRUE
  )
```

For RECS, details are included in the RECS documentation and Chapters \@ref(c04-getting-started) and \@ref(c10-sample-designs-replicate-weights).

```{r}
#| label: model-recs-des
#| eval: FALSE
recs_des <- recs_2020 %>%
  as_survey_rep(
    weights = NWEIGHT,
    repweights = NWEIGHT1:NWEIGHT60,
    type = "JK1",
    scale = 59 / 60,
    mse = TRUE
  )
```
:::

## Introduction {#model-intro}

Modeling data is a way for researchers to investigate the relationship between a single dependent variable and one or more independent variables. This builds upon the analyses conducted in Chapter \@ref(c06-statistical-testing), which looked at the relationships between just two variables.  For example, in Example 3 in Section \@ref(stattest-ttest-examples), we investigated if there is a relationship between the electrical bill cost and whether or not the household used air conditioning (A/C).  However, there are potentially other elements that could go into what the cost of electrical bills are in a household (e.g., outside temperature, desired internal temperature, types and number of appliances, etc.).

\index{Analysis of variance (ANOVA)|(}
T-tests only allow us to investigate the relationship of one independent variable at a time, but using models, we can look into multiple variables and even explore interactions between these variables. There are several types of models, but in this chapter, we cover Analysis of Variance (ANOVA) and linear regression models following common normal (Gaussian) and logit models. Jonas Kristoffer Lindeløv has an interesting [discussion](https://lindeloev.github.io/tests-as-linear/) of many statistical tests and models being equivalent to a linear model. For example, a one-way ANOVA is a linear model with one categorical independent variable, and a two-sample t-test is an ANOVA where the independent variable has exactly two levels.
\index{Analysis of variance (ANOVA)|)}

When modeling data, it is helpful to first create an equation that provides an overview of what we are modeling. The main structure of these models is as follows:

$$y_i=\beta_0 +\sum_{i=1}^p \beta_i x_i + \epsilon_i$$

where $y_i$ is the outcome, $\beta_0$ is an intercept, $x_1, \cdots, x_p$ are the predictors with $\beta_1, \cdots, \beta_p$ as the associated coefficients, and $\epsilon_i$ is the error. Not all models have all components. For example, some models may not include an intercept ($\beta_0$), may have interactions between different independent variables ($x_i$), or may have different underlying structures for the dependent variable ($y_i$).  However, all linear models have the independent variables related to the dependent variable in a linear form.

\index{Formula notation|(}
To specify these models in R, the formulas are the same with both survey data and other data. The left side of the formula is the response/dependent variable, and the right side has the predictor/independent variable(s). There are many symbols used in R to specify the formula.

For example, a linear formula mathematically notated as

$$y_i=\beta_0+\beta_1 x_i+\epsilon_i$$ would be specified in R as `y~x` where the intercept is not explicitly included. To fit a model with no intercept, that is,

$$y_i=\beta_1 x_i+\epsilon_i$$
it can be specified in R as `y~x-1`. Formula notation details in R can be found in the help file for formula^[Use `help(formula)` or `?formula` in R]. A quick overview of the common formula notation is in Table \@ref(tab:notation-common):

Table: (\#tab:notation-common) Common symbols in formula notation

| Symbol |   Example   | Meaning                                                                  |
|:-----------:|:---------------:|---------------------------------------------------------------------------|
|   \+   |    `+x`     | include this variable                                                    |
|   \-   |    `-x`     | delete this variable                                                     |
|   :    |    `x:z`    | include the interaction between these variables                          |
|   \*   |    `x*z`    | include these variables and the interactions between them                |
|  `^n`  | `(x+y+z)^3` | include these variables and all interactions up to n-way                 |
|   I    |  `I(x-z)`   | as-is: include a new variable that is calculated inside the parentheses (e.g., x-z, x*z, x/z are possible calculations that could be done) |

There are often multiple ways to specify the same formula. For example, consider the following equation using the `mtcars` dataset that is built into R:

$$mpg_i=\beta_0+\beta_1cyl_{i}+\beta_2disp_{i}+\beta_3hp_{i}+\beta_4cyl_{i}disp_{i}+\beta_5cyl_{i}hp_{i}+\beta_6disp_{i}hp_{i}+\epsilon_i$$

This could be specified in R code as any of the following:

  - `mpg ~ (cyl + disp + hp)^2`
  - `mpg ~ cyl + disp + hp + cyl:disp + cyl:hp + disp:hp`
  - `mpg ~ cyl*disp + cyl*hp + disp*hp`

In the above options, the ways the `:` and `*` notations are implemented are different. Using `:` only includes the interactions and not the main effects, while using `*` includes the main effects and all possible interactions. Table \@ref(tab:notation-diffs) provides an overview of the syntax and differences between the two notations.
  
Table: (\#tab:notation-diffs) Differences in formulas for `:` and `*` code syntax

| Symbol | Syntax | Formula |
|:-----------:|:---------------:|---------------------------------------------------------------------------|
| : | `mpg ~ cyl:disp:hp` | $$ \begin{aligned} mpg_i = &\beta_0+\beta_4cyl_{i}disp_{i}+\beta_5cyl_{i}hp_{i}+ \\& \beta_6disp_{i}hp_{i}+\epsilon_i\end{aligned}$$ |
| \* | `mpg ~ cyl*disp*hp` |$$ \begin{aligned} mpg_i= &\beta_0+\beta_1cyl_{i}+\beta_2disp_{i}+\beta_3hp_{i}+\\&        \beta_4cyl_{i}disp_{i}+\beta_5cyl_{i}hp_{i}+\beta_6disp_{i}hp_{i}+\\&\beta_7cyl_{i}disp_{i}hp_{i}+\epsilon_i\end{aligned}$$ |

\index{Formula notation|)}

When using non-survey data, such as experimental or observational data, researchers use the `glm()` function for linear models.  With survey data, however, \index{Functions in survey!svyglm|(}we use `svyglm()` from the {survey} package to ensure that we account for the survey design and weights in modeling^[There is some debate about whether weights should be used in regression [@bollen2016weightsreg; @gelman2007weights]. However, for the purposes of providing complete information on how to analyze complex survey data, this chapter includes weights.].  This allows us to generalize a model to the population of interest and accounts for the fact that the observations in the survey data may not be independent.  As discussed in Chapter \@ref(c06-statistical-testing), modeling survey data cannot be directly done in {srvyr}, but can be done in the {survey} package [@lumley2010complex]. In this chapter, we provide syntax and examples for linear models, including ANOVA, normal linear regression, and logistic regression. For details on other types of regression, including ordinal regression, log-linear models, and survival analysis, refer to @lumley2010complex. @lumley2010complex also discusses custom models such as a negative binomial or Poisson model in appendix E of his book.\index{svyglm|see {Functions in survey}}

## Analysis of variance

\index{Analysis of variance (ANOVA)|(}
In ANOVA, we are testing whether the mean of an outcome is the same across two or more groups. Statistically, we set up this as follows:

  - $H_0: \mu_1 = \mu_2= \dots = \mu_k$ where $\mu_i$ is the mean outcome for group $i$
  - $H_A: \text{At least one mean is different}$
  
An ANOVA test is also a linear model, we can re-frame the problem using the framework as:

$$ y_i=\sum_{i=1}^k \mu_i x_i + \epsilon_i$$

where $x_i$ is a group indicator for groups $1, \cdots, k$. 

Some assumptions when using ANOVA on survey data include:

  - The outcome variable is normally distributed within each group.
  - The variances of the outcome variable between each group are approximately equal.
  - We do NOT assume independence between the groups as with ANOVA on non-survey data. The covariance is accounted for in the survey design.

### Syntax

To perform this type of analysis in R, the general syntax is as follows: 

``` r
des_obj %>%
  svyglm(
    formula = outcome ~ group,
    design = .,
    na.action = na.omit,
    df.resid = NULL
  )
```

The arguments are:

* `formula`: formula in the form of `outcome~group`. The group variable must be a factor or character.
* `design`: a `tbl_svy` object created by `as_survey`
* `na.action`: handling of missing data
* \index{Degrees of freedom|(}`df.resid`: degrees of freedom for Wald tests (optional); defaults to using `degf(design)-(g-1)` where $g$ is the number of groups\index{Degrees of freedom|)}

\index{Dot notation|(}The function `svyglm()` does not have the design as the first argument so the dot (`.`) notation is used to pass it with a pipe (see Chapter \@ref(c06-statistical-testing) for more details).\index{Dot notation|)} The default for missing data is `na.omit`. This means that we are removing all records with any missing data in either predictors or outcomes from analyses.  There are other options for handling missing data, and we recommend looking at the help documentation for `na.omit` (run `help(na.omit)` or `?na.omit`) for more information on options to use for `na.action`.  For a discussion on how to handle missing data, see Chapter \@ref(c11-missing-data). 

### Example

\index{Residential Energy Consumption Survey (RECS)|(}
Looking at an example helps us discuss the output and how to interpret the results. In RECS, respondents are asked what temperature they set their thermostat to during the evening when using A/C during the summer^[Question text: "During the summer, what is your home’s typical indoor temperature inside your home at night?" [@recs-svy]]. To analyze these data, we filter the respondents to only those using A/C (`ACUsed`)^[Question text: "Is any air conditioning equipment used in your home?" [@recs-svy]]. Then, if we want to see if there are regional differences, we can use `group_by()`. A descriptive analysis of the temperature at night (`SummerTempNight`) set by region and the sample sizes is displayed below. \index{Functions in srvyr!survey\_mean|(} \index{Functions in srvyr!unweighted|(} \index{Functions in srvyr!filter|(} \index{Functions in srvyr!summarize|(}

```{r}
#| label: model-anova-prep
recs_des %>%
  filter(ACUsed) %>%
  group_by(Region) %>%
  summarize(
    SMN = survey_mean(SummerTempNight, na.rm = TRUE),
    n = unweighted(n()),
    n_na = unweighted(sum(is.na(SummerTempNight)))
  )
```
\index{Functions in srvyr!filter|)} \index{Functions in srvyr!summarize|)} \index{Functions in srvyr!survey\_mean|)} \index{Functions in srvyr!unweighted|)} 

In the following code, we test whether this temperature varies by region by first using `svyglm()` to run the test and then using `broom::tidy()` to display the output. Note that the temperature setting is set to NA when the household does not use A/C, and since the default handling of NAs is `na.action=na.omit`, records that do not use A/C are not included in this regression.

```{r}
#| label: model-anova-ex
anova_out <- recs_des %>%
  svyglm(design = .,
         formula = SummerTempNight ~ Region)

tidy(anova_out)
```

In the output above, we can see the estimated coefficients (`estimate`), estimated standard errors of the coefficients (`std.error`), the t-statistic (`statistic`), and the p-value for each coefficient. In this output, the intercept represents the reference value of the Northeast region. The other coefficients indicate the difference in temperature relative to the Northeast region. For example, in the Midwest, temperatures are set, on average, `r tidy(anova_out) %>% filter(term=="RegionMidwest") %>% pull(estimate) %>% signif(3)` (p-value is `r tidy(anova_out) %>% filter(term=="RegionMidwest") %>% pull(p.value) %>% pretty_p_value()`) degrees higher than in the Northeast during summer nights, and each region sets its thermostats at significantly higher temperatures than the Northeast.

\index{Factor|(}
If we wanted to change the reference value, we would reorder the factor before modeling using the `relevel()` function from {stats} or using one of many factor ordering functions in {forcats} such as `fct_relevel()` or `fct_infreq()`.  For example, if we wanted the reference level to be the Midwest region, we could use the following code with the results in Table \@ref(tab:model-anova-ex-tab). \index{gt package|(}Note the usage of the `gt()` function on top of `tidy()` to print a nice-looking output table [@R-gt; @R-broom] (see Chapter \@ref(c08-communicating-results) for more information on the {gt} package).

```{r}
#| label: model-anova-ex-relevel
anova_out_relevel <- recs_des %>%
  mutate(Region=fct_relevel(Region, "Midwest", after = 0)) %>% 
  svyglm(design = .,
         formula = SummerTempNight ~ Region)
```

```{r}
#| label: model-anova-ex-noeval
#| eval: FALSE
tidy(anova_out_relevel) %>%
  mutate(p.value=pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number()
```

(ref:model-anova-ex-tab) ANOVA output for estimates of thermostat temperature setting at night by region with Midwest as the reference region, RECS 2020

```{r}
#| label: model-anova-ex-tab
#| echo: FALSE
#| warning: FALSE

tidy(anova_out_relevel) %>%
  mutate(p.value=pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number() %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

\index{p-value|(} 
This output now has the coefficients indicating the difference in temperature relative to the Midwest region. For example, in the Northeast, temperatures are set, on average, `r tidy(anova_out_relevel) %>% filter(term=="RegionNortheast") %>% pull(estimate) %>% abs() %>% signif(3)` (p-value is `r tidy(anova_out_relevel) %>% filter(term=="RegionNortheast") %>% pull(p.value) %>% pretty_p_value()`) degrees lower than in the Midwest during summer nights, and each region sets its thermostats at significantly lower temperatures than the Midwest.  This is the reverse of what we saw in the prior model, as we are still comparing the same two regions, just from different reference points. \index{Analysis of variance (ANOVA)|)} \index{Factor|)} \index{gt package|)} 
\index{p-value|)} 

## Normal linear regression

\index{Continuous data|(}\index{Linear regression|(} \index{Normal linear regression|see {Linear regression}} \index{Gaussian models|see {Linear regression}}
Normal linear regression is a more generalized method than ANOVA, where we fit a model of a continuous outcome with any number of categorical or continuous predictors (whereas ANOVA only has categorical predictors) and is similarly specified as:

```{=tex}
\begin{equation}
y_i=\beta_0 +\sum_{i=1}^p \beta_i x_i + \epsilon_i
\end{equation}
```

where $y_i$ is the outcome, $\beta_0$ is an intercept, $x_1, \cdots, x_p$ are the predictors with $\beta_1, \cdots, \beta_p$ as the associated coefficients, and $\epsilon_i$ is the error.
\index{Continuous data|)}

Assumptions in normal linear regression using survey data include:

  - The residuals ($\epsilon_i$) are normally distributed, but there is not an assumption of independence, and the correlation structure is captured in the survey design object
  - There is a linear relationship between the outcome variable and the independent variables
  - The residuals are homoscedastic; that is, the error term is the same across all values of independent variables

### Syntax

\index{Formula notation|(}
The syntax for this regression uses the same function as ANOVA but can have more than one variable listed on the right-hand side of the formula:

``` r
des_obj %>%
  svyglm(
    formula = outcomevar ~ x1 + x2 + x3,
    design = .,
    na.action = na.omit,
    df.resid = NULL
  )
```

\index{Formula notation|)}

The arguments are:

* `formula`: formula in the form of `y~x`
* `design`: a `tbl_svy` object created by `as_survey`
* `na.action`: handling of missing data
* \index{Degrees of freedom|(}`df.resid`: degrees of freedom for Wald tests (optional); defaults to using `degf(design)-p` where $p$ is the rank of the design matrix\index{Degrees of freedom|)}

\index{Formula notation|(}
As discussed in Section \@ref(model-intro), the formula on the right-hand side can be specified in many ways, for example, denoting whether or not interactions are desired.
\index{Formula notation|)}

### Examples

#### Example 1: Linear regression with a single variable {.unnumbered}

On RECS, we can obtain information on the square footage of homes^[Question text: "What is the square footage of your home?" [@recs-svy]] and the electric bills. We assume that square footage is related to the amount of money spent on electricity and examine a model for this. Before any modeling, we first plot the data to determine whether it is reasonable to assume a linear relationship. In Figure \@ref(fig:model-plot-sf-elbill), each hexagon represents the weighted count of households in the bin, and we can see a general positive linear trend (as the square footage increases, so does the amount of money spent on electricity).

```{r}
#| label: model-plot-sf-elbill
#| fig.cap: Relationship between square footage and dollars spent on electricity, RECS 2020
#| fig.alt: Hex chart where each hexagon represents a number of housing units at a point. x-axis is 'Total square footage' ranging from 0 to 7,500 and y-axis is 'Amount spent on electricity' ranging from $0 to 8,000. The trend is relatively linear and positive. A high concentration of points have square footage between 0 and 2,500 square feet as well as between electricity expenditure between $0 and 2,000
#| echo: TRUE
#| warning: FALSE
recs_2020 %>%
  ggplot(aes(
    x = TOTSQFT_EN,
    y = DOLLAREL,
    weight = NWEIGHT / 1000000
  )) +
  geom_hex() +
  scale_fill_gradientn(
    guide = "colorbar",
    name = "Housing Units\n(Millions)",
    labels = scales::comma,
    colors = book_colors[c(3, 2, 1)]
  ) +
  xlab("Total square footage") + ylab("Amount spent on electricity") +
  scale_y_continuous(labels = scales::dollar_format()) +
  scale_x_continuous(labels = scales::comma_format()) +
  theme_minimal() 
```

Given that the plot shows a potentially increasing relationship between square footage and electricity expenditure, fitting a model allows us to determine if the relationship is statistically significant.  The model is fit below with electricity expenditure as the outcome. 

```{r}
#| label: model-slr-examp
m_electric_sqft <- recs_des %>%
  svyglm(design = .,
         formula = DOLLAREL ~ TOTSQFT_EN,
         na.action = na.omit)
```

```{r}
#| label: model-slr-examp-noeval
#| eval: FALSE
tidy(m_electric_sqft) %>%
  mutate(p.value=pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number()
```

(ref:model-slr-examp-tab) Linear regression output predicting electricity expenditure given square footage, RECS 2020

```{r}
#| label: model-slr-examp-tab
#| echo: FALSE
#| warning: FALSE

tidy(m_electric_sqft) %>%
  mutate(p.value=pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number() %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```



In Table \@ref(tab:model-slr-examp-tab), we can see the estimated coefficients (`estimate`), estimated standard errors of the coefficients (`std.error`), the t-statistic (`statistic`), and the p-value for each coefficient. In these results, we can say that, on average, for every additional square foot of house size, the electricity bill increases by `r (tidy(m_electric_sqft) %>% filter(term=="TOTSQFT_EN") %>% pull(estimate) %>% signif(2))*100` cents, and that square footage is significantly associated with electricity expenditure (p-value is `r tidy(m_electric_sqft) %>% filter(term=="TOTSQFT_EN") %>% pull(p.value) %>% pretty_p_value()`). 

This is a straightforward model, and there are likely many more factors related to electricity expenditure, including the type of cooling, number of appliances, location, and more. However, starting with one-variable models can help analysts understand what potential relationships there are between variables before fitting more complex models. Often, we start with known relationships before building models to determine what impact additional variables have on the model.

#### Example 2: Linear regression with multiple variables and interactions {.unnumbered}

\index{Formula notation|(}
In the following example, a model is fit to predict electricity expenditure, including census region (factor/categorical), urbanicity (factor/categorical), square footage (double/numeric), and whether A/C is used (logical/categorical) with all two-way interactions also included.  In this example, we are choosing to fit this model without an intercept (using `-1` in the formula).  This results in an intercept estimate for each region instead of a single intercept for all data.

```{r}
#| label: model-lmr-examp
m_electric_multi <- recs_des %>%
  svyglm(
    design = .,
    formula = 
      DOLLAREL ~ (Region + Urbanicity + TOTSQFT_EN + ACUsed)^2 - 1, 
    na.action = na.omit
  )
```

\index{Formula notation|)}

```{r}
#| label: model-lmr-examp-noeval
#| eval: FALSE
tidy(m_electric_multi) %>%
  mutate(p.value=pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number()
```

(ref:model-lmr-examp-tab) Linear regression output predicting electricity expenditure given region, urbanicity, square footage, A/C usage, and one-way interactions, RECS 2020

```{r}
#| label: model-lmr-examp-tab
#| echo: FALSE
#| warning: FALSE

tidy(m_electric_multi) %>%
  mutate(p.value=pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number() %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```


As shown in Table \@ref(tab:model-lmr-examp-tab), there are many terms in this model. To test whether coefficients for a term are different from zero, the `regTermTest()` function can be used. For example, in the above regression, we can test whether the interaction of region and urbanicity is significant as follows:

```{r}
#| label: model-lmr-test-term

urb_reg_test <- regTermTest(m_electric_multi, ~Urbanicity:Region)
urb_reg_test
```

\index{p-value|(} 
This output indicates there is a significant interaction between urbanicity and region (p-value is `r pretty_p_value(urb_reg_test[["p"]])`).
\index{p-value|)} 

To examine the predictions, residuals, and more from the model, the `augment()` function from {broom} can be used. The `augment()` function returns a tibble with the independent and dependent variables and other fit statistics. The `augment()` function has not been specifically written for objects of class `svyglm`, and as such, a warning is displayed indicating this at this time. As it was not written exactly for this class of objects, a little tweaking needs to be done after using `augment()`. To obtain the standard error of the predicted values (`.se.fit`), we need to use the `attr()` function on the predicted values (`.fitted`) created by `augment()`. Additionally, the predicted values created are outputted with a type of `svrep`. If we want to plot the predicted values, we need to use `as.numeric()` to get the predicted values into a numeric format to work with.  However, it is important to note that this adjustment must be completed after the standard error adjustment. 

```{r}
#| label: model-aug-examp-se
#| warning: false
fitstats <-
  augment(m_electric_multi) %>%
  mutate(.se.fit = sqrt(attr(.fitted, "var")), 
         .fitted = as.numeric(.fitted)) 

fitstats
```

These results can then be used in a variety of ways, including examining residual plots as illustrated in the code below and Figure \@ref(fig:model-aug-examp-plot). In the residual plot, we look for any patterns in the data. If we do see patterns, this may indicate a violation of the heteroscedasticity assumption and the standard errors of the coefficients may be incorrect.  In Figure \@ref(fig:model-aug-examp-plot), we do not see a strong pattern indicating that our assumption of heteroscedasticity may hold.

```{r}
#| label: model-aug-examp-plot
#| fig.cap: 'Residual plot of electric cost model with the following covariates: Region, Urbanicity, TOTSQFT\_EN, and ACUsed'
#| fig.alt: Residual scatter plot with a x-axis of 'Fitted value of electricity cost' ranging between approximately $0 and $4,000 and a y-axis with the 'Residual of model' ranging from approximately -$3,000 to $5,000. The points create a slight megaphone shape with largest residuals in the middle of the x-range. A red line is drawn horizontally at y=0.
fitstats %>%
  ggplot(aes(x = .fitted, .resid)) +
  geom_point(alpha=.1) + 
  geom_hline(yintercept = 0, color = "red") +
  theme_minimal() +
  xlab("Fitted value of electricity cost") +
  ylab("Residual of model") +
  scale_y_continuous(labels = scales::dollar_format()) +
  scale_x_continuous(labels = scales::dollar_format()) 

```

Additionally, `augment()` can be used to predict outcomes for data not used in modeling. Perhaps we would like to predict the energy expenditure for a home in an urban area in the south that uses A/C and is 2,500 square feet. To do this, we first make a tibble including that additional data and then use the `newdata` argument in the `augment()` function.  As before, to obtain the standard error of the predicted values, we need to use the `attr()` function.

```{r}
#| label: model-predict-new-dat
add_data <- recs_2020 %>% 
  select(DOEID, Region, Urbanicity,
         TOTSQFT_EN, ACUsed,
         DOLLAREL) %>%
  rbind(
    tibble(
      DOEID = NA,
      Region = "South",
      Urbanicity = "Urban Area",
      TOTSQFT_EN = 2500,
      ACUsed = TRUE,
      DOLLAREL = NA
    )
  ) %>% 
  tail(1)

pred_data <- augment(m_electric_multi, newdata = add_data) %>%
  mutate(.se.fit = sqrt(attr(.fitted, "var")), 
         .fitted = as.numeric(.fitted)) 

pred_data 
```

In the above example, it is predicted that the energy expenditure would be \$`r pred_data %>% slice_tail(n=1) %>% pull(.fitted) %>% prettyNum(big.mark=",")`.
\index{Residential Energy Consumption Survey (RECS)|)} \index{Linear regression|)}

## Logistic regression

\index{Categorical data|(} \index{Logistic regression|(} \index{logit models|see {Logistic regression}}
Logistic regression is used to model binary outcomes, such as whether or not someone voted. There are several instances where an outcome may not be originally binary but is collapsed into being binary. For example, given that gender is often asked in surveys with multiple response options and not a binary scale, many researchers now code gender in logistic modeling as "cis-male" compared to not "cis-male." We could also convert a 4-point Likert scale that has levels of "Strongly Agree," "Agree," "Disagree," and "Strongly Disagree" to group the agreement levels into one group and disagreement levels into a second group.

Logistic regression is a specific case of the generalized linear model (GLM). A GLM uses a link function to link the response variable to the linear model. If we tried to use a normal linear regression with a binary outcome, many assumptions would not hold, namely, the response would not be continuous. Logistic regression allows us to link a linear model between the covariates and the propensity of an outcome. In logistic regression, the link model is the logit function. Specifically, the model is specified as follows:

$$ y_i \sim \text{Bernoulli}(\pi_i)$$

```{=tex}
\begin{equation}
\log \left(\frac{\pi_i}{1-\pi_i} \right)=\beta_0 +\sum_{i=1}^n \beta_i x_i
\end{equation}
```
which can be re-expressed as

$$ \pi_i=\frac{\exp \left(\beta_0 +\sum_{i=1}^n \beta_i x_i \right)}{1+\exp \left(\beta_0 +\sum_{i=1}^n \beta_i x_i \right)}$$ where $y_i$ is the outcome, $\beta_0$ is an intercept, and $x_1, \cdots, x_n$ are the predictors with $\beta_1, \cdots, \beta_n$ as the associated coefficients. 

The Bernoulli distribution is a distribution which has an outcome of 0 or 1 given some probability ($\pi_i$) in this case, and we model $\pi_i$ as a function of the covariates $x_i$ using this logit link.
\index{Categorical data|)}

Assumptions in logistic regression using survey data include:

  - The outcome variable has two levels
  - There is a linear relationship between the independent variables and the log odds (the equation for the logit function)
  - The residuals are homoscedastic; that is, the error term is the same across all values of independent variables



### Syntax

The syntax for logistic regression is as follows: 

``` r
des_obj %>%
  svyglm(
    formula = outcomevar ~ x1 + x2 + x3,
    design = .,
    na.action = na.omit,
    df.resid = NULL,
    family = quasibinomial
  )
```

The arguments are:

* `formula`: Formula in the form of `y~x`
* \index{Degrees of freedom|(}`design`: a `tbl_svy` object created by `as_survey`\index{Degrees of freedom|)}
* `na.action`: handling of missing data
* `df.resid`: degrees of freedom for Wald tests (optional); defaults to using `degf(design)-p` where $p$ is the rank of the design matrix
* `family`: the error distribution/link function to be used in the model

Note `svyglm()` is the same function used in both ANOVA and normal linear regression. However, we've added the link function quasibinomial. While we can use the binomial link function, it is recommended to use the quasibinomial as our weights may not be integers, and the quasibinomial also allows for overdispersion [@lumley2010complex; @mccullagh1989binary;  @R-base]. The quasibinomial family has a default logit link, which is specified in the equations above. When specifying the outcome variable, it is likely specified in one of three ways with survey data:

  - \index{Factor|(}A two-level factor variable where the first level of the factor indicates a "failure," and the second level indicates a "success"\index{Factor|)}
  - A numeric variable which is 1 or 0 where 1 indicates a success
  - A logical variable where TRUE indicates a success

### Examples

#### Example 1: Logistic regression with single variable {.unnumbered}

\index{American National Election Studies (ANES)|(}
In the following example, we use the ANES data to model whether someone usually has trust in the government^[Question text: "How often can you trust the federal government in Washington to do what is right?" [@anes-svy]] by whom someone voted for president in 2020. As a reminder, the leading candidates were Biden and Trump, though people could vote for someone else not in the Democratic or Republican parties. Those votes are all grouped into an "Other" category. \index{Factor|(}We first create a binary outcome for trusting in the government by collapsing "Always" and "Most of the time" into a single-factor level, and the other response options ("About half the time," "Some of the time," and "Never") into a second factor level.  Next, a scatter plot of the raw data is not useful, as it is all 0 and 1 outcomes; so instead, we plot a summary of the data. \index{Functions in srvyr!survey\_mean|(} \index{Functions in srvyr!summarize|(} \index{Factor|)}

```{r}
#| label: model-logisticexamp-plot
#| fig.cap: Relationship between candidate selection and trust in government, ANES 2020
#| fig.alt: Bar chart with x-axis of election choice with levels of Biden, Trump, and Other and y-axis of 'Usually trust the government' ranging from 0% to 20%. Bar 1 is centered at 1, and length is from 0 to 0.12 with fill color dark blue which maps to VotedPres2020_selection = Biden. Bar 2 is centered at 2, and length is from 0 to 0.17 with fill color very pale blue which maps to VotedPres2020_selection = Trump. Bar 3 is centered at 3, and length is from 0 to 0.06 with fill color moderate purple which maps to VotedPres2020_selection = Other. Error bars are drawn as well with the width of the Biden and Trump error bars being similar and the error bar for Other being significantly wider.
#| warning: false
anes_des_der <- anes_des %>%
  mutate(TrustGovernmentUsually = case_when(
    is.na(TrustGovernment) ~ NA,
    TRUE ~ TrustGovernment %in% c("Always", "Most of the time")
  ))

anes_des_der %>%
  group_by(VotedPres2020_selection) %>%
  summarize(pct_trust = survey_mean(TrustGovernmentUsually,
                                    na.rm = TRUE,
                                    proportion = TRUE,
                                    vartype = "ci"),
    .groups = "drop") %>%
  filter(complete.cases(.)) %>%
  ggplot(aes(x = VotedPres2020_selection, y = pct_trust, 
             fill = VotedPres2020_selection)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = pct_trust_low, ymax = pct_trust_upp), 
                width = .2) +
  scale_fill_manual(values = c("#0b3954", "#bfd7ea", "#8d6b94")) +
  xlab("Election choice (2020)") +
  ylab("Usually trust the government") +
  scale_y_continuous(labels = scales::percent) +
  guides(fill = "none") +
  theme_minimal()
```
\index{Functions in srvyr!summarize|)} \index{Functions in srvyr!survey\_mean|)}

Looking at Figure \@ref(fig:model-logisticexamp-plot), it appears that people who voted for Trump are more likely to say that they usually have trust in the government compared to those who voted for Biden and other candidates. To determine if this insight is accurate, we next fit the model. 

```{r}
#| label: model-logisticexamp-model
logistic_trust_vote <- anes_des_der %>%
  svyglm(design = .,
         formula = TrustGovernmentUsually ~ VotedPres2020_selection,
         family = quasibinomial) 
```

```{r}
#| label: model-logisticexamp-noeval
#| eval: FALSE
tidy(logistic_trust_vote) %>%
  mutate(p.value=pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number()
```

(ref:model-logisticexamp-tab) Logistic regression output predicting trust in government by presidential candidate selection, RECS 2020

```{r}
#| label: model-logisticexamp-tab
#| echo: FALSE
#| warning: FALSE

tidy(logistic_trust_vote) %>%
  mutate(p.value=pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number() %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

In Table \@ref(tab:model-logisticexamp-tab), we can see the estimated coefficients (`estimate`), estimated standard errors of the coefficients (`std.error`), the t-statistic (`statistic`), and the p-value for each coefficient.  This output indicates that respondents who voted for Trump are more likely to usually have trust in the government compared to those who voted for Biden (the reference level). The coefficient of `r signif(logistic_trust_vote$coefficients[2],3)` represents the increase in the log odds of usually trusting the government.

In most cases, it is easier to talk about the odds instead of the log odds. To do this, we need to exponentiate the coefficients. We can use the same `tidy()` function but include the argument `exponentiate = TRUE` to see the odds.

```{r}
#| label: model-logisticexamp-model-odds-noeval
#| eval: FALSE
tidy(logistic_trust_vote, exponentiate = TRUE) %>% 
  select(term, estimate) %>%
  gt() %>%
  fmt_number()
```

(ref:model-logisticexamp-model-odds-tab) Logistic regression predicting trust in government by presidential candidate selection with exponentiated coefficients (odds), RECS 2020

```{r}
#| label: model-logisticexamp-model-odds-tab
#| echo: FALSE
#| warning: FALSE
tidy(logistic_trust_vote, exponentiate = TRUE) %>%
  select(term, estimate) %>%
  gt() %>%
  fmt_number() %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

```{r}
#| label: model-logisticcalc
#| echo: false
or_trump <-
  tidy(logistic_trust_vote, exponentiate = TRUE) %>%
  filter(str_detect(term, "Trump")) %>%
  pull(estimate)

or_other <-
  tidy(logistic_trust_vote, exponentiate = TRUE) %>%
  filter(str_detect(term, "Other")) %>%
  pull(estimate)
```

Using the output in Table \@ref(tab:model-logisticexamp-model-odds-tab), we can interpret this as saying that the odds of usually trusting the government for someone who voted for Trump is `r signif(or_trump*100, 3)`% as likely to trust the government compared to a person who voted for Biden (the reference level). In comparison, a person who voted for neither Biden nor Trump is `r signif(or_other*100, 3)`% as likely to trust the government as someone who voted for Biden.

As with linear regression, the `augment()` can be used to predict values. By default, the prediction is the link function, not the probability model. To predict the probability, add an argument of `type.predict="response"` as demonstrated below:

```{r}
#| label: model-logistic-aug
#| warning: false
logistic_trust_vote %>%
  augment(type.predict = "response") %>%
  mutate(.se.fit = sqrt(attr(.fitted, "var")),
         .fitted = as.numeric(.fitted)) %>%
  select(TrustGovernmentUsually,
         VotedPres2020_selection,
         .fitted,
         .se.fit) 
```


#### Example 2: Interaction effects {.unnumbered}

\index{Interaction effects|(}

Let's look at another example with interaction effects.  If we're interested in understanding the demographics of people who voted for Biden among all voters in 2020, we could include the indicator of whether respondents voted early (`EarlyVote2020`) and their income group (`Income7`) in our model.  

First, we need to subset the data to 2020 voters and then create an indicator for who voted for Biden. \index{Functions in srvyr!filter|(}

```{r}
#| label: model-logisticexamp-biden-ind
anes_des_ind <- anes_des %>%
  filter(!is.na(VotedPres2020_selection)) %>%
  mutate(VoteBiden = case_when(VotedPres2020_selection == "Biden" ~ 1,
                               TRUE ~ 0))
```
\index{Functions in srvyr!filter|)}

Let's first look at the main effects of income grouping and early voting behavior.

```{r}
#| label: model-logisticexamp-biden-main
log_biden_main <- anes_des_ind %>%
  mutate(
    EarlyVote2020 = fct_relevel(EarlyVote2020, "No", after = 0)
  ) %>%
  svyglm(
    design = .,
    formula = VoteBiden ~ EarlyVote2020 + Income7,
    family = quasibinomial
  ) 
```

```{r}
#| label: model-logisticexamp-biden-main-noeval
#| eval: FALSE
tidy(log_biden_main) %>%
  mutate(p.value = pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number()
```

(ref:model-logisticexamp-biden-main-tab) Logistic regression output for predicting voting for Biden given early voting behavior and income; main effects only, ANES 2020

```{r}
#| label: model-logisticexamp-biden-main-tab
#| echo: FALSE
#| warning: FALSE

tidy(log_biden_main) %>%
  mutate(p.value = pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number() %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

\index{p-value|(} 
This main effect model (see Table \@ref(tab:model-logisticexamp-biden-main-tab)) indicates that people with incomes of \$125,000 or more have a significant negative coefficient -`r signif(log_biden_main$coefficients[8],3)` (p-value is `r tidy(log_biden_main) %>% slice(8) %>% pull(p.value) %>% pretty_p_value()`). This indicates that people with incomes of \$125,000 or more were less likely to vote for Biden in the 2020 election compared to people with incomes of \$20,000 or less (reference level).
\index{p-value|)} 

Although early voting behavior was not significant, there may be an interaction between income and early voting behavior. To determine this, we can create a model that includes the interaction effects:

```{r}
#| label: model-logisticexamp-biden-int
log_biden_int <- anes_des_ind %>%
  mutate(
    EarlyVote2020 = fct_relevel(EarlyVote2020, "No", after = 0)
  ) %>% 
  svyglm(design = .,
         formula = VoteBiden ~ (EarlyVote2020 + Income7)^2,
         family = quasibinomial) 
```

```{r}
#| label: model-logisticexamp-biden-int-noeval
#| eval: FALSE
tidy(log_biden_int) %>%
  mutate(p.value=pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number()
```

(ref:model-logisticexamp-biden-int-tab) Logistic regression output for predicting voting for Biden given early voting behavior and income; with interaction, ANES 2020

```{r}
#| label: model-logisticexamp-biden-int-tab
#| echo: FALSE
#| warning: FALSE

tidy(log_biden_int) %>%
  mutate(p.value=pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number() %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

The results from the interaction model (see Table \@ref(tab:model-logisticexamp-biden-int-tab)) show that one interaction between early voting behavior and income is significant. To better understand what this interaction means, we can plot the predicted probabilities with an interaction plot.  Let's first obtain the predicted probabilities for each possible combination of variables using the `augment()` function.

```{r}
#| label: model-logisticexamp-biden-aug
#| warning: false
log_biden_pred <- log_biden_int %>%
  augment(type.predict = "response") %>%
  mutate(.se.fit = sqrt(attr(.fitted, "var")),
         .fitted = as.numeric(.fitted)) %>%
  select(VoteBiden, EarlyVote2020, Income7, .fitted, .se.fit) 
```

The y-axis is the predicted probabilities, one of our x-variables is on the x-axis, and the other is represented by multiple lines. Figure \@ref(fig:model-logisticexamp-biden-plot) shows the interaction plot with early voting behavior on the x-axis and income represented by the lines.

```{r}
#| label: model-logisticexamp-biden-plot
#| fig.cap: Interaction plot of early voting and income predicting the probability of voting for Biden
#| fig.alt: "Line plot with x-axis as indicator for voted early, with did not early vote on the left and did early vote on the right, and y-axis as 'Predicted Probability of Voting for Biden'. There are seven lines for income groups with lines being from top to bottom: Under $20k, $80k to less than $100k, $40k to less than $60k, $100k to less than $125k, $20k to less than 40k, $125k or more, and $60k to less than $80k. The lines for $40k to less than $60k, $60k to less than $80k, and $125k or more are all relatively flat with the probabilities for did not early vote and did early vote being equivalent. The lines for $20k to less than $40k and $100k to less than $125k have a slight positive slope. The line for less than $20k has a slight negative slope and has overall the highest probability for both levels of early voting. The line for $80k to less than $100k has a large positive slope. This line shows the lowest probability for those who did not early vote, and the second highest probability for those who did early vote."

log_biden_pred %>%
  filter(VoteBiden == 1) %>%
  distinct() %>%
  arrange(EarlyVote2020, Income7) %>%
  ggplot(aes(
    x = EarlyVote2020,
    y = .fitted,
    group = Income7,
    color = Income7,
    linetype = Income7
  )) +
  geom_line(linewidth = 1.1) +
  scale_color_manual(values = colorRampPalette(book_colors)(7)) +
  ylab("Predicted Probability of Voting for Biden") +
  labs(x = "Voted Early",
       color = "Income",
       linetype = "Income") +
  coord_cartesian(ylim = c(0, 1)) +
  guides(fill = "none") +
  theme_minimal()
```

From Figure \@ref(fig:model-logisticexamp-biden-plot), we can see that people who have incomes in most groups (e.g., \$40,000 to less than \$60,000) have roughly the same probability of voting for Biden regardless of whether they voted early or not. However, those with income in the \$100,000 to less than \$125,000 group were more likely to vote for Biden if they voted early than if they did not vote early. 

Interactions in models can be difficult to understand from the coefficients alone. Using these interaction plots can help others understand the nuances of the results.\index{Functions in survey!svyglm|)} \index{American National Election Studies (ANES)|)} \index{Logistic regression|)} \index{Interaction effects|)}

## Exercises

1.  The type of housing unit may have an impact on energy expenses. Is there any relationship between housing unit type (`HousingUnitType`) and total energy expenditure (`TOTALDOL`)? First, find the average energy expenditure by housing unit type as a descriptive analysis and then do the test. The reference level in the comparison should be the housing unit type that is most common.

2.  Does temperature play a role in electricity expenditure? Cooling degree days are a measure of how hot a place is. CDD65 for a given day indicates the number of degrees Fahrenheit warmer than 65°F (18.3°C) it is in a location. On a day that averages 65°F and below, CDD65=0, while a day that averages 85°F (29.4°C) would have CDD65=20 because it is 20 degrees Fahrenheit warmer [@eia-cdd]. Each day in the year is summed up to indicate how hot the place is throughout the year. Similarly, HDD65 indicates the days colder than 65°F. Can energy expenditure be predicted using these temperature indicators along with square footage? Is there a significant relationship? Include main effects and two-way interactions.

3.  Continuing with our results from Exercise 2, create a plot between the actual and predicted expenditures and a residual plot for the predicted expenditures.

4.  Early voting expanded in 2020 [@npr-voting-trend]. Build a logistic model predicting early voting in 2020 (`EarlyVote2020`) using age (`Age`), education (`Education`), and party identification (`PartyID`). Include two-way interactions.

5.  Continuing from Exercise 4, predict the probability of early voting for two people. Both are 28 years old and have a graduate degree; however, one person is a strong Democrat, and the other is a strong Republican.

<!--chapter:end:07-modeling.Rmd-->

# (PART) Reporting {-}

# Communication of results {#c08-communicating-results}

```{r}
#| label: results-styler
#| include: false
knitr::opts_chunk$set(tidy = 'styler')
```

::: {.prereqbox-header}
`r if (knitr:::is_html_output()) '### Prerequisites {- #prereq8}'`
:::

::: {.prereqbox data-latex="{Prerequisites}"}
For this chapter, load the following packages:
```{r}
#| label: results-setup
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(survey) 
library(srvyr) 
library(srvyrexploR)
library(gt)
library(gtsummary)
```

We are using data from ANES as described in Chapter \@ref(c04-getting-started). As a reminder, here is the code to create the design objects for each to use throughout this chapter. For ANES, we need to adjust the weight so it sums to the population instead of the sample (see the ANES documentation and Chapter \@ref(c04-getting-started) for more information).

```{r}
#| label: results-anes-des
#| eval: FALSE
targetpop <- 231592693

anes_adjwgt <- anes_2020 %>%
  mutate(Weight = Weight / sum(Weight) * targetpop)

anes_des <- anes_adjwgt %>%
  as_survey_design(
    weights = Weight,
    strata = Stratum,
    ids = VarUnit,
    nest = TRUE
  )
```
:::

## Introduction

After finishing the analysis and modeling, we proceed to the task of communicating the survey results. Our audience may range from seasoned researchers familiar with our survey data to newcomers encountering the information for the first time. We should aim to explain the methodology and analysis while presenting findings in an accessible way, and it is our responsibility to report information with care.

Before beginning any dissemination of results, consider questions such as:

  - How are we presenting results? Examples include a website, print, or other media. Based on the medium, we might limit or enhance the use of graphical representation. 
  - What is the audience's familiarity with the study and/or data? Audiences can range from the general public to data experts. If we anticipate limited knowledge about the study, we should provide detailed descriptions (we discuss recommendations later in the chapter).
  - What are we trying to communicate? It could be summary statistics, trends, patterns, or other insights. Tables may suit summary statistics, while plots are better at conveying trends and patterns.
  - Is the audience accustomed to interpreting plots? If not, include explanatory text to guide them on how to interpret the plots effectively.
  - What is the audience's statistical knowledge? If the audience does not have a strong statistics background, provide text on standard errors, confidence intervals, and other estimate types to enhance understanding.

## Describing results through text

As analysts, we often emphasize the data, and communicating results can sometimes be overlooked. To be effective communicators, we need to identify the appropriate information to share with our audience. Chapters \@ref(c02-overview-surveys) and \@ref(c03-survey-data-documentation) provide insights into factors we need to consider during analysis, and they remain relevant when presenting results to others.

### Methodology

If we are using existing data, methodologically sound surveys provide documentation about how the survey was fielded, the questionnaires, and other necessary information for analyses. \index{Population of interest|(}For example, the survey's methodology reports should include the population of interest, sampling procedures, response rates, questionnaire documentation, weighting, and a general overview of disclosure statements.\index{Population of interest|)} Many American organizations follow the American Association for Public Opinion Research's (AAPOR) [Transparency Initiative](https://aapor.org/standards-and-ethics/transparency-initiative). The AAPOR Transparency Initiative requires organizations to include specific details in their methodology, making it clear how we can and should analyze and interpret the results. Being transparent about these methods is vital for the scientific rigor of the field.

The details provided in Chapter \@ref(c02-overview-surveys) about the survey process should be shared with the audience when presenting the results. When using publicly available data, like the examples in this book, we can often link to the methodology report in our final output. We should also provide high-level information for the audience to quickly grasp the context around the findings. For example, we can mention when and where the study was conducted, the population's age range, or other contextual details. This information helps the audience understand how generalizable the results are.

Providing this material is especially important when no methodology report is available for the analyzed data. For example, if we conducted a new survey for a specific purpose, we should document and present all the pertinent information during the analysis and reporting process. Adhering to the AAPOR Transparency Initiative guidelines is a reliable method to guarantee that all essential information is communicated to the audience.

### Analysis

\index{American Community Survey (ACS)|(}
Along with the survey methodology and weight calculations, we should also share our approach to preparing, cleaning, and analyzing the data. For example, in Chapter \@ref(c06-statistical-testing), we compared education distributions from the ANES survey to the American Community Survey (ACS). To make the comparison, we had to collapse the education categories provided in the ANES data to match the ACS. The process for this particular example may seem straightforward (like combining bachelor's and graduate degrees into a single category), but there are multiple ways to deal with the data. Our choice is just one of many. We should document both the original ANES question and response options and the steps we took to match them with ACS data. This transparency helps clarify our analysis to our audience.
\index{American Community Survey (ACS)|)}

\index{Missing data|(}
Missing data is another instance where we want to be unambiguous and upfront with our audience. In this book, numerous examples and exercises remove missing data, as this is often the easiest way to handle them. However, there are circumstances where missing data holds substantive importance, and excluding them could introduce bias (see Chapter \@ref(c11-missing-data)). Being transparent about our handling of missing data is important to maintaining the integrity of our analysis and ensuring a comprehensive understanding of the results.
\index{Missing data|)}

### Results

While tables and graphs are commonly used to communicate results, there are instances where text can be more effective in sharing information. Narrative details, such as context around point estimates or model coefficients, can go a long way in improving our communication. We have several strategies to effectively convey the significance of the data to the audience through text.

First, we can highlight important data elements in a sentence using plain language. For example, if we were looking at election polling data conducted before an election, we could say:  

> As of [DATE], an estimated XX% of registered U.S. voters say they will vote for [CANDIDATE NAME] for president in the [YEAR] general election.

This sentence provides key pieces of information in a straightforward way:

 1. [DATE\]: Given that polling data are time-specific, providing the date of reference lets the audience know when these data were valid.
 2. Registered U.S. voters: This tells the audience who we surveyed, letting them know the population of interest.
 3. XX%: This part provides the estimated percentage of people voting for a specific candidate for a specific office.
 4. [YEAR] general election: Adding this gives more context about the election type and year. The estimate would take on a different meaning if we changed it to a primary election instead of a general election.
 
We also included the word "estimated." When presenting aggregate survey results, we have errors around each estimate. We want to convey this uncertainty rather than talk in absolutes. Words like "estimated," "on average," or "around" can help communicate this uncertainty to the audience. Instead of saying "XX%," we can also say "XX% (+/- Y%)" to show the margin of error. Confidence intervals can also be incorporated into the text to assist readers.  

Second, providing context and discussing the meaning behind a point estimate can help the audience glean some insight into why the data are important. For example, when comparing two values, it can be helpful to highlight if there are statistically significant differences and explain the impact and relevance of this information. This is where we should do our best to be mindful of biases and present the facts logically. 

Keep in mind how we discuss these findings can greatly influence how the audience interprets them. If we include speculation, phrases like "the authors speculate" or "these findings may indicate," it relays the uncertainty around the notion while still lending a plausible solution. Additionally, we can present alternative viewpoints or competing discussion points to explain the uncertainty in the results.

## Visualizing data

Although discussing key findings in the text is important, presenting large amounts of data in tables or visualizations is often more digestible for the audience. Effectively combining text, tables, and graphs can be powerful in communicating results. This section provides examples of using the {gt}, {gtsummary}, and {ggplot2} packages to enhance the dissemination of results [@R-gt; @gtsummarysjo; @ggplot2wickham].

### Tables

\index{Publication-ready tables|see {gtsummary}} \index{Publication-ready tables|see {gt tables}} 

Tables are a great way to provide a large amount of data when individual data points need to be examined. However, it is important to present tables in a reader-friendly format. Numbers should align, rows and columns should be easy to follow, and the table size should not compromise readability. Using key visualization techniques, we can create tables that are informative and nice to look at. \index{gt package|(}Many packages create easy-to-read tables (e.g., {kable} \+ {kableExtra}, {gt}, {gtsummary}, {DT}, {formattable}, {flextable}, {reactable}). We appreciate the flexibility, ability to use pipes (e.g., `%>%`), and numerous extensions of the {gt} package. While we focus on {gt} here, we encourage learning about others, as they may have additional helpful features.\index{gt package|)} \index{Replicate weights|(}\index{gtsummary|(} Please note, at this time, {gtsummary} needs additional features to be widely used for survey analysis, particularly due to its lack of ability to work with replicate designs.\index{Replicate weights|)} We provide one example using {gtsummary} and hope it evolves into a more comprehensive tool over time.\index{gtsummary|)} 

#### Transitioning {srvyr} output to a {gt} table {-}

\index{American National Election Studies (ANES)|(} \index{gt package|(} 
Let's start by using some of the data we calculated earlier in this book. In Chapter \@ref(c06-statistical-testing), we looked at data on trust in government with the proportions calculated below: \index{Functions in srvyr!survey\_prop|(} \index{Functions in srvyr!summarize|(} \index{Functions in srvyr!drop\_na}

```{r}
#| label: results-table-raw
trust_gov <- anes_des %>%
  drop_na(TrustGovernment) %>%
  group_by(TrustGovernment) %>%
  summarize(trust_gov_p = survey_prop())

trust_gov
```
\index{Functions in srvyr!summarize|)} \index{Functions in srvyr!survey\_prop|)}

The default output generated by R may work for initial viewing inside our IDE or when creating basic output in an R Markdown or Quarto document. However, when presenting these results in other publications, such as the print version of this book or with other formal dissemination modes, modifying the display can improve our reader's experience. 

Looking at the output from `trust_gov`, a couple of improvements stand out: (1) switching to percentages instead of proportions and (2) removing the variable names as column headers. The {gt} package is a good tool for implementing better labeling and creating publishable tables. Let's walk through some code as we make a few changes to improve the table's usefulness. 

First, we initiate the formatted table with the `gt()` function on the `trust_gov` tibble previously created. Next, we use the argument `rowname_col()` to designate the `TrustGovernment` column as the label for each row (called the table "stub"). We apply the `cols_label()` function to create informative column labels instead of variable names and then the `tab_spanner()` function to add a label across multiple columns. In this case, we label all columns except the stub with "Trust in Government, 2020." We then format the proportions into percentages with the `fmt_percent()` function and reduce the number of decimals shown to one with `decimals = 1`. Finally, the `tab_caption()` function adds a table title for the HTML version of the book. We can use the caption for cross-referencing in R Markdown, Quarto, and bookdown, as well as adding it to the list of tables in the book. These changes are all seen in Table \@ref(tab:results-table-gt1-tab).

```{r}
#| label: results-table-gt1
trust_gov_gt <- trust_gov %>%
  gt(rowname_col = "TrustGovernment") %>%
  cols_label(trust_gov_p = "%",
             trust_gov_p_se = "s.e. (%)") %>%
  tab_spanner(label = "Trust in Government, 2020",
              columns = c(trust_gov_p, trust_gov_p_se)) %>%
  fmt_percent(decimals = 1)
```

```{r}
#| label: results-table-gt1-noeval
#| eval: false
trust_gov_gt %>% 
  tab_caption("Example of {gt} table with trust in government estimate")
```

(ref:results-table-gt1-tab) Example of {gt} table with trust in government estimate

```{r}
#| label: results-table-gt1-tab
#| echo: FALSE
#| warning: FALSE

trust_gov_gt %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

We can add a few more enhancements, such as a title (which is different from a caption^[The `tab_caption()` function is intended for usage in R Markdown, Quarto, or bookdown to add cross-references across the document. The caption is placed within the table based on the output type. The `tab_header()` function adds a title or subtitle to a table in any context, including Shiny or GitHub-flavored Markdown, without cross-referencing. The header is placed within the table object itself.]), a data source note, and a footnote with the question information, using the functions `tab_header()`, `tab_source_note()`, and `tab_footnote()`. If having the percentage sign in both the header and the cells seems redundant, we can opt for `fmt_number()` instead of `fmt_percent()` and scale the number by 100 with `scale_by = 100`. The resulting table is displayed in Table \@ref(tab:results-table-gt2-tab).

```{r}
#| label: results-table-gt2
trust_gov_gt2 <- trust_gov_gt %>%
  tab_header("American voter's trust
             in the federal government, 2020") %>%
  tab_source_note(
    md("*Source*: American National Election Studies, 2020")
  ) %>%
  tab_footnote(
    "Question text: How often can you trust the federal government
    in Washington to do what is right?"
  ) %>%
  fmt_number(scale_by = 100,
             decimals = 1)
```

```{r}
#| label: results-table-gt2-noeval
#| eval: false
trust_gov_gt2
```

(ref:results-table-gt2-tab) Example of {gt} table with trust in government estimates and additional context

```{r}
#| label: results-table-gt2-tab
#| echo: FALSE
#| warning: FALSE

trust_gov_gt2 %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

\index{gt package|)}

#### Expanding tables using {gtsummary} {-}

\index{Design effect|(} \index{gtsummary|(} 
The {gtsummary} package simultaneously summarizes data and creates publication-ready tables. Initially designed for clinical trial data, it has been extended to include survey analysis in certain capacities. \index{Replicate weights|(}At this time, it is only compatible with survey objects using Taylor's Series Linearization and not replicate methods.\index{Replicate weights|)} While it offers a restricted set of summary statistics, the following are available for categorical variables:

\index{Categorical data|(}

  - `{n}` frequency
  - `{N}` denominator, or respondent population
  - `{p}` proportion (stylized as a percentage by default)
  - `{p.std.error}` standard error of the sample proportion
  - `{deff}` design effect of the sample proportion
  - `{n_unweighted}` unweighted frequency
  - `{N_unweighted}` unweighted denominator
  - `{p_unweighted}` unweighted formatted proportion (stylized as a percentage by default)

\index{Categorical data|)}

The following summary statistics are available for continuous variables:

\index{Continuous data|(}

  - `{median}` median
  - `{mean}` mean
  - `{mean.std.error}` standard error of the sample mean
  - `{deff}` design effect of the sample mean
  - `{sd}` standard deviation
  - `{var}` variance
  - `{min}` minimum
  - `{max}` maximum
  - `{p#}` any integer percentile, where `#` is an integer from 0 to 100
  - `{sum}` sum
  
\index{Continuous data|)} 

\index{Design effect|)}
In the following example, we build a table using {gtsummary}, similar to the table in the {gt} example. The main function we use is `tbl_svysummary()`. In this function, we include the variables we want to analyze in the `include` argument and define the statistics we want to display in the `statistic` argument. To specify the statistics, we apply the syntax from the {glue} package, where we enclose the variables we want to insert within curly brackets. We must specify the desired statistics using the names listed above. For example, to specify that we want the proportion followed by the standard error of the proportion in parentheses, we use `{p} ({p.std.error})`. Table \@ref(tab:results-gts-ex-1-tab) displays the resulting table.

```{r}
#| label: results-gts-ex-1
anes_des_gtsum <- anes_des %>%
  tbl_svysummary(include = TrustGovernment,
                 statistic = list(all_categorical() ~ "{p} ({p.std.error})")) 
```

```{r}
#| label: results-table-gt3-noeval
#| eval: false
anes_des_gtsum
```

(ref:results-gts-ex-1-tab) Example of {gtsummary} table with trust in government estimates

```{r}
#| label: results-gts-ex-1-tab
#| echo: FALSE
#| warning: FALSE

anes_des_gtsum %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

The default table (shown in Table \@ref(tab:results-gts-ex-1-tab)) includes the weighted number of missing (or Unknown) records. The standard error is reported as a proportion, while the proportion is styled as a percentage. In the next step, we remove the Unknown category by setting the missing argument to "no" and format the standard error as a percentage using the `digits` argument. To improve the table for publication, we provide a more polished label for the "TrustGovernment" variable using the `label` argument. The resulting table is displayed in Table \@ref(tab:results-gts-ex-2-tab).

```{r}
#| label: results-gts-ex-2
anes_des_gtsum2 <- anes_des %>%
  tbl_svysummary(
    include = TrustGovernment,
    statistic = list(all_categorical() ~ "{p} ({p.std.error})"),
    missing = "no",
    digits = list(TrustGovernment ~ style_percent),
    label = list(TrustGovernment ~ "Trust in Government, 2020")
  )
```

```{r}
#| label: results-gts-ex-2-noeval
#| eval: false
anes_des_gtsum2
```

(ref:results-gts-ex-2-tab) Example of {gtsummary} table with trust in government estimates with labeling and digits options

```{r}
#| label: results-gts-ex-2-tab
#| echo: FALSE
#| warning: FALSE

anes_des_gtsum2 %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

Table \@ref(tab:results-gts-ex-2-tab) is closer to our ideal output, but we still want to make a few changes. To exclude the term "Characteristic" and the estimated population size (N), we can modify the header using the `modify_header()` function to update the `label`. Further adjustments can be made based on personal preferences, organizational guidelines, or other style guides. If we prefer having the standard error in the header, similar to the {gt} table, instead of in the footnote (the {gtsummary} default), we can make these changes by specifying `stat_0` in the `modify_header()` function. Additionally, using `modify_footnote()` with `update = everything() ~ NA` removes the standard error from the footnote. After transforming the object into a {gt} table using `as_gt()`, we can add footnotes and a title using the same methods explained in the previous section. This updated table is displayed in Table \@ref(tab:results-gts-ex-3-tab).

```{r}
#| label: results-gts-ex-3
anes_des_gtsum3 <- anes_des %>%
  tbl_svysummary(
    include = TrustGovernment,
    statistic = list(all_categorical() ~ "{p} ({p.std.error})"),
    missing = "no",
    digits = list(TrustGovernment ~ style_percent),
    label = list(TrustGovernment ~ "Trust in Government, 2020")
  ) %>%
  modify_footnote(update = everything() ~ NA) %>%
  modify_header(label = " ",
                stat_0 = "% (s.e.)") %>%
  as_gt() %>%
  tab_header("American voter's trust
             in the federal government, 2020") %>%
  tab_source_note(
    md("*Source*: American National Election Studies, 2020")
  ) %>%
  tab_footnote(
    "Question text: How often can you trust the federal government
    in Washington to do what is right?"
  )
```

```{r}
#| label: results-gts-ex-3-noeval
#| eval: false
anes_des_gtsum3
```

(ref:results-gts-ex-3-tab) Example of {gtsummary} table with trust in government estimates with more labeling options and context

```{r}
#| label: results-gts-ex-3-tab
#| echo: FALSE
#| warning: FALSE

anes_des_gtsum3 %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

We can also include summaries of more than one variable in the table. These variables can be either categorical or continuous. In the following code and Table \@ref(tab:results-gts-ex-4-tab), we add the mean age by updating the `include`, `statistic`, and `digits` arguments.

```{r}
#| label: results-gts-ex-4
#| tidy: FALSE
anes_des_gtsum4 <- anes_des %>%
  tbl_svysummary(
    include = c(TrustGovernment, Age),
    statistic = list(
      all_categorical() ~ "{p} ({p.std.error})",
      all_continuous() ~ "{mean} ({mean.std.error})"
    ),
    missing = "no",
    digits = list(TrustGovernment ~ style_percent,
                  Age ~ c(1, 2)),
    label = list(TrustGovernment ~ "Trust in Government, 2020")
  ) %>%
  modify_footnote(update = everything() ~ NA) %>%
  modify_header(label = " ",
                stat_0 = "% (s.e.)") %>%
  as_gt() %>%
  tab_header(
    "American voter's trust in the federal government, 2020") %>%
  tab_source_note(
    md("*Source*: American National Election Studies, 2020")
  ) %>%
  tab_footnote(
    "Question text: How often can you trust the federal government
    in Washington to do what is right?"
  ) %>%
  tab_caption("Example of {gtsummary} table with trust in government
              estimates and average age")
```

```{r}
#| label: results-gts-ex-4-noeval
#| eval: false
anes_des_gtsum4
```

(ref:results-gts-ex-4-tab) Example of {gtsummary} table with trust in government estimates and average age

```{r}
#| label: results-gts-ex-4-tab
#| echo: FALSE
#| warning: FALSE

anes_des_gtsum4 %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

With {gtsummary}, we can also calculate statistics by different groups. Let's modify the previous example (displayed in Table \@ref(tab:results-gts-ex-4-tab)) to analyze data on whether a respondent voted for president in 2020. We update the `by` argument and refine the header. The resulting table is displayed in Table \@ref(tab:results-gts-ex-5-tab). \index{Functions in srvyr!drop\_na}

```{r}
#| label: results-gts-ex-5
#| messages: FALSE
anes_des_gtsum5 <- anes_des %>%
  drop_na(VotedPres2020) %>%
  tbl_svysummary(
    include = TrustGovernment,
    statistic = list(all_categorical() ~ "{p} ({p.std.error})"),
    missing = "no",
    digits = list(TrustGovernment ~ style_percent),
    label = list(TrustGovernment ~ "Trust in Government, 2020"),
    by = VotedPres2020
  ) %>%
  modify_footnote(update = everything() ~ NA) %>%
  modify_header(label = " ",
                stat_1 = "Voted",
                stat_2 = "Didn't vote") %>%
  modify_spanning_header(all_stat_cols() ~ "% (s.e.)") %>%
  as_gt() %>%
  tab_header(
    "American voter's trust
             in the federal government by whether they voted
             in the 2020 presidential election"
  ) %>%
  tab_source_note(
    md("*Source*: American National Election Studies, 2020")
  ) %>%
  tab_footnote(
    "Question text: How often can you trust the federal government
    in Washington to do what is right?"
  )
```

```{r}
#| label: results-gts-ex-5-noeval
#| eval: false
anes_des_gtsum5
```

(ref:results-gts-ex-5-tab) Example of {gtsummary} table with trust in government estimates by voting status

```{r}
#| label: results-gts-ex-5-tab
#| echo: FALSE
#| warning: FALSE

anes_des_gtsum5 %>%
    print_gt_book(knitr::opts_current$get()[["label"]])
```

\index{gtsummary|)} 

### Charts and plots

\index{Plots|(} \index{Charts|see {Plots }} \index{ggplot|see {Plots }} \index{Graphs| see {Plots }}
Survey analysis can yield an abundance of printed summary statistics and models. Even with the most careful analysis, interpreting the results can be overwhelming. This is where charts and plots play a key role in our work. By transforming complex data into a visual representation, we can recognize patterns, relationships, and trends with greater ease.

R has numerous packages for creating compelling and insightful charts. In this section, we focus on {ggplot2}, a member of the {tidyverse} collection of packages. Known for its power and flexibility, {ggplot2} is an invaluable tool for creating a wide range of data visualizations [@ggplot2wickham].

The {ggplot2} package follows the "grammar of graphics," a framework that incrementally adds layers of chart components. This approach allows us to customize visual elements such as scales, colors, labels, and annotations to enhance the clarity of our results. After creating the survey design object, we can modify it to include additional outcomes and calculate estimates for our desired data points. Below, we create a binary variable `TrustGovernmentUsually`, which is `TRUE` when `TrustGovernment` is "Always" or "Most of the time" and `FALSE` otherwise. Then, we calculate the percentage of people who usually trust the government based on their vote in the 2020 presidential election (`VotedPres2020_selection`). We remove the cases where people did not vote or did not indicate their choice. \index{Functions in srvyr!survey\_mean|(} \index{Functions in srvyr!summarize|(} \index{Functions in srvyr!drop\_na}

```{r}
#| label: results-anes-prep
anes_des_der <- anes_des %>%
  mutate(TrustGovernmentUsually = case_when(
    is.na(TrustGovernment) ~ NA,
    TRUE ~ TrustGovernment %in% c("Always", "Most of the time")
  )) %>%
  drop_na(VotedPres2020_selection) %>%
  group_by(VotedPres2020_selection) %>%
  summarize(
    pct_trust = survey_mean(
      TrustGovernmentUsually,
      na.rm = TRUE,
      proportion = TRUE,
      vartype = "ci"
    ),
    .groups = "drop"
  )

anes_des_der
```
\index{Functions in srvyr!summarize|)} \index{Functions in srvyr!survey\_mean|)}

Now, we can begin creating our chart with {ggplot2}. First, we set up our plot with `ggplot()`. Next, we define the data points to be displayed using aesthetics, or `aes`. Aesthetics represent the visual properties of the objects in the plot. In the following example, we create a bar chart of the percentage of people who usually trust the government by who they voted for in the 2020 election.  To do this, we want to have who they voted for on the x-axis (`VotedPres2020_selection`) and the percent they usually trust the government on the y-axis (`pct_trust`). We specify these variables in `ggplot()` and then indicate we want a bar chart with `geom_bar()`. The resulting plot is displayed in Figure \@ref(fig:results-plot1).

```{r}
#| label: results-plot1
#| fig.cap: "Bar chart of trust in government, by chosen 2020 presidential candidate"
#| fig.alt: "Bar chart with x-axis of 'VotedPres2020_selection' with labels Biden, Trump and Other. It has y-axis 'pct_trust' with labels 0.00, 0.05, 0.10 and 0.15. The chart is a bar chart with 3 vertical bars. Bar 1 (Biden) has a height of 0.12. Bar 2 (Trump) has a height of 0.17. Bar 3 (Other) has a height of 0.06."
p <- anes_des_der %>%
  ggplot(aes(x = VotedPres2020_selection,
             y = pct_trust)) +
  geom_bar(stat = "identity")

p
```

This is a great starting point: it appears that a higher percentage of people state they usually trust the government among those who voted for Trump compared to those who voted for Biden or other candidates. Now, what if we want to introduce color to better differentiate the three groups? We can add `fill` under `aesthetics`, indicating that we want to use distinct colors for each value of `VotedPres2020_selection`. In this instance, Biden and Trump are displayed in different colors in Figure \@ref(fig:results-plot2).

```{r}
#| label: results-plot2
#| fig.cap: "Bar chart of trust in government by chosen 2020 presidential candidate, with colors"
#| fig.alt: "Bar chart with x-axis of 'VotedPres2020_selection' with labels Biden, Trump and Other. It has y-axis 'pct_trust' with labels 0.00, 0.05, 0.10 and 0.15. The chart is a bar chart with 3 vertical bars. Bar 1 (Biden) has a height of 0.12 and a color of strong reddish orange. Bar 2 (Trump) has a height of 0.17 and a color of vivid yellowish green. Bar 3 (Other) has a height of 0.06 and color of brilliant blue."
pcolor <- anes_des_der %>%
  ggplot(aes(x = VotedPres2020_selection,
             y = pct_trust,
             fill = VotedPres2020_selection)) +
  geom_bar(stat = "identity")

pcolor
```

Let's say we wanted to follow proper statistical analysis practice and incorporate variability in our plot. We can add another geom, `geom_errorbar()`, to display the confidence intervals on top of our existing `geom_bar()` layer. We can add the layer using a plus sign (`+`). The resulting graph is displayed in Figure \@ref(fig:results-plot3).

```{r}
#| label: results-plot3
#| fig.cap: "Bar chart of trust in government by chosen 2020 presidential candidate, with colors and error bars"
#| fig.alt: "Bar chart with x-axis of 'VotedPres2020_selection' with labels Biden, Trump and Other. It has y-axis 'pct_trust' with labels 0.00, 0.05, 0.10 and 0.15. The chart is a bar chart with 3 vertical bars. Bar 1 (Biden) has a height of 0.12 and a color of strong reddish orange. Bar 2 (Trump) has a height of 0.17 and a color of vivid yellowish green. Bar 3 (Other) has a height of 0.06 and color of brilliant blue. Error bars are added with the Bar 1 (Biden) error ranging from 0.11 to 0.14, Bar 2 (Trump) error ranging from 0.16 to 0.19, and the Bar 3 (Other) error ranging from 0.02 to 0.14."
pcol_error <- anes_des_der %>%
  ggplot(aes(x = VotedPres2020_selection,
             y = pct_trust,
             fill = VotedPres2020_selection)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = pct_trust_low,
                    ymax = pct_trust_upp),
                width = .2)

pcol_error
```

We can continue adding to our plot until we achieve our desired look. For example, since the color legend does not contribute meaningful information, we can eliminate it with `guides(fill = "none")`. We can also specify colors for `fill` using `scale_fill_manual()`. Inside this function, we provide a vector of values corresponding to the colors in our plot. These values are hexadecimal (hex) color codes, denoted by a leading pound sign `#` followed by six letters or numbers. The hex code `#0b3954` used below is dark blue. There are many tools online that help pick hex codes, such as htmlcolorcodes.com. Additionally, Figure \@ref(fig:results-plot4) incorporates better labels for the x and y axes (`xlab()`, `ylab()`), a title (`labs(title=)`), and a footnote with the data source (`labs(caption=)`).

```{r}
#| label: results-plot4
#| fig.cap: "Bar chart of trust in government by chosen 2020 presidential candidate with colors, labels, error bars, and title"
#| fig.alt: "Bar chart with x-axis of 'VotedPres2020_selection' with labels Biden, Trump and Other. It has y-axis 'pct_trust' with labels 0.00, 0.05, 0.10 and 0.15. The chart is a bar chart with 3 vertical bars. Bar 1 (Biden) has a height of 0.12 and a color of dark blue. Bar 2 (Trump) has a height of 0.17 and a color of very pale blue. Bar 3 (Other) has a height of 0.06 and color of moderate purple. Error bars are added with the Bar 1 (Biden) error ranging from 0.11 to 0.14, Bar 2 (Trump) error ranging from 0.16 to 0.19, and the Bar 3 (Other) error ranging from 0.02 to 0.14."
pfull <-
  anes_des_der %>%
  ggplot(aes(x = VotedPres2020_selection,
             y = pct_trust,
             fill = VotedPres2020_selection)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = pct_trust_low,
                    ymax = pct_trust_upp),
                width = .2) +
  scale_fill_manual(values = c("#0b3954", "#bfd7ea", "#8d6b94")) +
  xlab("Election choice (2020)") +
  ylab("Usually trust the government") +
  scale_y_continuous(labels = scales::percent) +
  guides(fill = "none") +
  labs(title = "Percent of voters who usually trust the government
       by chosen 2020 presidential candidate",
       caption = "Source: American National Election Studies, 2020")

pfull
```

What we have explored in this section are just the foundational aspects of {ggplot2}, and the capabilities of this package extend far beyond what we have covered. Advanced features such as annotation, faceting, and theming allow for more sophisticated and customized visualizations. The {ggplot2} book by  @ggplot2wickham is a comprehensive guide to learning more about this powerful tool.
\index{American National Election Studies (ANES)|)} \index{Plots|)}

<!--chapter:end:08-communicating-results.Rmd-->

# Reproducible research {#c09-reprex-data}

```{r}
#| label: reprex-styler
#| include: false
#| message: false
knitr::opts_chunk$set(tidy = 'styler')
```

## Introduction

Reproducing results is an important aspect of any research. First, reproducibility serves as a form of quality assurance. If we pass an analysis project to another person, they should be able to run the entire project from start to finish and obtain the same results. They can critically assess the methodology and code while detecting potential errors. Another goal of reproducibility is enabling the verification of our analysis. When someone else is able to check our results, it ensures the integrity of the analyses by determining that the conclusions are not dependent on a particular person running the code or workflow on a particular day or in a particular environment.

Not only is reproducibility a key component in ethical and accurate research, but it is also a requirement for many scientific journals. For example, the *Journal of Survey Statistics and Methodology* (JSSAM) and *Public Opinion Quarterly* (POQ) require authors to make code, data, and methodology transparent and accessible to other researchers who wish to verify or build on existing work.

Reproducible research requires that the key components of analysis are available, discoverable, documented, and shared with others. The four main components that we should consider are:

  - Code: source code used for data cleaning, analysis, modeling, and reporting
  - Data: raw data used in the workflow, or if data are sensitive or proprietary, as much data as possible that would allow others to run our workflow or provide details on how to access the data (e.g., access to a restricted use file (RUF))
  - Environment: environment of the project, including the R version, packages, operating system, and other dependencies used in the analysis
  - Methodology: survey and analysis methodology, including rationale behind sample, questionnaire and analysis decisions, interpretations, and assumptions

In Chapter \@ref(c08-communicating-results), we briefly mention how each of these is important to include in the methodology report and when communicating the findings of a study. However, to be transparent and effective analysts, we need to ensure we not only discuss these through text but also provide files and additional information when requested. Often, when starting a project, we may be eager to jump into the data and make decisions as we go without full documentation. This can be challenging if we need to go back and make changes or understand even what we did a few months ago. It benefits other analysts and potentially our future selves to document everything from the start. The good news is that many tools, practices, and project management techniques make survey analysis projects easy to reproduce. For best results, we should decide which techniques and tools to use before starting a project (or very early on).

This chapter covers some of our suggestions for tools and techniques we can use in projects. This list is not comprehensive but aims to provide a starting point for those looking to create a reproducible workflow.

## Project-based workflows

\index{R projects|(}
We recommend a project-based workflow for analysis projects as described by @wickham2023r4ds. A project-based workflow maintains a "source of truth" for our analyses. It helps with file system discipline by putting everything related to a project in a designated folder. Since all associated files are in a single location, they are easy to find and organize. When we reopen the project, we can recreate the environment in which we originally ran the code to reproduce our results.

The RStudio IDE has built-in support for projects. When we create a project in RStudio, it creates an `.Rproj` file that stores settings specific to that project. Once we have created a project, we can create folders that help us organize our workflow. For example, a project directory could look like this:

```
| anes_analysis/
  | anes_analysis.Rproj
  | README.md
  | codebooks
    | codebook2020.pdf
    | codebook2016.pdf
  | rawdata
    | anes2020_raw.csv
    | anes2016_raw.csv
  | scripts
    | data-prep.R
  | data
    | anes2020_clean.csv
    | anes2016_clean.csv
  | report
    | anes_report.Rmd
    | anes_report.html
    | anes_report.pdf
```

\index{here package|(}
In a project-based workflow, all paths are relative and, by default, relative to the folder the `.Rproj` file is located in. By using relative paths, others can open and run our files even if their directory configuration differs from ours (e.g., Mac and Windows users have different directory path structures). The {here} package enables easy file referencing, and we can start by using the `here::here()` function to build the path for loading or saving data [@R-here]. Below, we ask R to read the CSV file `anes_2020.csv` in the project directory's `data` folder: 

```{r}
#| label: reprex-project-file-example
#| eval: false
anes <- 
  read_csv(here::here("data", "anes2020_clean.csv"))
```

The combination of projects and the {here} package keep all associated files organized. This workflow makes it more likely that our analyses can be reproduced by us or our colleagues.
\index{here package|)} \index{R projects|)}

## Functions and packages

We may find that we are repeating ourselves in our script, and the chance of errors increases whenever we copy and paste our code. By creating a function, we can create a consistent set of commands that reduce the likelihood of mistakes. Functions also organize our code, improve the code readability, and allow others to execute the same commands. For example, in Chapter \@ref(c13-ncvs-vignette), we create a function to run sequences of `rename()`, `filter()`, `group_by()`, and summarize statements across different variables. Creating functions helps us avoid overlooking necessary steps.

A package is made up of a collection of functions. If we find ourselves sharing functions with others to replicate the same series of commands in a separate project, creating a package can be a useful tool for sharing the code along with data and documentation.

## Version control with Git

\index{Version control|(} \index{Git| see {Version control }}
Often, a survey analysis project produces a lot of code. Keeping track of the latest version can become challenging, as files evolve throughout a project. If a team of analysts is working on the same script, someone may use an outdated version, resulting in incorrect results or redundant work.

Version control systems like Git can help alleviate these pains. Git is a system that tracks changes in files. We can use Git to follow code evaluation and manage asynchronous work. With Git, it is easy to see any changes made in a script, revert changes, and resolve differences between code versions (called conflicts).

Services such as GitHub or GitLab provide hosting and sharing of files as well as version control with Git. For example, we can visit the [GitHub repository for this book](https://github.com/tidy-survey-r/tidy-survey-book) and see the files that build the book, when they were committed to the repository, and the history of modifications over time.

In addition to code scripts, platforms like GitHub can store data and documentation. They provide a way to maintain a history of data modifications through versioning and timestamps. By saving the data and documentation alongside the code, it becomes easier for others to refer to and access everything they need in one place.

Using version control in analysis projects makes collaboration and maintenance more manageable. To connect Git with R, we recommend referencing the book [Happy Git and GitHub for the useR](https://happygitwithr.com/) [@git-w-R].

\index{Version control|)}

## Package management with {renv}

\index{renv package|(} \index{Package management|see {renv package}} 
Ensuring reproducibility involves not only using version control of code but also managing the versions of packages. If two people run the same code but use different package versions, the results might differ because of changes to those packages. For example, this book currently uses a version of the {srvyr} package from GitHub and not from CRAN. This is because the version of {srvyr} on CRAN has some bugs (errors) that result in incorrect calculations. The version on GitHub has corrected these errors, so we have asked readers to install the GitHub version to obtain the same results.

One way to handle different package versions is with the {renv} package. This package allows researchers to set the versions for each used package and manage package dependencies. Specifically, {renv} creates isolated, project-specific environments that record the packages and their versions used in the code. When initiated by a new user, {renv} checks whether the installed packages are consistent with the recorded version for the project. If not, it installs the appropriate versions so that others can replicate the project's environment to rerun the code and obtain consistent results [@R-renv].

\index{renv package|)}

## R environments with Docker

\index{Environment management|(} \index{Docker|see {Environment management}}
Just as different versions of packages can introduce discrepancies or compatibility issues, the version of R can also prevent reproducibility. Tools such as Docker can help with this potential issue by creating isolated environments that define the version of R being used, along with other dependencies and configurations. The entire environment is bundled in a container. The container, defined by a Dockerfile, can be shared so that anybody, regardless of their local setup, can run the R code in the same environment.
\index{Environment management|)}

## Workflow management with {targets}

With complex studies involving multiple code files and dependencies, it is important to ensure each step is executed in the intended sequence. We can do this manually, e.g., by numbering files to indicate the order or providing detailed documentation on the order. Alternatively, we can automate the process so the code flows sequentially. Making sure that the code runs in the correct order helps ensure that the research is reproducible. Anyone should be able to pick up the set of scripts and get the same results by following the workflow.

The {targets} package is an increasingly popular workflow manager that documents, automates, and executes complex data workflows with multiple steps and dependencies. With this package, we first define the order of execution for our code, and then it consistently executes the code in that order each time it is run. One beneficial feature of {targets} is that if code changes later in the workflow, only the affected code and its downstream targets (i.e., the subsequent code files) are re-executed when we change a script. The {targets} package also provides interactive progress monitoring and reporting, allowing us to track the status and progress of our analysis pipeline [@targetslandau]. 

## Documentation with Quarto and R Markdown

\index{R Markdown|(} \index{Quarto|(}
Tools like Quarto and R Markdown aid in reproducibility by creating documents that weave together code, text, and results. We can present analysis results alongside the report's narrative, so there's no need to copy and paste code output into the final documentation. By eliminating manual steps, we can reduce the chances of errors in the final output.

Quarto and R Markdown documents also allow users to re-execute the underlying code when needed. Another analyst can see the steps we took, follow the scripts, and recreate the report. We can include details about our work in one place thanks to the combination of text and code, making our work transparent and easier to verify [@R-quarto; @rmarkdown2020man].

### Parameterization

Another useful feature of Quarto and R Markdown is the ability to reduce repetitive code by parameterizing the files. Parameters can control various aspects of the analysis, such as dates, geography, or other analysis variables. We can define and modify these parameters to explore different scenarios or inputs. For example, suppose we start by creating a document that provides survey analysis results for North Carolina but then later decide we want to look at another state. In that case, we can define a `state` parameter and rerun the same analysis for a state like Washington without having to edit the code throughout the document.

Parameters can be defined in the header or code chunks of our Quarto or R Markdown documents and easily modified and documented. By manually editing code throughout the script, we reduce errors that may occur and offer a flexible way for others to replicate the analysis and explore variations.

\index{R Markdown|)} \index{Quarto|)}

## Other tips for reproducibility

### Random number seeds  

Some tasks in survey analysis require randomness, such as imputation\index{Imputation}, model training, or creating random samples. By default, the random numbers generated by R change each time we rerun the code, making it difficult to reproduce the same results. By "setting the seed," we can control the randomness and ensure that the random numbers remain consistent whenever we rerun the code. Others can use the same seed value to reproduce our random numbers and achieve the same results.

In R, we can use the `set.seed()` function to control the randomness in our code. We set a seed value by providing an integer in the function argument. The following code chunk sets a seed using `999`, then runs a random number function (`runif()`) to get five random numbers from a uniform distribution.

```{r}
#| label: reprex-set-seed
set.seed(999)
runif(5)
```

Since the seed is set to `999`, running `runif(5)` multiple times always produces the same output. The choice of the seed number is up to the analyst. For example, this could be the date (`20240102`) or time of day (`1056`) when the analysis was first conducted, a phone number (`8675309`), or the first few numbers that come to mind (`369`). As long as the seed is set for a given analysis, the actual number is up to the analyst to decide. It is important to note that `set.seed()` should be used before random number generation. Run it once per program, and the seed is applied to the entire script. We recommend setting the seed at the beginning of a script, where libraries are loaded.

### Descriptive names and labels

\index{American National Election Studies (ANES)|(}
Using descriptive variable names or labeling data can also assist with reproducible research. For example, in the ANES data, the variable names in the raw data all start with `V20` and are a string of numbers. To make things easier to reproduce in this book, we opted to change the variable names to be more descriptive of what they contained (e.g., `Age`).\index{American National Election Studies (ANES)|)} This can also be done with the data values themselves. \index{Categorical data|(}\index{Factor|(}One way to accomplish this is by creating factors for categorical data, which can ensure that we know that a value of `1` really means `Female`, for example.\index{Factor|)} There are other ways of handling this, such as attaching labels to the data instead of recoding variables to be descriptive (see Chapter \@ref(c11-missing-data)). \index{Categorical data|)} As with random number seeds, the exact method is up to the analyst, but providing this information can help ensure our research is reproducible.

## Additional resources

We can promote accuracy and verification of results by making our analysis reproducible. There are various tools and guides available to help achieve reproducibility in analysis work, a few of which were described in this chapter. Here are additional resources to explore:

* [R for Data Science chapter on project-based workflows](https://r4ds.hadley.nz/workflow-scripts.html#projects)
* [Building reproducible analytical pipelines with R](https://raps-with-r.dev/)
* [Posit Solutions Site page on reproducible environments](https://solutions.posit.co/envs-pkgs/environments/)

<!--chapter:end:09-reproducible-data.Rmd-->

# (PART) Real-life data {-}

# Sample designs and replicate weights {#c10-sample-designs-replicate-weights}

```{r}
#| label: samp-styler
#| include: false
knitr::opts_chunk$set(tidy = 'styler')
```

::: {.prereqbox-header}
`r if (knitr:::is_html_output()) '### Prerequisites {- #prereq3}'`
:::

::: {.prereqbox data-latex="{Prerequisites}"}
For this chapter, load the following packages:
```{r}
#| label: samp-setup-libraries
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(survey)
library(srvyr)
library(srvyrexploR)
```

To help explain the different types of sample designs, this chapter uses the `api` and `scd` data that are included in the {survey} package  [@lumley2010complex]:
```{r}
#| label: samp-setup-surveydata
data(api)
data(scd)
```

This chapter uses data from the Residential Energy Consumption Survey (RECS), both 2015 and 2020, so we load the RECS data from the {srvyrexploR} package using their object names `recs_2015` and `recs_2020`, respectively [@R-srvyrexploR].
:::

## Introduction

The primary reason for using packages like {survey} and {srvyr} is to account for the sampling design or replicate weights into point and uncertainty estimates [@R-srvyr; @lumley2010complex]. By incorporating the sampling design or replicate weights, these estimates are appropriately calculated.

In this chapter, we introduce common sampling designs and common types of replicate weights, the mathematical methods for calculating estimates and standard errors for a given sampling design, and the R syntax to specify the sampling design or replicate weights. While we show the math behind the estimates, the functions in these packages handle the calculation. To deeply understand the math and the derivation, refer to @pennstate506, @sarndal2003model, @wolter2007introduction, or @fuller2011sampling (these are listed in order of increasing statistical rigorousness). 

\index{Survey analysis process|(}The general process for estimation in the {srvyr} package is to:

1. Create a `tbl_svy` object (a survey object) using: `as_survey_design()` or `as_survey_rep()`

2. Subset data (if needed) using `filter()` (subpopulations)

3. Specify domains of analysis using `group_by()` 

4. Within `summarize()`, specify variables to calculate, including means, totals, proportions, quantiles, and more

This chapter includes details on the first step: creating the survey object. Once this survey object is created, it can be used in the other steps (detailed in Chapters \@ref(c05-descriptive-analysis) through \@ref(c07-modeling)) to account for the complex survey design.\index{Survey analysis process|)}

## Common sampling designs

A sampling design is the method used to draw a sample. Both logistical and statistical elements are considered when developing a sampling design. When specifying a sampling design in R, we specify the levels of sampling along with the weights. The weight for each record is constructed so that the particular record represents that many units in the population. For example, in a survey of 6th-grade students in the United States, the weight associated with each responding student reflects how many 6th-grade students across the country that record represents. Generally, the weights represent the inverse of the probability of selection, such that the sum of the weights corresponds to the total population size, although some studies may have the sum of the weights equal to the number of respondent records.

Some common terminology across the designs are:

  - sample size, generally denoted as $n$, is the number of units selected to be sampled
  - population size, generally denoted as $N$, is the number of units in the population of interest
  - \index{Sampling frame|(}sampling frame, the list of units from which the sample is drawn (see Chapter \@ref(c02-overview-surveys) for more information)\index{Sampling frame|)}

### Simple random sample without replacement

\index{Simple random sampling|(}
The simple random sample (SRS) without replacement is a sampling design in which a fixed sample size is selected from a sampling frame, and every possible subsample has an equal probability of selection. Without replacement refers to the fact that once a sampling unit has been selected, it is removed from the sample frame and cannot be selected again.

  - Requirements: The sampling frame must include the entire population.
  - Advantages: SRS requires no information about the units apart from contact information.
  - Disadvantages: The sampling frame may not be available for the entire population.
  - Example: Randomly select students in a university from a roster provided by the registrar's office. 

#### The math {-}

The estimate for the population mean of variable $y$ is: 

$$\bar{y}=\frac{1}{n}\sum_{i=1}^n y_i$$
where $\bar{y}$ represents the sample mean, $n$ is the total number of respondents (or observations), and $y_i$ is each individual value of $y$.

The estimate of the standard error of the mean is:

$$se(\bar{y})=\sqrt{\frac{s^2}{n}\left( 1-\frac{n}{N} \right)}$$ where

$$s^2=\frac{1}{n-1}\sum_{i=1}^n\left(y_i-\bar{y}\right)^2.$$

\index{Finite population correction|(} \index{fpc|see {Finite population correction}}
and $N$ is the population size. This standard error estimate might look very similar to equations in other statistical applications except for the part on the right side of the equation: $1-\frac{n}{N}$. This is called the finite population correction (FPC) factor. If the size of the frame, $N$, is very large in comparison to the sample, the FPC is negligible, so it is often ignored. A common guideline is if the sample is less than 10% of the population, the FPC is negligible.

To estimate proportions, we define $x_i$ as the indicator if the outcome is observed. That is, $x_i=1$ if the outcome is observed, and $x_i=0$ if the outcome is not observed for respondent $i$. Then the estimated proportion from an SRS design is:

$$\hat{p}=\frac{1}{n}\sum_{i=1}^n x_i $$
and the estimated standard error of the proportion is:

$$se(\hat{p})=\sqrt{\frac{\hat{p}(1-\hat{p})}{n-1}\left(1-\frac{n}{N}\right)} $$

#### The syntax {-} 

\index{Functions in srvyr!as\_survey\_design|(}
If a sample was drawn through SRS and had no nonresponse or other weighting adjustments, we specify this design in R as:

```r
srs1_des <- dat %>%
  as_survey_design(fpc = fpcvar)
```

where `dat` is a tibble or data.frame with the survey data, and `fpcvar` is a variable in the data indicating the sampling frame's size (this variable has the same value for all cases in an SRS design). If the frame is very large, sometimes the frame size is not provided. In that case, the FPC is not needed, and we specify the design as:

```r
srs2_des <- dat %>%
  as_survey_design()
```

If some post-survey adjustments were implemented and the weights are not all equal, we specify the design as:

```r
srs3_des <- dat %>%
  as_survey_design(weights = wtvar,
                   fpc = fpcvar)
```

where `wtvar` is a variable in the data indicating the weight for each case. Again, the FPC can be omitted if it is unnecessary because the frame is large compared to the sample size.

#### Example {-} 

The {survey} package in R provides some example datasets that we use throughout this chapter. One of the example datasets we use is from the Academic Performance Index Program (APIP). The APIP program administered by the California Department of Education, and the {survey} package includes a population file (sample frame) of all schools with at least 100 students and several different samples pulled from that data using different sampling methods. For this first example, we use the `apisrs` dataset, which contains an SRS of 200 schools. For printing purposes, we create a new dataset called `apisrs_slim`, which sorts the data by the school district and school ID and subsets the data to only a few columns. The SRS sample data are illustrated below: 

```{r}
#| label: samp-des-apisrs-display
#| message: false
apisrs_slim <-
  apisrs %>%
  as_tibble() %>%
  arrange(dnum, snum) %>%
  select(cds, dnum, snum, dname, sname, fpc, pw)

apisrs_slim
```

Table \@ref(tab:apidata) provides details on all the variables in this dataset.

Table: (\#tab:apidata) Overview of Variables in APIP Data

Variable Name | Description
:--: | --------
`cds` | Unique identifier for each school
`dnum` | School district identifier within county
`snum` | School identifier within district
`dname` | District Name
`sname` | School Name
`fpc` | Finite population correction factor
`pw` | Weight

To create the `tbl_survey` object for the SRS data, we specify the design as:

```{r}
#| label: samp-des-apisrs-des
apisrs_des <- apisrs_slim %>%
  as_survey_design(weights = pw,
                   fpc = fpc)

apisrs_des
```

In the printed design object, the design is described as an "Independent Sampling design," which is another term for SRS. The ids are specified as `1`, which means there is no clustering (a topic described in Section \@ref(samp-cluster)), the FPC variable is indicated, and the weights are indicated. We can also look at the summary of the design object (`summary()`) and see the distribution of the probabilities (inverse of the weights) along with the population size and a list of the variables in the dataset. 

```{r}
#| label: samp-des-apisrs-summary
summary(apisrs_des)
```
\index{Finite population correction|)}

### Simple random sample with replacement

Similar to the SRS design, the simple random sample with replacement (SRSWR) design randomly selects the sample from the entire sampling frame.  However, while SRS removes sampled units before selecting again, the SRSWR instead replaces each sampled unit before drawing again, so units can be selected more than once.

  - Requirements: The sampling frame must include the entire population.
  - Advantages: SRSWR requires no information about the units apart from contact information.
  - Disadvantages: 
    - The sampling frame may not be available for the entire population. 
    - Units can be selected more than once, resulting in a smaller realized sample size because receiving duplicate information from a single respondent does not provide additional information. 
    - For small populations, SRSWR has larger standard errors than SRS designs.
  - Example: A professor puts all students' names on paper slips and selects them randomly to ask students questions, but the professor replaces the paper after calling on the student so they can be selected again at any time.

In general for surveys, using an SRS design (without replacement) is preferred as we do not want respondents to answer a survey more than once.

#### The math {-}

The estimate for the population mean of variable $y$ is:

$$\bar{y}=\frac{1}{n}\sum_{i=1}^n y_i$$

and the estimate of the standard error of mean is:

$$se(\bar{y})=\sqrt{\frac{s^2}{n}}$$ where

$$s^2=\frac{1}{n-1}\sum_{i=1}^n\left(y_i-\bar{y}\right)^2.$$
To calculate the estimated proportion, we define $x_i$ as the indicator that the outcome is observed (as we did with SRS):

$$\hat{p}=\frac{1}{n}\sum_{i=1}^n x_i $$
and the estimated standard error of the proportion is:

$$se(\hat{p})=\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} $$

#### The syntax {-} 

If we had a sample that was drawn through SRSWR and had no nonresponse or other weighting adjustments, in R, we specify this design as:  

```r
srswr1_des <- dat %>%
 as_survey_design()
```
\index{Finite population correction|(}
where `dat` is a tibble or data.frame containing our survey data.  This syntax is the same as an SRS design, except an FPC is not included. This is because when calculating a sample with replacement, the population pool to select from is no longer finite, so a correction is not needed. Therefore, with large populations where the FPC is negligible, the underlying formulas for SRS and SRSWR designs are the same.

\index{Finite population correction|)}
If some post-survey adjustments were implemented and the weights are not all equal, we specify the design as:

```r
srswr2_des <- dat %>%
 as_survey_design(weights = wtvar)
```

where `wtvar` is the variable for the weight of the data.

#### Example {-} 

The {survey} package does not include an example of SRSWR. To illustrate this design, we need to create an example.  We use the APIP population data provided by the {survey} package (`apipop`) and select a sample of 200 cases using the `slice_sample()` function from the tidyverse. One of the arguments in the `slice_sample()` function is `replace`.  If `replace=TRUE`, then we are conducting an SRSWR. We then calculate selection weights as the inverse of the probability of selection and call this new dataset `apisrswr`.

```{r}
#| label: samp-des-apisrs-wr-display
set.seed(409963)

apisrswr <- apipop %>%
  as_tibble() %>%
  slice_sample(n = 200, replace = TRUE) %>%
  select(cds, dnum, snum, dname, sname) %>%
  mutate(weight = nrow(apipop) / 200)

head(apisrswr)
```

Because this is an SRS design with replacement, there may be duplicates in the data. It is important to keep the duplicates in the data for proper estimation. For reference, we can view the duplicates in the example data we just created.

```{r}
#| label: samp-des-apisrs-wr-duplicates

apisrswr %>%
  group_by(cds) %>%
  filter(n() > 1) %>%
  arrange(cds)
```


We created a weight variable in this example data, which is the inverse of the probability of selection. We specify the sampling design for `apisrswr` as:

```{r}
#| label: samp-des-apisrswr-des
apisrswr_des <- apisrswr %>%
 as_survey_design(weights = weight)

apisrswr_des
summary(apisrswr_des)
```
\index{Finite population correction|(}
In the output above, the design object and the object summary are shown. Both note that the sampling is done "with replacement" because no FPC was specified. The probabilities, which are derived from the weights, are summarized in the summary function output.
\index{Finite population correction|)}\index{Simple random sampling|)}

### Stratified sampling \index{Stratified sampling|(} \index{Strata|(}

Stratified sampling occurs when a population is divided into mutually exclusive subpopulations (strata), and then samples are selected independently within each stratum.

  - Requirements: The sampling frame must include the information to divide the population into strata for every unit.
  - Advantages: 
    - This design ensures sample representation in all subpopulations. 
    - If the strata are correlated with survey outcomes, a stratified sample has smaller standard errors compared to a SRS sample of the same size.
    - This results in a more efficient design.
  - Disadvantages: Auxiliary data may not exist to divide the sampling frame into strata, or the data may be outdated.
  - Examples: 
    - Example 1: A population of North Carolina residents could be stratified into urban and rural areas, and then an SRS of residents from both rural and urban areas is selected independently. This ensures there are residents from both areas in the sample.
    - Example 2: Law enforcement agencies could be stratified into the three primary general-purpose categories in the U.S.: local police, sheriff's departments, and state police. An SRS of agencies from each of the three types is then selected independently to ensure all three types of agencies are represented.

#### The math {-} 

Let $\bar{y}_h$ be the sample mean for stratum $h$, $N_h$ be the population size of stratum $h$, $n_h$ be the sample size of stratum $h$, and $H$ be the total number of strata. Then, the estimate for the population mean under stratified SRS sampling is:

$$\bar{y}=\frac{1}{N}\sum_{h=1}^H N_h\bar{y}_h$$ 
and the estimate of the standard error of $\bar{y}$ is:

$$se(\bar{y})=\sqrt{\frac{1}{N^2} \sum_{h=1}^H N_h^2 \frac{s_h^2}{n_h}\left(1-\frac{n_h}{N_h}\right)} $$ 

where 
$$s_h^2=\frac{1}{n_h-1}\sum_{i=1}^{n_h}\left(y_{i,h}-\bar{y}_h\right)^2$$

For estimates of proportions, let $\hat{p}_h$ be the estimated proportion in stratum $h$. Then, the population proportion estimate is:

$$\hat{p}= \frac{1}{N}\sum_{h=1}^H N_h \hat{p}_h$$

The standard error of the proportion is:

$$se(\hat{p}) = \frac{1}{N} \sqrt{ \sum_{h=1}^H N_h^2 \frac{\hat{p}_h(1-\hat{p}_h)}{n_h-1} \left(1-\frac{n_h}{N_h}\right)}$$

#### The syntax {-} 

\index{Finite population correction|(}
In addition to the `fpc` and `weights` arguments discussed in the types above, stratified designs require the addition of the `strata` argument. For example, to specify a stratified SRS design in {srvyr} when using the FPC, that is, where the population sizes of the strata are not too large and are known, we specify the design as:

```r
stsrs1_des <- dat %>%
 as_survey_design(fpc = fpcvar, 
                  strata = stratavar)
```

where `fpcvar` is a variable on our data that indicates $N_h$ for each row, and `stratavar` is a variable indicating the stratum for each row. We can omit the FPC if it is not applicable. Additionally, we can indicate the weight variable if it is present where `wtvar` is a variable on our data with a numeric weight.

```r
stsrs2_des <- dat %>%
 as_survey_design(weights = wtvar, 
                  strata = stratavar)
```

#### Example {-} 

In the example APIP data, `apistrat` is a stratified random sample, stratified by school type (`stype`) with three levels: `E` for elementary school, `M` for middle school, and `H` for high school. As with the SRS example above, we sort and select specific variables for use in printing. The data are illustrated below, including a count of the number of cases per stratum:

```{r}
#| label: samp-des-apistrat-dis
apistrat_slim <-
 apistrat %>%
 as_tibble() %>%
 arrange(dnum, snum) %>%
 select(cds, dnum, snum, dname, sname, stype, fpc, pw)

apistrat_slim %>%
 count(stype, fpc)
```

The FPC is the same for each case within each stratum.  This output also shows that 100 elementary schools, 50 middle schools, and 50 high schools were sampled. It is often common for the number of units sampled from each strata to be different based on the goals of the project, or to mirror the size of each strata in the population. We specify the design as:

```{r}
#| label: samp-des-apistrat-des
apistrat_des <- apistrat_slim %>%
  as_survey_design(strata = stype,
                   weights = pw,
                   fpc = fpc)

apistrat_des
summary(apistrat_des)
```

When printing the object, it is specified as a "Stratified Independent Sampling design," also known as a stratified SRS, and the strata variable is included. Printing the summary, we see a distribution of probabilities, as we saw with SRS; but we also see the sample and population sizes by stratum. \index{Finite population correction|)} \index{Stratified sampling|)} \index{Strata|)}

### Clustered sampling {#samp-cluster}

\index{Clustered sampling|(} \index{Primary sampling unit|(}
Clustered sampling occurs when a population is divided into mutually exclusive subgroups called clusters or primary sampling units (PSUs). A random selection of PSUs is sampled, and then another level of sampling is done within these clusters. There can be multiple levels of this selection. \index{Data collection|(}Clustered sampling is often used when a list of the entire population is not available or data collection involves interviewers needing direct contact with respondents.

  - Requirements: There must be a way to divide the population into clusters. Clusters are commonly structural, such as institutions (e.g., schools, prisons) or geography (e.g., states, counties). 
  - Advantages: 
    - Clustered sampling is advantageous when data collection is done in person, so interviewers are sent to specific sampled areas rather than completely at random across a country. \index{Data collection|)}
    - With clustered sampling, a list of the entire population is not necessary. For example, if sampling students, we do not need a list of all students, but only a list of all schools. Once the schools are sampled, lists of students can be obtained within the sampled schools.
  - Disadvantages: Compared to a simple random sample for the same sample size, clustered samples generally have larger standard errors of estimates.
  - Examples: 
    - Example 1: Consider a study needing a sample of 6th-grade students in the United States. No list likely exists of all these students. However, it is more likely to obtain a list of schools that enroll 6th graders, so a study design could select a random sample of schools that enroll 6th graders. The selected schools can then provide a list of students to do a second stage of sampling where 6th-grade students are randomly sampled within each of the sampled schools. This is a one-stage sample design (the one representing the number of clusters) and is the type of design we discuss in the formulas below.
    - Example 2: Consider a study sending interviewers to households for a survey. This is a more complicated example that requires two levels of clustering (two-stage sample design) to efficiently use interviewers in geographic clusters. First, in the U.S., counties could be selected as the PSU and then census block groups within counties could be selected as the secondary sampling unit (SSU). Households could then be randomly sampled within the block groups. This type of design is popular for in-person surveys, as it reduces the travel necessary for interviewers.

#### The math {-}

Consider a survey where $a$ clusters are sampled from a population of $A$ clusters via SRS. Within each sampled cluster, $i$, there are $B_i$ units in the population, and $b_i$ units are sampled via SRS. Let $\bar{y}_{i}$ be the sample mean of cluster $i$. Then, a ratio estimator of the population mean is:

$$\bar{y}=\frac{\sum_{i=1}^a B_i \bar{y}_{i}}{ \sum_{i=1}^a B_i}$$
Note this is a consistent but biased estimator. Often the population size is not known, so this is a method to estimate a mean without knowing the population size. The estimated standard error of the mean is:

$$se(\bar{y})= \frac{1}{\hat{N}}\sqrt{\left(1-\frac{a}{A}\right)\frac{s_a^2}{a} + \frac{A}{a} \sum_{i=1}^a \left(1-\frac{b_i}{B_i}\right) \frac{s_i^2}{b_i} }$$
where $\hat{N}$ is the estimated population size, $s_a^2$ is the between-cluster variance, and $s_i^2$ is the within-cluster variance.

The formula for the between-cluster variance ($s_a^2$) is:

$$s_a^2=\frac{1}{a-1}\sum_{i=1}^a \left( \hat{y}_i - \frac{\sum_{i=1}^a \hat{y}_{i} }{a}\right)^2$$
where  $\hat{y}_i =B_i\bar{y_i}$.

The formula for the within-cluster variance ($s_i^2$) is:

$$s_i^2=\frac{1}{a(b_i-1)} \sum_{j=1}^{b_i} \left(y_{ij}-\bar{y}_i\right)^2$$
where $y_{ij}$ is the outcome for sampled unit $j$ within cluster $i$.

#### The syntax {-} 

\index{Finite population correction|(}
Clustered sampling designs require the addition of the `ids` argument, which specifies the cluster level variable(s). To specify a two-stage clustered design without replacement, we specify the design as:

```r
clus2_des <- dat %>%
 as_survey_design(weights = wtvar, 
                  ids = c(PSU, SSU), 
                  fpc = c(A, B))
```

where `PSU` and `SSU` are the variables indicating the PSU and SSU identifiers, and `A` and `B` are the variables indicating the population sizes for each level (i.e., `A` is the number of clusters, and `B` is the number of units within each cluster). Note that `A` is the same for all records, and `B` is the same for all records within the same cluster.

If clusters were sampled with replacement or from a very large population, the FPC is unnecessary. Additionally, only the first stage of selection is necessary regardless of whether the units were selected with replacement at any stage. The subsequent stages of selection are ignored in computation as their contribution to the variance is overpowered by the first stage (see @sarndal2003model or @wolter2007introduction for a more in-depth discussion). Therefore, the two design objects specified below yield the same estimates in the end:
\index{Finite population correction|)}

```r
clus2ex1_des <- dat %>%
 as_survey_design(weights = wtvar, 
                  ids = c(PSU, SSU))

clus2ex2_des <- dat %>%
 as_survey_design(weights = wtvar, 
                  ids = PSU)

```

\index{Strata|(} \index{Primary sampling unit|(}
Note that there is one additional argument that is sometimes necessary, which is `nest = TRUE`. This option relabels cluster IDs to enforce nesting within strata. Sometimes, as an example, there may be a cluster `1` within each stratum, but cluster `1` in stratum `1` is a different cluster than cluster `1` in stratum `2`. These are actually different clusters. This option indicates that repeated numbering does not mean it is the same cluster. If this option is not used and there are repeated cluster IDs across different strata, an error is generated.
\index{Strata|)} \index{Primary sampling unit|)}

#### Example {-} 

\index{Finite population correction|(}
The `survey` package includes a two-stage cluster sample data, `apiclus2`, in which school districts were sampled, and then a random sample of five schools was selected within each district. For districts with fewer than five schools, all schools were sampled. School districts are identified by `dnum`, and schools are identified by `snum`. The variable `fpc1` indicates how many districts there are in California (the total number of PSUs or `A`), and `fpc2` indicates how many schools were in a given district with at least 100 students (the total number of SSUs or `B`). The data include a row for each school. In the data printed below, there are 757 school districts, as indicated by `fpc1`, and there are nine schools in District 731, one school in District 742, two schools in District 768, and so on as indicated by `fpc2`. For illustration purposes, the object `apiclus2_slim` has been created from `apiclus2`, which subsets the data to only the necessary columns and sorts the data.

```{r}
#| label: samp-des-api2clus-dis
apiclus2_slim <-
  apiclus2 %>%
  as_tibble() %>%
  arrange(desc(dnum), snum) %>%
  select(cds, dnum, snum, fpc1, fpc2, pw)

apiclus2_slim
```

To specify this design in R, we use the following:

```{r}
#| label: samp-des-api2clus-des
apiclus2_des <- apiclus2_slim %>%
  as_survey_design(
    ids = c(dnum, snum),
    fpc = c(fpc1, fpc2),
    weights = pw
  )

apiclus2_des
summary(apiclus2_des)
```

The design objects are described as "2 - level Cluster Sampling design," and include the ids (cluster), FPC, and weight variables. The summary notes that the sample includes 40 first-level clusters (PSUs), which are school districts, and 126 second-level clusters (SSUs), which are schools. Additionally, the summary includes a numeric summary of the probabilities of selection and the population size (number of PSUs) as 757.
\index{Functions in srvyr!as\_survey\_design|)} \index{Finite population correction|)} \index{Clustered sampling|)} \index{Primary sampling unit|)}

## Combining sampling methods {#samp-combo} 
\index{Finite population correction|)} \index{Stratified sampling|(} \index{Clustered sampling|(} \index{Primary sampling unit|(} \index{Strata|(}

SRS, stratified, and clustered designs are the backbone of sampling designs, and the features are often combined in one design. Additionally, rather than using SRS for selection, other sampling mechanisms are commonly used, such as probability proportional to size (PPS), systematic sampling, or selection with unequal probabilities, which are briefly described here. In PPS sampling, a size measure is constructed for each unit (e.g., the population of the PSU or the number of occupied housing units), and units with larger size measures are more likely to be sampled. Systematic sampling is commonly used to ensure representation across a population. Units are sorted by a feature, and then every $k$ units is selected from a random start point so the sample is spread across the population. \index{Burden|(}In addition to PPS, other unequal probabilities of selection may be used. For example, in a study of establishments (e.g., businesses or public institutions) that conducts a survey every year, an establishment that recently participated (e.g., participated last year) may have a reduced chance of selection in a subsequent round to reduce the burden on the establishment.\index{Burden|)} To learn more about sampling designs, refer to @valliant2013practical, @cox2011business, @cochran1977sampling, and @deming1991sample.

A common method of sampling is to stratify PSUs, select PSUs within the stratum using PPS selection, and then select units within the PSUs either with SRS or PPS. Reading survey documentation is an important first step in survey analysis to understand the design of the survey we are using and variables necessary to specify the design. Good documentation highlights the variables necessary to specify the design. This is often found in the user guide, methodology report, analysis guide, or technical documentation (see Chapter \@ref(c03-survey-data-documentation) for more details).
\index{Finite population correction|)} \index{Stratified sampling|)} \index{Clustered sampling|)} \index{Primary sampling unit|)} \index{Strata|(}

### Example {-}

\index{Functions in srvyr!as\_survey\_design|(} \index{Finite population correction|)} \index{Stratified sampling|(} \index{Clustered sampling|(} \index{Primary sampling unit|(} \index{Strata|(}
For example, the [2017-2019 National Survey of Family Growth](https://www.cdc.gov/nchs/data/nsfg/NSFG-2017-2019-Sample-Design-Documentation-508.pdf) had a stratified multi-stage area probability sample: 

  1. In the first stage, PSUs are counties or collections of counties and are stratified by Census region/division, size (population), and MSA status. Within each stratum, PSUs were selected via PPS. 
  2. In the second stage, neighborhoods were selected within the sampled PSUs using PPS selection. 
  3. In the third stage, housing units were selected within the sampled neighborhoods. 
  4. In the fourth stage, a person was randomly chosen among eligible persons within the selected housing units using unequal probabilities based on the person's age and sex. 
  
The public use file does not include all these levels of selection and instead has pseudo-strata and pseudo-clusters, which are the variables used in R to specify the design. As specified on page 4 of the documentation, the stratum variable is `SEST`, the cluster variable is `SECU`, and the weight variable is `WGT2017_2019`. Thus, to specify this design in R, we use the following syntax:

```r
nsfg_des <- nsfgdata %>%
  as_survey_design(ids = SECU,
                   strata = SEST,
                   weights = WGT2017_2019)
```
\index{Functions in srvyr!as\_survey\_design|)} \index{Finite population correction|)} \index{Stratified sampling|)} \index{Clustered sampling|)} \index{Primary sampling unit|)} \index{Strata|)}

## Replicate weights

\index{Replicate weights|(}
Replicate weights are often included on analysis files instead of, or in addition to, the design variables (strata and PSUs). Replicate weights are used as another method to estimate variability.  Often, researchers choose to use replicate weights to avoid publishing design variables (strata or clustering variables) as a measure to reduce the risk of disclosure. There are several types of replicate weights, including balanced repeated replication (BRR), Fay's BRR, jackknife, and bootstrap methods. An overview of the process for using replicate weights is as follows:

1. Divide the sample into subsample replicates that mirror the design of the sample
2. Calculate weights for each replicate using the same procedures for the full-sample weight (i.e., nonresponse and post-stratification)
3. Calculate estimates for each replicate using the same method as the full-sample estimate
4. Calculate the estimated variance, which is proportional to the variance of the replicate estimates

The different types of replicate weights largely differ between step 1 (how the sample is divided into subsamples) and step 4 (which multiplication factors, scales, are used to multiply the variance). The general format for the standard error is:

$$ \sqrt{\alpha \sum_{r=1}^R \alpha_r (\hat{\theta}_r - \hat{\theta})^2 }$$

where $R$ is the number of replicates, $\alpha$ is a constant that depends on the replication method, $\alpha_r$ is a factor associated with each replicate, $\hat{\theta}$ is the weighted estimate based on the full sample, and $\hat{\theta}_r$ is the weighted estimate of $\theta$ based on the $r^{\text{th}}$ replicate.

To create the design object for surveys with replicate weights, we use `as_survey_rep()` instead of `as_survey_design()`, which we use for the common sampling designs in the sections above.

### Balanced Repeated Replication method

\index{Replicate weights!Balanced repeated replication (BRR)|(} \index{Primary sampling unit|(} \index{Strata|(}
The balanced repeated replication (BRR) method requires a stratified sample design with two PSUs in each stratum. Each replicate is constructed by deleting one PSU per stratum using a Hadamard matrix. For the PSU that is included, the weight is generally multiplied by two but may have other adjustments, such as post-stratification. A Hadamard matrix is a special square matrix with entries of +1 or --1 with mutually orthogonal rows. Hadamard matrices must have one row, two rows, or a multiple of four rows. The size of the Hadamard matrix is determined by the first multiple of 4 greater than or equal to the number of strata.  For example, if a survey had seven strata, the Hadamard matrix would be an $8\times8$ matrix. Additionally, a survey with eight strata would also have an $8\times8$ Hadamard matrix. The columns in the matrix specify the strata, and the rows specify the replicate. In each replicate (row), a +1 means to use the first PSU, and a --1 means to use the second PSU in the estimate. For example, here is a $4\times4$ Hadamard matrix: 

$$ \begin{array}{rrrr} +1 &+1 &+1 &+1\\ +1&-1&+1&-1\\ +1&+1&-1&-1\\ +1 &-1&-1&+1 \end{array} $$
In the first replicate (row), all the values are +1; so in each stratum, the first PSU would be used in the estimate. In the second replicate, the first PSU would be used in strata 1 and 3, while the second PSU would be used in strata 2 and 4. In the third replicate, the first PSU would be used in strata 1 and 2, while the second PSU would be used in strata 3 and 4. Finally, in the fourth replicate, the first PSU would be used in strata 1 and 4, while the second PSU would be used in strata 2 and 3. For more information about Hadamard matrices, see @wolter2007introduction. Note that supplied BRR weights from a data provider already incorporate this adjustment, and the {survey} package generates the Hadamard matrix, if necessary, for calculating BRR weights; so an analyst does not need to create or provide the matrix.
\index{Primary sampling unit|)} \index{Strata|)}

#### The math {-}

A weighted estimate for the full sample is calculated as $\hat{\theta}$, and then a weighted estimate for each replicate is calculated as $\hat{\theta}_r$ for $R$ replicates. Using the generic notation above, $\alpha=\frac{1}{R}$ and $\alpha_r=1$ for each $r$. The standard error of the estimate is calculated as follows:

$$se(\hat{\theta})=\sqrt{\frac{1}{R} \sum_{r=1}^R \left( \hat{\theta}_r-\hat{\theta}\right)^2}$$

Specifying replicate weights in R requires specifying the type of replicate weights, the main weight variable, the replicate weight variables, and other options. One of the key options is for the mean squared error (MSE). If `mse=TRUE`, variances are computed around the point estimate $(\hat{\theta})$; whereas if `mse=FALSE`, variances are computed around the mean of the replicates $(\bar{\theta})$ instead, which looks like this: 

$$se(\hat{\theta})=\sqrt{\frac{1}{R} \sum_{r=1}^R \left( \hat{\theta}_r-\bar{\theta}\right)^2}$$ where $$\bar{\theta}=\frac{1}{R}\sum_{r=1}^R \hat{\theta}_r$$

The default option for `mse` is to use the global option of "survey.replicates.mse," which is set to `FALSE` initially unless a user changes it. To determine if `mse` should be set to `TRUE` or `FALSE`, read the survey documentation.  If there is no indication in the survey documentation for BRR, we recommend setting `mse` to `TRUE`, as this is the default in other software (e.g., SAS, SUDAAN).

#### The syntax {-} 

\index{Functions in srvyr!as\_survey\_rep|(} \index{American Community Survey (ACS)|(}
Replicate weights generally come in groups and are sequentially numbered, such as PWGTP1, PWGTP2, ..., PWGTP80 for the person weights in the American Community Survey (ACS) [@acs-pums-2021] or BRRWT1, BRRWT2, ..., BRRWT96 in the 2015 Residential Energy Consumption Survey (RECS) [@recs-2015-micro]. This makes it easy to use some of the [tidy selection](https://dplyr.tidyverse.org/reference/dplyr_tidy_select.html) functions in R. 
\index{American Community Survey (ACS)|)}

To specify a BRR design, we need to specify the weight variable (`weights`), the replicate weight variables (`repweights`), the type of replicate weights as BRR (`type = BRR`), and whether the mean squared error should be used (`mse = TRUE`) or not (`mse = FALSE`).  For example, if a dataset had WT0 for the main weight and had 20 BRR weights indicated WT1, WT2, ..., WT20, we can use the following syntax (both are equivalent):

```r
brr_des <- dat %>%
  as_survey_rep(weights = WT0,
                repweights = all_of(str_c("WT", 1:20)), 
                type = "BRR",
                mse = TRUE)

brr_des <- dat %>%
  as_survey_rep(weights = WT0,
                repweights = num_range("WT", 1:20),
                type = "BRR",
                mse = TRUE)
```

If a dataset had WT for the main weight and had 20 BRR weights indicated REPWT1, REPWT2, ..., REPWT20, we can use the following syntax (both are equivalent):

```r
brr_des <- dat %>%
  as_survey_rep(weights = WT,
                repweights = all_of(str_c("REPWT", 1:20)),
                type = "BRR",
                mse = TRUE)

brr_des <- dat %>%
  as_survey_rep(weights = WT,
                repweights = starts_with("REPWT"),
                type = "BRR",
                mse = TRUE)
```

If the replicate weight variables are in the file consecutively, we can also use the following syntax:

```r
brr_des <- dat %>%
  as_survey_rep(weights = WT,
                repweights = REPWT1:REPWT20,
                type = "BRR",
                mse = TRUE)
```

Typically, each replicate weight sums to a value similar to the main weight, as both the replicate weights and the main weight are supposed to provide population estimates. Rarely, an alternative method is used where the replicate weights have values of 0 or 2 in the case of BRR weights. This would be indicated in the documentation (see Chapter \@ref(c03-survey-data-documentation) for more information on reading documentation). In this case, the replicate weights are not combined, and the option `combined_weights = FALSE` should be indicated, as the default value for this argument is `TRUE`. This specific syntax is shown below:

```r
brr_des <- dat %>%
  as_survey_rep(weights = WT,
                repweights = starts_with("REPWT"),
                type = "BRR",
                combined_weights = FALSE,
                mse = TRUE)
```

#### Example {-} 

\index{Primary sampling unit|(}
The {survey} package includes a data example from section 12.2 of @levy2013sampling. In this fictional data, two out of five ambulance stations were sampled from each of three emergency service areas (ESAs); thus BRR weights are appropriate with two PSUs (stations) sampled in each stratum (ESA). In the code below, we create BRR weights as was done by @levy2013sampling.

```{r}
#| label: samp-des-brr-display
scdbrr <- scd %>%
  as_tibble() %>%
  mutate(wt = 5 / 2,
         rep1 = 2 * c(1, 0, 1, 0, 1, 0),
         rep2 = 2 * c(1, 0, 0, 1, 0, 1),
         rep3 = 2 * c(0, 1, 1, 0, 0, 1),
         rep4 = 2 * c(0, 1, 0, 1, 1, 0))

scdbrr
```

To specify the BRR weights, we use the following syntax:

```{r}
#| label: samp-scdbrr-des
scdbrr_des <- scdbrr %>%
  as_survey_rep(type = "BRR",
                repweights = starts_with("rep"),
                combined_weights = FALSE,  
                weight = wt)

scdbrr_des

summary(scdbrr_des)
```

Note that `combined_weights` was specified as `FALSE` because these weights are simply specified as 0 and 2 and do not incorporate the overall weight. When printing the object, the type of replication is noted as Balanced Repeated Replicates, and the replicate weights and the weight variable are specified. Additionally, the summary lists the variables included in the data and design object.
\index{Primary sampling unit|)}

### Fay's BRR method

Fay's BRR method for replicate weights is similar to the BRR method in that it uses a Hadamard matrix to construct replicate weights. However, rather than deleting PSUs for each replicate, with Fay's BRR, half of the PSUs have a replicate weight, which is the main weight multiplied by $\rho$, and the other half have the main weight multiplied by $(2-\rho)$, where $0 \le \rho < 1$. Note that when $\rho=0$, this is equivalent to the standard BRR weights, and as $\rho$ becomes closer to 1, this method is more similar to jackknife discussed in Section \@ref(samp-jackknife). To obtain the value of $\rho$, it is necessary to read the survey documentation (see Chapter \@ref(c03-survey-data-documentation)).

#### The math {-}

The standard error estimate for $\hat{\theta}$ is slightly different than the BRR, due to the addition of the multiplier of $\rho$. Using the generic notation above, $\alpha=\frac{1}{R \left(1-\rho\right)^2}$ and $\alpha_r=1 \text{ for all } r$. The standard error is calculated as:

$$se(\hat{\theta})=\sqrt{\frac{1}{R (1-\rho)^2} \sum_{r=1}^R \left( \hat{\theta}_r-\hat{\theta}\right)^2}$$

#### The syntax {-} 

The syntax is very similar for BRR and Fay's BRR. To specify a Fay's BRR design, we need to specify the weight variable (`weights`), the replicate weight variables (`repweights`), the type of replicate weights as Fay's BRR (`type = Fay`), whether the mean squared error should be used (`mse = TRUE`) or not (`mse = FALSE`), and Fay's multiplier (`rho`). For example, if a dataset had WT0 for the main weight and had 20 BRR weights indicated as WT1, WT2, ..., WT20, and Fay's multiplier is 0.3, we use the following syntax:

```r
fay_des <- dat %>%
  as_survey_rep(weights = WT0,
                repweights = num_range("WT", 1:20),
                type = "Fay",
                mse = TRUE,
                rho = 0.3)
```

#### Example {-} 

\index{Residential Energy Consumption Survey (RECS)|(}
The 2015 RECS [@recs-2015-micro] uses Fay's BRR weights with the final weight as NWEIGHT and replicate weights as BRRWT1 - BRRWT96, and the documentation specifies a Fay's multiplier of 0.5. On the file, DOEID is a unique identifier for each respondent, TOTALDOL is the total energy cost, TOTSQFT_EN is the total square footage of the residence, and REGOINC is the census region. We use the 2015 RECS data from the {srvyrexploR} package that provides data for this book (see the Prerequisites box at the beginning of this chapter).  To specify the design for the `recs_2015` data, we use the following syntax:

```{r}
#| label: samp-des-recs-des
#| eval: TRUE
recs_2015_des <- recs_2015 %>%
  as_survey_rep(weights = NWEIGHT,
                repweights = BRRWT1:BRRWT96,
                type = "Fay",
                rho = 0.5,
                mse = TRUE,
                variables = c(DOEID, TOTALDOL, TOTSQFT_EN, REGIONC))

recs_2015_des 

summary(recs_2015_des) 
```


In specifying the design, the `variables` option was also used to include which variables might be used in analyses. This is optional but can make our object smaller and easier to work with. When printing the design object or looking at the summary, the replicate weight type is re-iterated as `Fay's variance method (rho= 0.5) with 96 replicates and MSE variances`, and the variables are included. No weight or probability summary is included in this output, as we have seen in some other design objects.\index{Replicate weights!Balanced repeated replication (BRR)|)} \index{Residential Energy Consumption Survey (RECS)|)}

### Jackknife method {#samp-jackknife}

\index{Replicate weights!Jackknife|(} \index{Primary sampling unit|(}
There are three jackknife estimators implemented in {srvyr}: jackknife 1 (JK1), jackknife n (JKn), and jackknife 2 (JK2). The JK1 method can be used for unstratified designs, and replicates are created by removing one PSU at a time so the number of replicates is the same as the number of PSUs. If there is no clustering, then the PSU is the ultimate sampling unit (e.g., students). 

\index{Strata|(}
The JKn method is used for stratified designs and requires two or more PSUs per stratum. In this case, each replicate is created by deleting one PSU from a single stratum, so the number of replicates is the number of total PSUs across all strata. The JK2 method is a special case of JKn when there are exactly 2 PSUs sampled per stratum. For variance estimation, we also need to specify the scaling constants. 
\index{Strata|)}

#### The math {-}

Using the generic notation above, $\alpha=\frac{R-1}{R}$ and $\alpha_r=1 \text{ for all } r$. For the JK1 method, the standard error estimate for $\hat{\theta}$ is calculated as:

$$se(\hat{\theta})=\sqrt{\frac{R-1}{R} \sum_{r=1}^R \left( \hat{\theta}_r-\hat{\theta}\right)^2}$$
The JKn method is a bit more complex, but the coefficients are generally provided with restricted and public-use files. For each replicate, one stratum has a PSU removed, and the weights are adjusted by $n_h/(n_h-1)$ where $n_h$ is the number of PSUs in stratum $h$. The coefficients in other strata are set to 1. Denote the coefficient that results from this process for replicate $r$ as $\alpha_r$, then the standard error estimate for $\hat{\theta}$ is calculated as:

$$se(\hat{\theta})=\sqrt{\sum_{r=1}^R \alpha_r \left( \hat{\theta}_r-\hat{\theta}\right)^2}$$

\index{Primary sampling unit|)}

#### The syntax {-}

To specify the jackknife method, we use the survey documentation to understand the type of jackknife (1, n, or 2) and the multiplier. In the syntax, we need to specify the weight variable (`weights`), the replicate weight variables (`repweights`), the type of replicate weights as jackknife 1 (`type = "JK1"`), n (`type = "JKN"`), or 2 (`type = "JK2"`), whether the mean squared error should be used (`mse = TRUE`) or not (`mse = FALSE`), and the multiplier (`scale`). For example, if the survey is a jackknife 1 method with a multiplier of $\alpha_r=(R-1)/R=19/20=0.95$, the dataset has WT0 for the main weight and 20 replicate weights indicated as WT1, WT2, ..., WT20, we use the following syntax:

```r
jk1_des <- dat %>%
  as_survey_rep(
    weights = WT0,
    repweights = num_range("WT", 1:20),
    type = "JK1",
    mse = TRUE,
    scale = 0.95
  )
```

For a jackknife n method, we need to specify the multiplier for all replicates. In this case, we use the `rscales` argument to specify each one.  The documentation provides details on what the multipliers ($\alpha_r$) are, and they may be the same for all replicates.  For example, consider a case where $\alpha_r=0.1$ for all replicates, and the dataset had WT0 for the main weight and had 20 replicate weights indicated as WT1, WT2, ..., WT20. We specify the type as `type = "JKN"`, and the multiplier as `rscales=rep(0.1,20)`: 

```r
jkn_des <- dat %>%
  as_survey_rep(
    weights = WT0,
    repweights = num_range("WT", 1:20),
    type = "JKN",
    mse = TRUE,
    rscales = rep(0.1, 20)
  )
```

#### Example {-}

\index{Residential Energy Consumption Survey (RECS)|(}
The 2020 RECS [@recs-2020-micro] uses jackknife weights with the final weight as NWEIGHT and replicate weights as NWEIGHT1 - NWEIGHT60 with a scale of $(R-1)/R=59/60$. On the file, DOEID is a unique identifier for each respondent, TOTALDOL is the total cost of energy, TOTSQFT_EN is the total square footage of the residence, and REGOINC is the census region.  We use the 2020 RECS data from the {srvyrexploR} package that provides data for this book (see the Prerequisites box at the beginning of this chapter). 

To specify this design, we use the following syntax:

```{r}
#| label: samp-des-recs2020-des

recs_des <- recs_2020 %>%
  as_survey_rep(
    weights = NWEIGHT,
    repweights = NWEIGHT1:NWEIGHT60,
    type = "JK1",
    scale = 59/60,
    mse = TRUE,
    variables = c(DOEID, TOTALDOL, TOTSQFT_EN, REGIONC)
  )

recs_des

summary(recs_des)
```

```{r}
#| label: samp-des-recs2020-des-save
#| echo: false

recs_des <- recs_2020 %>%
  as_survey_rep(
    weights = NWEIGHT,
    repweights = NWEIGHT1:NWEIGHT60,
    type = "JK1",
    scale = 59/60,
    mse = TRUE
  )
```


When printing the design object or looking at the summary, the replicate weight type is reiterated as `Unstratified cluster jacknife (JK1) with 60 replicates and MSE variances`, and the variables are included. No weight or probability summary is included. \index{Replicate weights!Jackknife|)} \index{Residential Energy Consumption Survey (RECS)|)}

### Bootstrap method

\index{Replicate weights!Bootstrap|(} \index{Primary sampling unit|(}
In bootstrap resampling, replicates are created by selecting random samples of the PSUs with replacement (SRSWR). If there are $A$ PSUs in the sample, then each replicate is created by selecting a random sample of $A$ PSUs with replacement. Each replicate is created independently, and the weights for each replicate are adjusted to reflect the population, generally using the same method as how the analysis weight was adjusted.  
\index{Primary sampling unit|)}

#### The math {-}

A weighted estimate for the full sample is calculated as $\hat{\theta}$, and then a weighted estimate for each replicate is calculated as $\hat{\theta}_r$ for $R$ replicates. Then the standard error of the estimate is calculated as follows:

$$se(\hat{\theta})=\sqrt{\alpha \sum_{r=1}^R \left( \hat{\theta}_r-\hat{\theta}\right)^2}$$ 

where $\alpha$ is the scaling constant. Note that the scaling constant ($\alpha$) is provided in the survey documentation, as there are many types of bootstrap methods that generate custom scaling constants.


#### The syntax {-}

To specify a bootstrap method, we need to specify the weight variable (`weights`), the replicate weight variables (`repweights`), the type of replicate weights as bootstrap (`type = "bootstrap"`), whether the mean squared error should be used (`mse = TRUE`) or not (`mse = FALSE`), and the multiplier (`scale`). For example, if a dataset had WT0 for the main weight, 20 bootstrap weights indicated WT1, WT2, ..., WT20, and a multiplier of $\alpha=.02$, we use the following syntax:

```r
bs_des <- dat %>%
  as_survey_rep(
    weights = WT0,
    repweights = num_range("WT", 1:20),
    type = "bootstrap",
    mse = TRUE,
    scale = .02
  )
```
\index{Functions in srvyr!as\_survey\_rep|)}

#### Example {-}

\index{Primary sampling unit|(} 
Returning to the APIP example, we are going to create a dataset with bootstrap weights to use as an example. In this example, we construct a one-cluster design with 50 replicate weights^[We provide the code here to replicate this example but are not focusing on the creation of the weights, as that is outside the scope of this book.  We recommend referencing @wolter2007introduction for more information on creating bootstrap weights.].

```{r}
#| label: samp-des-genbs
apiclus1_slim <-
  apiclus1 %>%
  as_tibble() %>%
  arrange(dnum) %>%
  select(cds, dnum, fpc, pw)

set.seed(662152)
apibw <-
  bootweights(psu = apiclus1_slim$dnum,
              strata = rep(1, nrow(apiclus1_slim)),
              fpc = apiclus1_slim$fpc,
              replicates = 50)

bwmata <-
  apibw$repweights$weights[apibw$repweights$index,] * apiclus1_slim$pw

apiclus1_slim <- bwmata %>%
  as.data.frame() %>%
  set_names(str_c("pw", 1:50)) %>%
  cbind(apiclus1_slim) %>%
  as_tibble() %>%
  select(cds, dnum, fpc, pw, everything())

apiclus1_slim
```

The output of `apiclus1_slim` includes the same variables we have seen in other APIP examples (see Table \@ref(tab:apidata)), but now it additionally includes bootstrap weights `pw1`, ..., `pw50`.  When creating the survey design object, we use the bootstrap weights as the replicate weights. Additionally, with replicate weights we need to include the scale ($\alpha$).  For this example, we created:

$$\alpha=\frac{A}{(A-1)(R-1)}=\frac{15}{(15-1)*(50-1)}=0.02186589$$

where $A$ is the average number of PSUs per stratum, and $R$ is the number of replicates. There is only 1 stratum and the number of clusters/PSUs is 15 so $A=15$. Using this information, we specify the design object as:
\index{Primary sampling unit|)}

```{r}
#| label: samp-des-bsexamp
api1_bs_des <- apiclus1_slim %>%
  as_survey_rep(weights = pw,
                repweights = pw1:pw50,
                type = "bootstrap",
                scale = 0.02186589,
                mse = TRUE)

api1_bs_des 

summary(api1_bs_des) 
```

As with other replicate design objects, when printing the object or looking at the summary, the replicate weights are provided along with the data variables. \index{Replicate weights|)} \index{Replicate weights!Bootstrap|)}


## Exercises

For this chapter, the exercises entail reading public documentation to determine how to specify the survey design. While reading the documentation, be on the lookout for description of the weights and the survey design variables or replicate weights.

1. The National Health Interview Survey (NHIS) is an annual household survey conducted by the National Center for Health Statistics (NCHS). The NHIS includes a wide variety of health topics for adults including health status and conditions, functioning and disability, health care access and health service utilization, health-related behaviors, health promotion, mental health, barriers to receiving care, and community engagement. Like many national in-person surveys, the sampling design is a stratified clustered design with details included in the Survey Description [@nhis-svy-des]. The Survey Description provides information on setting up syntax in SUDAAN, Stata, SPSS, SAS, and R ({survey} package implementation). We have imported the data and the variable containing the data as: `nhis_adult_data`. How would we specify the design using either `as_survey_design()` or `as_survey_rep()`?

2. The General Social Survey (GSS) is a survey that has been administered since 1972 on social, behavioral, and attitudinal topics. The 2016-2020 GSS Panel codebook provides examples of setting up syntax in SAS and Stata but not R [@gss-codebook]. We have imported the data and the variable containing the data as: `gss_data`. How would we specify the design in R using either `as_survey_design()` or `as_survey_rep()`?

<!--chapter:end:10-sample-designs-replicate-weights.Rmd-->

# Missing data {#c11-missing-data}

\index{Missing data|(}
```{r}
#| label: missing-styler
#| include: false
knitr::opts_chunk$set(tidy = 'styler')
```

::: {.prereqbox-header}
`r if (knitr:::is_html_output()) '### Prerequisites {- #prereq11}'`
:::

::: {.prereqbox data-latex="{Prerequisites}"}
For this chapter, load the following packages:
```{r}
#| label: missing-setup
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(survey) 
library(srvyr) 
library(srvyrexploR)
library(naniar)
library(haven)
library(gt)
```


We are using data from ANES and RECS described in Chapter \@ref(c04-getting-started). As a reminder, here is the code to create the design objects for each to use throughout this chapter. For ANES, we need to adjust the weight so it sums to the population instead of the sample (see the ANES documentation and Chapter \@ref(c04-getting-started) for more information).

```{r}
#| label: missing-anes-des
#| eval: FALSE
targetpop <- 231592693

anes_adjwgt <- anes_2020 %>%
  mutate(Weight = Weight / sum(Weight) * targetpop)

anes_des <- anes_adjwgt %>%
  as_survey_design(
    weights = Weight,
    strata = Stratum,
    ids = VarUnit,
    nest = TRUE
  )
```

For RECS, details are included in the RECS documentation and Chapter \@ref(c10-sample-designs-replicate-weights).

```{r}
#| label: missing-recs-des
#| eval: FALSE
recs_des <- recs_2020 %>%
  as_survey_rep(
    weights = NWEIGHT,
    repweights = NWEIGHT1:NWEIGHT60,
    type = "JK1",
    scale = 59/60,
    mse = TRUE
  )
```
:::

## Introduction

Missing data in surveys refer to situations where participants do not provide complete responses to survey questions. Respondents may not have seen a question by design. Or, they may not respond to a question for various other reasons, such as not wanting to answer a particular question, not understanding the question, or simply forgetting to answer. Missing data are important to consider and account for, as they can introduce bias and reduce the representativeness of the data. This chapter provides an overview of the types of missing data, how to assess missing data in surveys, and how to conduct analysis when missing data are present. Understanding this complex topic can help ensure accurate reporting of survey results and provide insight into potential changes to the survey design for the future.

## Missing data mechanisms

\index{Item nonresponse|(}There are two main categories that missing data typically fall into: missing by design and unintentional missing data. Missing by design is part of the survey plan and can be more easily incorporated into weights and analyses.  Unintentional missing data, on the other hand, can lead to bias in survey estimates if not correctly accounted for.  Below we provide more information on the types of missing data.

1. Missing by design/questionnaire skip logic: This type of missingness occurs when certain respondents are intentionally directed to skip specific questions based on their previous responses or characteristics. For example, in a survey about employment, if a respondent indicates that they are not employed, they may be directed to skip questions related to their job responsibilities. Additionally, some surveys randomize questions or modules so that not all participants respond to all questions. In these instances, respondents would have missing data for the modules not randomly assigned to them.

2. Unintentional missing data: This type of missingness occurs when researchers do not intend for there to be missing data on a particular question, for example, if respondents did not finish the survey or refused to answer individual questions.  There are three main types of unintentional missing data that each should be considered and handled differently [@mack; @Schafer2002]:   

    a. Missing completely at random (MCAR): The missing data are unrelated to both observed and unobserved data, and the probability of being missing is the same across all cases. For example, if a respondent missed a question because they had to leave the survey early due to an emergency.

    b. Missing at random (MAR): The missing data are related to observed data but not unobserved data, and the probability of being missing is the same within groups. For example, we know the respondents' ages and older respondents choose not to answer specific questions but younger respondents do answer them.

    c. Missing not at random (MNAR): The missing data are related to unobserved data, and the probability of being missing varies for reasons we are not measuring. For example, if respondents with depression do not answer a question about depression severity.


## Assessing missing data

Before beginning an analysis, we should explore the data to determine if there is missing data and what types of missing data are present. Conducting descriptive analysis can help with the analysis and reporting of survey data and can inform the survey design in future studies. For example, large amounts of unexpected missing data may indicate the questions were unclear or difficult to recall.  There are several ways to explore missing data, which we walk through below. When assessing the missing data, we recommend using a data.frame object and not the survey object, as most of the analysis is about patterns of records, and weights are not necessary.

### Summarize data

\index{American National Election Studies (ANES)|(}
A very rudimentary first exploration is to use the `summary()` function to summarize the data, which illuminates `NA` values in the data. Let's look at a few analytic variables on the ANES 2020 data using `summary()`:

```{r}
#| label: missing-anes-summary

anes_2020 %>%
  select(V202051:EarlyVote2020) %>%
  summary()
```

We see that there are `NA` values in several of the derived variables (those not beginning with "V") and negative values in the original variables (those beginning with "V"). We can also use the `count()` function to get an understanding of the different types of missing data on the original variables.  For example, let's look at the count of data for `V202072`, which corresponds to our `VotedPres2020` variable.

```{r}
#| label: missing-anes-count

anes_2020 %>% 
  count(VotedPres2020,V202072)
```

Here, we can see that there are three types of missing data, and the majority of them fall under the "Inapplicable" category.  This is usually a term associated with data missing due to skip patterns and is considered to be missing data by design.  Based on the documentation from ANES [@debell], we can see that this question was only asked to respondents who voted in the election. 

### Visualization of missing data

It can be challenging to look at tables for every variable and instead may be more efficient to view missing data in a graphical format to help narrow in on patterns or unique variables. The {naniar} package is very useful in exploring missing data visually. We can use the `vis_miss()` function available in both {visdat} and {naniar} packages to view the amount of missing data by variable (see Figure \@ref(fig:missing-anes-vismiss)) [@visdattierney; @naniar2023].

```{r}
#| label: missing-anes-vismiss
#| warning: FALSE
#| message: FALSE
#| fig.cap: "Visual depiction of missing data in the ANES 2020 data"
#| fig.alt: "This chart shows a the missingness of the selected variables where missing is highlighted in a dark color. Each row of the plot is an observation and each column is a variable. There are some patterns observed such as a large block of missing for `VotedPres2016_selection` and many of the same respondents also having missing for `VotedPres2020_selection`."

anes_2020_derived<-anes_2020 %>%
  select(
    -starts_with("V2"), -CaseID, -InterviewMode, 
    -Weight, -Stratum, -VarUnit)

anes_2020_derived %>%
  vis_miss(cluster= TRUE, show_perc = FALSE) +
  scale_fill_manual(values = book_colors[c(3,1)], 
                    labels = c("Present","Missing"),
                    name = "") +
  theme(
    plot.margin=margin(5.5,30,5.5,5.5, "pt"),
    axis.text.x=element_text(angle=70))

```

From the visualization in Figure \@ref(fig:missing-anes-vismiss), we can start to get a picture of what questions may be connected in terms of missing data.  Even if we did not have the informative variable names, we could deduce that `VotedPres2020`, `VotedPres2020_selection`, and `EarlyVote2020` are likely connected since their missing data patterns are similar.

Additionally, we can also look at `VotedPres2016_selection` and see that there are a lot of missing data in that variable. The missing data are likely due to a skip pattern, and we can look at other graphics to see how they relate to other variables. The {naniar} package has multiple visualization functions that can help dive deeper, such as the `gg_miss_fct()` function, which looks at missing data for all variables by levels of another variable (see Figure \@ref(fig:missing-anes-ggmissfct)).

```{r}
#| label: missing-anes-ggmissfct
#| warning: FALSE
#| message: FALSE
#| fig.cap: Missingness in variables for each level of 'VotedPres2016,' in the ANES 2020 data
#| fig.alt: "This chart has x-axis 'Voted for President in 2016' with labels Yes, No and NA and has y-axis 'Variable' with labels Age, AgeGroup, CampaignInterest, EarlyVote2020, Education, Gender, Income, Income7, PartyID, RaceEth, TrustGovernment, TrustPeople, VotedPres2016_selection, VotedPres2020 and VotedPres2020_selection. There is a legend indicating fill is used to show pct_miss, ranging from 0 represented by fill very pale blue to 100 shown as fill dark blue. Among those that voted for president in 2016, they had little missing for other variables (light color) but those that did not vote have more missing data in their 2020 voting patterns and their 2016 president selection."

anes_2020_derived %>% 
  gg_miss_fct(VotedPres2016) +
  scale_fill_gradientn(
    guide = "colorbar",
    name = "% Miss",
    colors = book_colors[c(3, 2, 1)] 
  ) +
  ylab("Variable") +
  xlab("Voted for President in 2016")
```

In Figure \@ref(fig:missing-anes-ggmissfct), we can see that if respondents did not vote for president in 2016 or did not answer that question, then they were not asked about who they voted for in 2016 (the percentage of missing data is 100%).  Additionally, we can see with Figure \@ref(fig:missing-anes-ggmissfct) that there are more missing data across all questions if they did not provide an answer to `VotedPres2016`.
\index{American National Election Studies (ANES)|)}

\index{Residential Energy Consumption Survey (RECS)|(}
There are other visualizations that work well with numeric data.  For example, in the RECS 2020 data, we can plot two continuous variables and the missing data associated with them to see if there are any patterns in the missingness.  To do this, we can use the `bind_shadow()` function from the {naniar} package. This creates a nabular (combination of "na" with "tabular"), which features the original columns followed by the same number of columns with a specific `NA` format.  These `NA` columns are indicators of whether the value in the original data is missing or not.  The example printed below shows how most levels of `HeatingBehavior` are not missing (`!NA`) in the NA variable of `HeatingBehavior_NA`, but those missing in `HeatingBehavior` are also missing in `HeatingBehavior_NA`.

```{r}
#| label: missing-recs-shadow

recs_2020_shadow <- recs_2020 %>% 
  bind_shadow()

ncol(recs_2020)
ncol(recs_2020_shadow)

recs_2020_shadow %>% 
  count(HeatingBehavior,HeatingBehavior_NA)
```

We can then use these new variables to plot the missing data alongside the actual data.  For example, let's plot a histogram of the total electric bill grouped by those missing and not missing by heating behavior (see Figure \@ref(fig:missing-recs-hist)).

```{r}
#| label: missing-recs-hist
#| fig.cap: "Histogram of energy cost by heating behavior missing data"
#| fig.alt: "This chart has title 'Histogram of Energy Cost by Heating Behavior Missing Data'. It has x-axis 'Total Energy Cost (Truncated at $5000)' with labels 0, 1000, 2000, 3000, 4000 and 5000. It has y-axis 'Number of Households' with labels 0, 500, 1000 and 1500. There is a legend indicating fill is used to show HeatingBehavior_NA, with 2 levels: !NA shown as very pale blue fill and  NA shown as dark blue fill. The chart is a bar chart with 30 vertical bars. These are stacked, as sorted by HeatingBehavior_NA."
recs_2020_shadow %>%
  filter(TOTALDOL < 5000) %>%
  ggplot(aes(x = TOTALDOL, fill = HeatingBehavior_NA)) +
  geom_histogram() +
  scale_fill_manual(
    values = book_colors[c(3, 1)],
    labels = c("Present", "Missing"),
    name = "Heating Behavior"
  ) +
  theme_minimal() +
  xlab("Total Energy Cost (Truncated at $5000)") +
  ylab("Number of Households")
```

Figure \@ref(fig:missing-recs-hist) indicates that respondents who did not provide a response for the heating behavior question may have a different distribution of total energy cost compared to respondents who did provide a response. This view of the raw data and missingness could indicate some bias in the data. Researchers take these different bias aspects into account when calculating weights, and we need to make sure that we incorporate the weights when analyzing the data. \index{Residential Energy Consumption Survey (RECS)|)}

There are many other visualizations that can be helpful in reviewing the data, and we recommend reviewing the {naniar} documentation for more information [@naniar2023].


## Analysis with missing data

\index{Imputation|(}
Once we understand the types of missingness, we can begin the analysis of the data. Different missingness types may be handled in different ways. In most publicly available datasets, researchers have already calculated weights and imputed missing values if necessary. Often, there are imputation flags included in the data that indicate if each value in a given variable is imputed.  For example, in the RECS data we may see a logical variable of `ZWinterTempNight`, where a value of `TRUE` means that the value of `WinterTempNight` for that respondent was imputed, and `FALSE` means that it was not imputed. We may use these imputation flags if we are interested in examining the nonresponse rates in the original data. For those interested in learning more about how to calculate weights and impute data for different missing data mechanisms, we recommend @Kim2021 and @Valliant2018weights.

Even with weights and imputation, missing data are most likely still present and need to be accounted for in analysis. This section provides an overview on how to recode missing data in R, and how to account for skip patterns in analysis.
\index{Imputation|)}

### Recoding missing data

\index{American National Election Studies (ANES)|(}
Even within a variable, there can be different reasons for missing data. In publicly released data, negative values are often present to provide different meanings for values.  For example, in the ANES 2020 data, they have the following negative values to represent different types of missing data:


  * --9: Refused
  * --8: Don't Know
  * --7: No post-election data, deleted due to incomplete interview
  * --6: No post-election interview
  * --5: Interview breakoff (sufficient partial IW)
  * --4: Technical error
  * --3: Restricted
  * --2: Other missing reason (question specific)
  * --1: Inapplicable

When we created the derived variables for use in this book, we coded all negative values as `NA` and proceeded to analyze the data.  For most cases, this is an appropriate approach as long as we filter the data appropriately to account for skip patterns (see Section \@ref(missing-skip-patt)). However, the {naniar} package does have the option to code special missing values. For example, if we wanted to have two `NA` values, one that indicated the question was missing by design (e.g., due to skip patterns) and one for the other missing categories, we can use the `nabular` format to incorporate these with the `recode_shadow()` function. 


```{r}
#| label: missing-anes-shadow-recode

anes_2020_shadow<-anes_2020 %>%
  select(starts_with("V2")) %>%
  mutate(across(everything(),~case_when(.x < -1 ~ NA,
                                        TRUE~.x))) %>% 
  bind_shadow() %>%
  recode_shadow(V201103 = .where(V201103==-1~"skip"))

anes_2020_shadow %>% 
  count(V201103,V201103_NA)
```

However, it is important to note that at the time of publication, there is no easy way to implement `recode_shadow()` to multiple variables at once (e.g., we cannot use the tidyverse feature of `across()`). The example code above only implements this for a single variable, so this would have to be done manually or in a loop for all variables of interest. \index{American National Election Studies (ANES)|)}

### Accounting for skip patterns {#missing-skip-patt}

When questions are skipped by design in a survey, it is meaningful that the data are later missing. For example, the RECS asks people how they control the heat in their home in the winter (`HeatingBehavior`). This is only among those who have heat in their home (`SpaceHeatingUsed`). If there is no heating equipment used, the value of `HeatingBehavior` is missing. One has several choices when analyzing these data which include: (1) only including those with a valid value of `HeatingBehavior` and specifying the universe as those with heat or (2) including those who do not have heat. It is important to specify what population an analysis generalizes to.

\index{Residential Energy Consumption Survey (RECS)|(}
Here is an example where we only include those with a valid value of `HeatingBehavior` (choice 1). Note that we use the design object (`recs_des`) and then \index{Functions in srvyr!filter|(}filter to those that are not missing on `HeatingBehavior`.\index{Functions in srvyr!survey\_prop} \index{Functions in srvyr!summarize|(}

```{r}
#| label: missing-recs-heatcc

heat_cntl_1 <- recs_des %>%
  filter(!is.na(HeatingBehavior)) %>%
  group_by(HeatingBehavior) %>%
  summarize(
    p=survey_prop()
  )

heat_cntl_1
```
\index{Functions in srvyr!filter|)}

Here is an example where we include those who do not have heat (choice 2). To help understand what we are looking at, we have included the output to show both variables, `SpaceHeatingUsed` and `HeatingBehavior`. \index{Functions in srvyr!survey\_prop} \index{Functions in srvyr!interact|(} 

```{r}
#| label: missing-recs-heatpop

heat_cntl_2 <- recs_des %>%
  group_by(interact(SpaceHeatingUsed, HeatingBehavior)) %>%
  summarize(
    p=survey_prop()
  )

heat_cntl_2
```
\index{Functions in srvyr!interact|)} \index{Functions in srvyr!summarize|)}

```{r}
#| label: missing-recs-heattext
#| echo: FALSE

pct_1 <- heat_cntl_1 %>% 
  filter(str_detect(HeatingBehavior, "Program")) %>%
  mutate(p=round(p*100, 1)) %>%
  pull(p)

pct_2 <- heat_cntl_2 %>% 
  filter(str_detect(HeatingBehavior, "Program")) %>%
  mutate(p=round(p*100, 1)) %>%
  pull(p)

```

If we ran the first analysis, we would say that `r pct_1`% of households with heat use a programmable or smart thermostat for heating their home. If we used the results from the second analysis, we would say that `r pct_2`% of households use a programmable or smart thermostat for heating their home. The distinction between the two statements is made bold for emphasis. Skip patterns often change the universe we are talking about and need to be carefully examined. \index{Residential Energy Consumption Survey (RECS)|)}

\index{American National Election Studies (ANES)|(}
Filtering to the correct universe is important when handling these types of missing data.  The `nabular` we created above can also help with this.  If we have `NA_skip` values in the shadow, we can make sure that we filter out all of these values and only include relevant missing values.  To do this with survey data, we could first create the `nabular`, then create the \index{Functions in srvyr!as\_survey\_design|(} design object on that data, and then use the shadow variables to assist with filtering the data.  Let's use the `nabular` we created above for ANES 2020 (`anes_2020_shadow`) to create the design object. 

```{r}
#| label: missing-anes-shadow-des
#| warning: FALSE

anes_adjwgt_shadow <- anes_2020_shadow %>% 
  mutate(V200010b = V200010b/sum(V200010b)*targetpop)

anes_des_shadow <- anes_adjwgt_shadow %>% 
  as_survey_design(
    weights = V200010b,
    strata = V200010d,
    ids = V200010c,
    nest = TRUE
  )
```
\index{Functions in srvyr!as\_survey\_design|)}

Then, we can use this design object to look at the percentage of the population who voted for each candidate in 2016 (`V201103`). First, let's look at the percentages without removing any cases: \index{Functions in srvyr!survey\_prop} \index{Functions in srvyr!summarize|(}

```{r}
#| label: missing-anes-shadow-ex1

pres16_select1<-anes_des_shadow %>% 
  group_by(V201103) %>%
  summarize(
    All_Missing=survey_prop()
  )

pres16_select1
```

Next, we look at the percentages, removing only those missing due to skip patterns (i.e., they did not receive this question). \index{Functions in srvyr!survey\_prop} \index{Functions in srvyr!filter|(} 

```{r}
#| label: missing-anes-shadow-ex2

pres16_select2<-anes_des_shadow %>% 
  filter(V201103_NA!="NA_skip") %>% 
  group_by(V201103) %>%
  summarize(
    No_Skip_Missing=survey_prop()
  )

pres16_select2
```

Finally, we look at the percentages, removing all missing values both due to skip patterns and due to those who refused to answer the question. \index{Functions in srvyr!survey\_prop}  

```{r}
#| label: missing-anes-shadow-ex3

pres16_select3<-anes_des_shadow %>% 
  filter(V201103_NA=="!NA") %>% 
  group_by(V201103) %>%
  summarize(
    No_Missing=survey_prop()
  )

pres16_select3
```
\index{Functions in srvyr!filter|)} \index{Functions in srvyr!summarize|)}

```{r}
#| label: missing-anes-shadow-gt
#| echo: FALSE

pres16_select_gt<-pres16_select1 %>% 
  full_join(pres16_select2,by="V201103") %>% 
  full_join(pres16_select3,by="V201103") %>% 
  mutate(Candidate=case_when(V201103==-1~"Did Not Vote for President in 2016",
                             V201103==1~"Hillary Clinton",
                             V201103==2~"Donald Trump",
                             V201103==5~"Other Candidate",
                             TRUE~"Missing")) %>% 
  select(Candidate,everything()) %>% 
  select(-V201103) %>% 
  gt() %>% 
  cols_label(All_Missing = "%",
             All_Missing_se = "s.e. (%)",
             No_Skip_Missing = "%",
             No_Skip_Missing_se = "s.e. (%)",
             No_Missing = "%",
             No_Missing_se = "s.e. (%)") %>%
  tab_spanner(label = "Including All Missing Data",
              columns = c(All_Missing, All_Missing_se)) %>%
  tab_spanner(label = "Removing Skip Patterns Only",
              columns = c(No_Skip_Missing, No_Skip_Missing_se)) %>%
  tab_spanner(label = "Removing All Missing Data",
              columns = c(No_Missing, No_Missing_se)) %>%
  fmt_number(decimals = 1, scale_by=100)
```

(ref:missing-anes-shadow-tab) Percentage of votes by candidate for different missing data inclusions

```{r}
#| label: missing-anes-shadow-tab
#| echo: FALSE
#| warning: FALSE

pres16_select_gt %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

```{r}
#| label: missing-anes-shadow-tab-text
#| echo: FALSE
#| warning: FALSE

pres16_select1_1<-pres16_select1 %>% 
  filter(V201103==1) %>% 
  pull(All_Missing)

pres16_select1_2<-pres16_select1 %>% 
  filter(V201103==2) %>% 
  pull(All_Missing)

pres16_select1_out<-round(pres16_select1_1*100-pres16_select1_2*100,1)


pres16_select2_1<-pres16_select2 %>% 
  filter(V201103==1) %>% 
  pull(No_Skip_Missing)

pres16_select2_2<-pres16_select2 %>% 
  filter(V201103==2) %>% 
  pull(No_Skip_Missing)

pres16_select2_out<-round(pres16_select2_1*100-pres16_select2_2*100,1)

```


As Table \@ref(tab:missing-anes-shadow-tab) shows, the results can vary greatly depending on which type of missing data are removed. If we remove only the skip patterns, the margin between Clinton and Trump is `r pres16_select2_out` percentage points; but if we include all data, even those who did not vote in 2016, the margin is `r pres16_select1_out` percentage points. How we handle the different types of missing values is important for interpreting the data.
\index{Item nonresponse|)} \index{American National Election Studies (ANES)|)}

\index{Missing data|)}

<!--chapter:end:11-missing-data.Rmd-->

# Successful survey analysis recommendations {#c12-recommendations}

```{r}
#| label: recommendations-styler
#| include: false
knitr::opts_chunk$set(tidy = 'styler')
```

::: {.prereqbox-header}
`r if (knitr:::is_html_output()) '### Prerequisites {- #prereq12}'`
:::

::: {.prereqbox data-latex="{Prerequisites}"}
For this chapter, load the following packages:
```{r}
#| label: recommendations-setup
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(survey)
library(srvyr)
library(srvyrexploR)
```

To illustrate the importance of data visualization, we discuss Anscombe's Quartet. The dataset can be replicated by running the code below:

```{r}
#| label: recommendations-anscombe-setup
anscombe_tidy <- anscombe %>%
  mutate(obs = row_number()) %>%
  pivot_longer(-obs, names_to = "key", values_to = "value") %>%
  separate(key, c("variable", "set"), 1, convert = TRUE) %>%
  mutate(set = c("I", "II", "III", "IV")[set]) %>%
  pivot_wider(names_from = variable, values_from = value)
```

We create an example survey dataset to explain potential pitfalls and how to overcome them in survey analysis. To recreate the dataset, run the code below:

```{r}
#| label: recommendations-example-dat
example_srvy <- tribble(
  ~id, ~region, ~q_d1,                 ~q_d2_1, ~gender, ~weight,
   1L,   1L,    1L,      "Somewhat interested", "female",  1740,
   2L,   1L,    1L,    "Not at all interested", "female",  1428,
   3L,   2L,    NA,      "Somewhat interested", "female",   496,
   4L,   2L,    1L,    "Not at all interested", "female",   550,
   5L,   3L,    1L,      "Somewhat interested", "female",  1762,
   6L,   4L,    NA,          "Very interested", "female",  1004,
   7L,   4L,    NA,      "Somewhat interested", "female",   522,
   8L,   3L,    2L,    "Not at all interested", "female",  1099,
   9L,   4L,    2L,      "Somewhat interested", "female",  1295,
   10L,  2L,    2L,      "Somewhat interested",   "male",   983
)

example_des <-
  example_srvy %>%
  as_survey_design(weights = weight)
```
:::

## Introduction

The previous chapters in this book aimed to provide the technical skills and knowledge required for running survey analyses. This chapter builds upon the previously mentioned best practices to present a curated set of recommendations for running a successful survey analysis. We hope this list provides practical insights that assist in producing meaningful and reliable results.

## Follow the survey analysis process {#recs-survey-process}

\index{Survey analysis process|(}As we first introduced in Chapter \@ref(c04-getting-started), there are four main steps to successfully analyze survey data:

1. Create a `tbl_svy` object (a survey object) using: `as_survey_design()` or `as_survey_rep()`

2. Subset data (if needed) using `filter()` (to create subpopulations)

3. Specify domains of analysis using `group_by()` 

4. Within `summarize()`, specify variables to calculate, including means, totals, proportions, quantiles, and more

The order of these steps matters in survey analysis. For example, if we need to subset the data, \index{Functions in srvyr!filter}we must use `filter()` on our data after creating the survey design. If we do this before the survey design is created, we may not be correctly accounting for the study design, resulting in inaccurate findings.\index{Survey analysis process|)}

Additionally, correctly identifying the survey design is one of the most important steps in survey analysis.  Knowing the type of sample design (e.g., clustered, stratified) helps ensure the underlying error structure is correctly calculated and weights are correctly used. Learning about complex design factors such as clustering, stratification, and weighting is foundational to complex survey analysis, and we recommend that all analysts review Chapter \@ref(c10-sample-designs-replicate-weights) before creating their first design object. Reviewing the documentation (see Chapter \@ref(c03-survey-data-documentation)) helps us understand what variables to use from the data.

Making sure to use the survey analysis functions from the {srvyr} and {survey} packages is also important in survey analysis. For example, using `mean()` and \index{Functions in srvyr!survey\_mean}`survey_mean()` on the same data results in different findings and outputs. Each of the survey functions from {srvyr} and {survey} impacts standard errors and variance, and we cannot treat complex surveys as unweighted simple random samples if we want to produce unbiased estimates [@R-srvyr; @lumley2010complex]. 

## Begin with descriptive analysis

When receiving a fresh batch of data, it is tempting to jump right into running models to find significant results. However, a successful data analyst begins by exploring the dataset. Chapter \@ref(c11-missing-data) talks about the importance of reviewing data when examining missing data patterns. In this chapter, we illustrate the value of reviewing all types of data. This involves running descriptive analysis on the dataset as a whole, as well as individual variables and combinations of variables. As described in Chapter \@ref(c05-descriptive-analysis), descriptive analyses should always precede statistical analysis to prevent avoidable (and potentially embarrassing) mistakes.

### Table review

\index{Cross-tabulation|(}
Even before applying weights, consider running cross-tabulations on the raw data. Cross-tabs can help us see if any patterns stand out that may be alarming or something worth further investigating.
\index{Cross-tabulation|)}

For example, let’s explore the example survey dataset introduced in the Prerequisites box, `example_srvy`. We run the code below on the unweighted data to inspect the `gender` variable:

```{r}
#| label: recommendations-example-desc
example_srvy %>% 
  group_by(gender) %>% 
  summarize(n = n())
```

The data show that females comprise 9 out of 10, or 90%, of the sample. Generally, we assume something close to a 50/50 split between male and female respondents in a population. The sizable female proportion could indicate either a unique sample or a potential error in the data. If we review the survey documentation and see this was a deliberate part of the design, we can continue our analysis using the appropriate methods. If this was not an intentional choice by the researchers, the results alert us that something may be incorrect in the data or our code, and we can verify if there’s an issue by comparing the results with the weighted means.

### Graphical review

Tables provide a quick check of our assumptions, but there is no substitute for graphs and plots to visualize the distribution of data. We might miss outliers or nuances if we scan only summary statistics. 

For example, Anscombe's Quartet demonstrates the importance of visualization in analysis. Let's say we have a dataset with x- and y-variables in an object called `anscombe_tidy`. Let's take a look at how the dataset is structured:

```{r}
#| label: recommendations-anscombe-head
head(anscombe_tidy)
```

We can begin by checking one set of variables. For Set I, the x-variables have an average of 9 with a standard deviation of 3.3; for y, we have an average of 7.5 with a standard deviation of 2.03. The two variables have a correlation of 0.81.

```{r}
#| label: recommendations-anscombe-calc
anscombe_tidy %>% 
  filter(set == "I") %>% 
  summarize(
    x_mean = mean(x),
    x_sd = sd(x),
    y_mean = mean(y),
    y_sd = sd(y),
    correlation = cor(x, y)
  )
```

These are useful statistics. We can note that the data do not have high variability, and the two variables are strongly correlated. Now, let’s check all the sets (I-IV) in the Anscombe data. Notice anything interesting?

```{r}
#| label: recommendations-anscombe-calc-2
anscombe_tidy %>% 
  group_by(set) %>%
  summarize(
    x_mean = mean(x),
    x_sd = sd(x, na.rm = TRUE),
    y_mean = mean(y),
    y_sd = sd(y, na.rm = TRUE),
    correlation = cor(x, y)
  )
```

The summary results for these four sets are nearly identical! Based on this, we might assume that each distribution is similar. Let's look at a graphical visualization to see if our assumption is correct (see Figure \@ref(fig:recommendations-anscombe-plot)).

```{r}
#| label: recommendations-anscombe-plot
#| warning: false
#| error: false
#| message: false
#| fig.cap: "Plot of Anscombe's Quartet data and the importance of reviewing data graphically"
#| fig.alt: "This figure shows four plots one for each of Anscombe's sets. The upper left plot is a plot of set I and has a trend line with a slope of 0.5 and an intercept of 3. The data points are distributed evenly around the trend line. The upper right plot is a plot of set II and has the same trend line as set I. The data points are curved around the trend line. The lower left plot is a plot of set III and has the same trend line as set I.  The data points closely followly the trend line with one outlier where the y-value for the point is much larger than the others.  The lower right plot is a plot of set IV and has the same trend line as set I. The data points all share the same x-value but different y-values with the exception of one data point, which has a much larger value for both y and x values."

ggplot(anscombe_tidy, aes(x, y)) +
  geom_point() +
  facet_wrap( ~ set) +
  geom_smooth(method = "lm", se = FALSE, alpha = 0.5) +
  theme_minimal()
```

Although each of the four sets has the same summary statistics and regression line, when reviewing the plots (see Figure \@ref(fig:recommendations-anscombe-plot)), it becomes apparent that the distributions of the data are not the same at all. Each set of points results in different shapes and distributions. Imagine sharing each set (I-IV) and the corresponding plot with a different colleague. The interpretations and descriptions of the data would be very different even though the statistics are similar.   Plotting data can also ensure that we are using the correct analysis method on the data, so understanding the underlying distributions is an important first step.

## Check variable types

When we pull the data from surveys into R, the data may be listed as character, factor, numeric, or logical/Boolean.  The tidyverse functions that read in data (e.g., `read_csv()`, `read_excel()`) default to have all strings load as character variables. This is important when dealing with survey data, as many strings may be better suited for factors than character variables. For example, let's revisit the `example_srvy` data. Taking a `glimpse()` of the data gives us insight into what it contains:

```{r}
#| label: recommendations-example-dat-glimpse
example_srvy %>%
  glimpse()
```

\index{Factor|(}
The output shows that `q_d2_1` is a character variable, but the values of that variable show three options (Very interested / Somewhat interested / Not at all interested). In this case, we most likely want to change `q_d2_1` to be a factor variable and order the factor levels to indicate that this is an ordinal variable.  Here is some code on how we might approach this task using the {forcats} package [@R-forcats]:

```{r}
#| label: recommendations-example-dat-fct
example_srvy_fct <- example_srvy %>%
  mutate(q_d2_1_fct = factor(
    q_d2_1,
    levels = c("Very interested",
               "Somewhat interested",
               "Not at all interested")
  ))

example_srvy_fct %>%
  glimpse()

example_srvy_fct %>%
  count(q_d2_1_fct, q_d2_1)
```

\index{Codebook|(} \index{Categorical data|(}
This example dataset also includes a column called `region`, which is imported as a number (`<int>`). This is a good reminder to use the questionnaire and codebook along with the data to find out if the values actually reflect a number or are perhaps a coded categorical variable (see Chapter \@ref(c03-survey-data-documentation) for more details). R calculates the mean even if it is not appropriate, leading to the common mistake of applying an average to categorical values instead of a proportion function. For example, for ease of coding, we may use the `across()` function to calculate the mean across all numeric variables: \index{Functions in srvyr!survey\_mean|(} \index{Functions in srvyr!summarize|(} \index{Codebook|()} \index{Categorical data|)}

```{r}
#| label: recommendations-example-dat-num-calc
example_des %>%
  select(-weight) %>%
  summarize(across(where(is.numeric), ~ survey_mean(.x, na.rm = TRUE)))
```

In this example, if we do not adjust `region` to be a factor variable type, we might accidentally report an average region of `r round(example_des %>% summarize(across(where(is.numeric), ~ survey_mean(.x, na.rm = TRUE))) %>% pull(region), 2)` in our findings, which is meaningless. Checking that our variables are appropriate avoids this pitfall and ensures the measures and models are suitable for the variable type.
\index{Factor|)}


## Improve debugging skills

\index{Debugging|(}
It is common for analysts working in R to come across warning or error messages, and learning how to debug these messages (i.e., find and fix issues) ensures we can proceed with our work and avoid potential mistakes.

We've discussed a few examples in this book. For example, if we calculate an average with `survey_mean()` and get `NA` instead of a number, it may be because our column has missing values. 

```{r}
#| label: recommendations-missing-dat
example_des %>%
  summarize(mean = survey_mean(q_d1))
```

Including the `na.rm = TRUE` would resolve the issue: 

```{r}
#| label: recommendations-missing-dat-fix
example_des %>%
  summarize(mean = survey_mean(q_d1, na.rm = TRUE))
```
\index{Functions in srvyr!summarize|)} \index{Functions in srvyr!survey\_mean|)}

Another common error message that we may see with survey analysis may look something like the following: \index{Functions in survey!svyttest|(}

```{r}
#| label: recommendations-desobj-loc
#| error: true
example_des %>% 
  svyttest(q_d1~gender)
```

\index{Dot notation|(}
In this case, we need to remember that with functions from the {survey} packages like `svyttest()`, the design object is not the first argument, and we have to use the dot (`.`) notation (see Chapter \@ref(c06-statistical-testing)).  Adding in the named argument of `design=.` fixes this error.

```{r}
#| label: recommendations-desobj-locfix
example_des %>%
  svyttest(q_d1 ~ gender,
           design = .)
```

\index{Dot notation|)}

Often, debugging involves interpreting the message from R. For example, if our code results in this error:

```
Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : 
  contrasts can be applied only to factors with 2 or more levels
```

\index{Factor|(}
We can see that the error has to do with a function requiring a factor with two or more levels and that it has been applied to something else. This ties back to our section on using appropriate variable types. We can check the variable of interest to examine whether it is the correct type. \index{Functions in survey!svyttest|)}
\index{Factor|)}

The internet also offers many resources for debugging. Searching for a specific error message can often lead to a solution. In addition, we can post on community forums like [Posit Community](https://forum.posit.co/) for direct help from others. \index{Debugging|)}

## Think critically about conclusions

Once we have our findings, we need to learn to think critically about them. As mentioned in Chapter \@ref(c02-overview-surveys), many aspects of the study design can impact our interpretation of the results, for example, the number and types of response options provided to the respondent or who was asked the question (both thinking about the full sample and any skip patterns). Knowing the overall study design can help us accurately think through what the findings may mean and identify any issues with our analyses. Additionally, we should make sure that our survey design object is correctly defined (see Chapter \@ref(c10-sample-designs-replicate-weights)), carefully consider how we are managing missing data (see Chapter \@ref(c11-missing-data)), and follow statistical analysis procedures such as avoiding model overfitting by using too many variables in our formulas.

These considerations allow us to conduct our analyses and review findings for statistically significant results. It is important to note that even significant results do not mean that they are meaningful or important. A large enough sample can produce statistically significant results. Therefore, we want to look at our results in context, such as comparing them with results from other studies or analyzing them in conjunction with confidence intervals and other measures.

Communicating the results (see Chapter \@ref(c08-communicating-results)) in an unbiased manner is also a critical step in any analysis project. If we present results without error measures or only present results that support our initial hypotheses, we are not thinking critically and may incorrectly represent the data. As survey data analysts, we often interpret the survey data for the public. We must ensure that we are the best stewards of the data and work to bring light to meaningful and interesting findings that the public wants and needs to know about.

<!--chapter:end:12-successful-survey-data-analysis.Rmd-->

# (PART) Vignettes {-}

# National Crime Victimization Survey vignette {#c13-ncvs-vignette}
\index{National Crime Victimization Survey (NCVS)|(}

```{r}
#| label: ncvs-styler
#| include: false
knitr::opts_chunk$set(tidy = 'styler')
```

::: {.prereqbox-header}
`r if (knitr:::is_html_output()) '### Prerequisites {- #prereq9}'`
:::

::: {.prereqbox data-latex="{Prerequisites}"}
For this chapter, load the following packages:
```{r}
#| label: ncvs-setup
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(survey)
library(srvyr) 
library(srvyrexploR)
library(gt)
```

We use data from the United States National Crime Victimization Survey (NCVS). These data are available in the {srvyrexploR} package as `ncvs_2021_incident`, `ncvs_2021_household`, and `ncvs_2021_person`.
:::

## Introduction

The National Crime Victimization Survey (NCVS) is a household survey sponsored by the Bureau of Justice Statistics (BJS), which collects data on criminal victimization, including characteristics of the crimes, offenders, and victims. Crime types include both household and personal crimes, as well as violent and non-violent crimes. The population of interest of this survey is all people in the United States age 12 and older living in housing units and non-institutional group quarters.

The NCVS has been ongoing since 1992. An earlier survey, the National Crime Survey, was run from 1972 to 1991 [@ncvs_tech_2016]. The survey is administered using a rotating panel. When an address enters the sample, the residents of that address are interviewed every 6 months for a total of 7 interviews. If the initial residents move away from the address during the period and new residents move in, the new residents are included in the survey, as people are not followed when they move. 

NCVS data are publicly available and distributed by Inter-university Consortium for Political and Social Research (ICPSR), with data going back to 1992. The vignette in this book includes data from 2021 [@ncvs_data_2021]. The NCVS data structure is complicated, and the User's Guide contains examples for analysis in SAS, SUDAAN, SPSS, and Stata, but not R [@ncvs_user_guide]. This vignette adapts those examples for R. 

## Data structure

The data from ICPSR are distributed with five files, each having its unique identifier indicated:

  - Address Record - `YEARQ`, `IDHH`
  - Household Record - `YEARQ`, `IDHH`
  - Person Record - `YEARQ`, `IDHH`, `IDPER`
  - Incident Record - `YEARQ`, `IDHH`, `IDPER`
  - 2021 Collection Year Incident - `YEARQ`, `IDHH`, `IDPER`

In this vignette, we focus on the household, person, and incident files and have selected a subset of columns for use in the examples. We have included data in the {srvyexploR} package with this subset of columns, but the complete data files can be downloaded from [ICPSR](https://www.icpsr.umich.edu/web/NACJD/studies/38429).

## Survey notation

The NCVS User Guide [@ncvs_user_guide] uses the following notation:

* $i$ represents NCVS households, identified on the household-level file with the household identification number `IDHH`.
* $j$ represents NCVS individual respondents within household $i$, identified on the person-level file with the person identification number `IDPER`.
* $k$ represents reporting periods (i.e., `YEARQ`) for household $i$ and individual respondent $j$.
* $l$ represents victimization records for respondent $j$ in household $i$ and reporting period $k$. Each record on the NCVS incident-level file is associated with a victimization record $l$.
* $D$ represents one or more domain characteristics of interest in the calculation of NCVS estimates. For victimization totals and proportions, domains can be defined on the basis of crime types (e.g., violent crimes, property crimes), characteristics of victims (e.g., age, sex, household income), or characteristics of the victimizations (e.g., victimizations reported to police, victimizations committed with a weapon present). Domains could also be a combination of all of these types of characteristics. For example, in the calculation of victimization rates, domains are defined on the basis of the characteristics of the victims.
* $A_a$ represents the level $a$ of covariate $A$. Covariate $A$ is defined in the calculation of victimization proportions and represents the characteristic we want to obtain the distribution of victimizations in domain $D$.
* $C$ represents the personal or property crime for which we want to obtain a victimization rate.

In this vignette, we discuss four estimates:

1. Victimization totals estimate the number of criminal victimizations with a given characteristic. As demonstrated below, these can be calculated from any of the data files. The estimated victimization total, $\hat{t}_D$ for domain $D$ is estimated as

$$ \hat{t}_D = \sum_{ijkl \in D} v_{ijkl}$$

where $v_{ijkl}$ is the series-adjusted victimization weight for household $i$, respondent $j$, reporting period $k$, and victimization $l$, represented in the data as `WGTVICCY`. 

2. Victimization proportions estimate characteristics among victimizations or victims. Victimization proportions are calculated using the incident data file. The estimated victimization proportion for domain $D$ across level $a$ of covariate $A$, $\hat{p}_{A_a,D}$ is 

$$ \hat{p}_{A_a,D} =\frac{\sum_{ijkl \in A_a, D} v_{ijkl}}{\sum_{ijkl \in D} v_{ijkl}}.$$
The numerator is the number of incidents with a particular characteristic in a domain, and the denominator is the number of incidents in a domain.

3. Victimization rates are estimates of the number of victimizations per 1,000 persons or households in the population^[BJS publishes victimization rates per 1,000, which are also presented in these examples.]. Victimization rates are calculated using the household or person-level data files. The estimated victimization rate for crime $C$ in domain $D$ is

$$\hat{VR}_{C,D}= \frac{\sum_{ijkl \in C,D} v_{ijkl}}{\sum_{ijk \in D} w_{ijk}}\times 1000$$
where $w_{ijk}$ is the person weight (`WGTPERCY`) for personal crimes or household weight (`WGTHHCY`) for household crimes. The numerator is the number of incidents in a domain, and the denominator is the number of persons or households in a domain. Notice that the weights in the numerator and denominator are different; this is important, and in the syntax and examples below, we discuss how to make an estimate that involves two weights.

4. Prevalence rates are estimates of the percentage of the population (persons or households) who are victims of a crime. These are estimated using the household or person-level data files. The estimated prevalence rate for crime $C$ in domain $D$ is

$$ \hat{PR}_{C, D}= \frac{\sum_{ijk \in {C,D}} I_{ij}w_{ijk}}{\sum_{ijk \in D} w_{ijk}} \times 100$$

where $I_{ij}$ is an indicator that a person or household in domain $D$ was a victim of crime $C$ at any time in the year. The numerator is the number of victims in domain $D$ for crime $C$, and the denominator is the number of people or households in the population.

## Data file preparation

\index{Strata|(} \index{Primary sampling unit|(}
Some work is necessary to prepare the files before analysis. The design variables indicating pseudo-stratum (`V2117`) and half-sample code (`V2118`) are only included on the household file, so they must be added to the person and incident files for any analysis.
\index{Strata|)} \index{Primary sampling unit|)}

For victimization rates, we need to know the victimization status for both victims and non-victims. Therefore, the incident file must be summarized and merged onto the household or person files for household-level and person-level crimes, respectively. We begin this vignette by discussing how to create these incident summary files. This is following Section 2.2 of the NCVS User's Guide [@ncvs_user_guide].

### Preparing files for estimation of victimization rates

Each record on the incident file represents one victimization, which is not the same as one incident. Some victimizations have several instances that make it difficult for the victim to differentiate the details of these incidents, labeled as "series crimes." Appendix A of the User's Guide indicates how to calculate the series weight in other statistical languages.

Here, we adapt that code for R. Essentially, if a victimization is a series crime, its series weight is top-coded at 10 based on the number of actual victimizations, that is, even if the crime occurred more than 10 times, it is counted as 10 times to reduce the influence of extreme outliers. If an incident is a series crime, but the number of occurrences is unknown, the series weight is set to 6. A description of the variables used to create indicators of series and the associated weights is included in Table \@ref(tab:cb-incident).

Table: (\#tab:cb-incident) Codebook for incident variables, related to series weight

|  | Description | Value | Label |
|:--:|:-----:|:-:|:-----:|
| V4016 | How many times incident occur last 6 months | 1--996 | Number of times |
|  |  | 997 | Don't know |
| V4017 | How many incidents | 1 | 1--5 incidents (not a "series") |
|  |  | 2 | 6 or more incidents |
|  |  | 8 | Residue (invalid data) |
| V4018 | Incidents similar in detail | 1 | Similar |
|  |  | 2 | Different (not in a "series") |
|  |  | 8 | Residue (invalid data) |
| V4019 | Enough detail to distinguish incidents | 1 | Yes (not a "series") |
|  |  | 2 | No (is a "series") |
|  |  | 8 | Residue (invalid data) |
| WGTVICCY | Adjusted victimization weight |  | Numeric |

We want to create four variables to indicate if an incident is a series crime.  First, we create a variable called `series` using `V4017`, `V4018`, and `V4019` where an incident is considered a series crime if there are 6 or more incidents (`V4107`), the incidents are similar in detail (`V4018`), or there is not enough detail to distinguish the incidents (`V4019`).  Second, we top-code the number of incidents (`V4016`) by creating a variable `n10v4016`, which is set to 10 if `V4016 > 10`.  Third, we create the `serieswgt` using the two new variables `series` and `n10v4019` to classify the max series based on missing data and number of incidents. Finally, we create the new weight using our new `serieswgt` variable and the existing weight (`WGTVICCY`).

```{r}
#| label: ncvs-vign-incfile
#| message: false

inc_series <- ncvs_2021_incident %>%
  mutate(
    series = case_when(V4017 %in% c(1, 8) ~ 1,
                       V4018 %in% c(2, 8) ~ 1,
                       V4019 %in% c(1, 8) ~ 1,
                       TRUE ~ 2
    ),
    n10v4016 = case_when(V4016 %in% c(997, 998) ~ NA_real_,
                         V4016 > 10 ~ 10,
                         TRUE ~ V4016),
    serieswgt = case_when(series == 2 & is.na(n10v4016) ~ 6,
                          series == 2 ~ n10v4016,
                          TRUE ~ 1),
    NEWWGT = WGTVICCY * serieswgt
  )
```

The next step in preparing the files for estimation is to create indicators on the victimization file for characteristics of interest. Almost all BJS publications limit the analysis to records where the victimization occurred in the United States (where `V4022` is not equal to 1). We do this for all estimates as well.  A brief codebook of variables for this task is located in Table \@ref(tab:cb-crimetype).

Table: (\#tab:cb-crimetype) Codebook for incident variables, crime type indicators and characteristics

| Variable | Description | Value | Label |
|:--:|:---:|:-:|:-----:|
| V4022 | In what city/town/village | 1 | Outside U.S. |
|  |  | 2 | Not inside a city/town/village |
|  |  | 3 | Same city/town/village as present residence |
|  |  | 4 | Different city/town/village as present residence |
|  |  | 5 | Don't know |
|  |  | 6 | Don't know if 2, 4, or 5 |
| V4049 | Did offender have a weapon | 1 | Yes |
|  |  | 2 | No |
|  |  | 3 | Don't know |
| V4050 | What was the weapon that offender had | 1 | At least one good entry |
|  |  | 3 | Indicates "Yes-Type Weapon-NA" |
|  |  | 7 | Indicates "Gun Type Unknown" |
|  |  | 8 | No good entry |
| V4051 | Hand gun | 0 | No |
|  |  | 1 | Yes |
| V4052 | Other gun | 0 | No |
|  |  | 1 | Yes |
| V4053 | Knife | 0 | No |
|  |  | 1 | Yes |
| V4399 | Reported to police | 1 | Yes |
|  |  | 2 | No |
|  |  | 3 | Don't know |
| V4529 | Type of crime code | 01 | Completed rape |
|  |  | 02 | Attempted rape |
|  |  | 03 | Sexual attack with serious assault |
|  |  | 04 | Sexual attack with minor assault |
|  |  | 05 | Completed robbery with injury from serious assault |
|  |  | 06 | Completed robbery with injury from minor assault |
|  |  | 07 | Completed robbery without injury from minor assault |
|  |  | 08 | Attempted robbery with injury from serious assault |
|  |  | 09 | Attempted robbery with injury from minor assault |
|  |  | 10 | Attempted robbery without injury |
|  |  | 11 | Completed aggravated assault with injury |
|  |  | 12 | Attempted aggravated assault with weapon |
|  |  | 13 | Threatened assault with weapon |
|  |  | 14 | Simple assault completed with injury |
|  |  | 15 | Sexual assault without injury |
|  |  | 16 | Unwanted sexual contact without force |
|  |  | 17 | Assault without weapon without injury |
|  |  | 18 | Verbal threat of rape |
|  |  | 19 | Verbal threat of sexual assault |
|  |  | 20 | Verbal threat of assault |
|  |  | 21 | Completed purse snatching |
|  |  | 22 | Attempted purse snatching |
|  |  | 23 | Pocket picking (completed only) |
|  |  | 31 | Completed burglary, forcible entry |
|  |  | 32 | Completed burglary, unlawful entry without force |
|  |  | 33 | Attempted forcible entry |
|  |  | 40 | Completed motor vehicle theft |
|  |  | 41 | Attempted motor vehicle theft |
|  |  | 54 | Completed theft less than $10 |
|  |  | 55 | Completed theft $10 to $49 |
|  |  | 56 | Completed theft $50 to $249 |
|  |  | 57 | Completed theft $250 or greater |
|  |  | 58 | Completed theft value NA |
|  |  | 59 | Attempted theft |

Using these variables, we create the following indicators:

1. Property crime
    - `V4529` \(\ge\) 31
    - Variable: `Property`
2. Violent crime
    - `V4529` \(\le\) 20
    - Variable: `Violent`
3. Property crime reported to the police
    - `V4529` \(\ge\) 31 and `V4399`=1
    - Variable: `Property_ReportPolice`
4. Violent crime reported to the police
    - `V4529` < 31 and `V4399`=1
    - Variable: `Violent_ReportPolice`
5. Aggravated assault without a weapon
    - `V4529` in 11:12 and `V4049`=2
    - Variable: `AAST_NoWeap`
6. Aggravated assault with a firearm
    - `V4529` in 11:12 and `V4049`=1 and (`V4051`=1 or `V4052`=1 or `V4050`=7)
    - Variable: `AAST_Firearm`
7. Aggravated assault with a knife or sharp object
    - `V4529` in 11:12 and `V4049`=1 and (`V4053`=1 or `V4054`=1)
    - Variable: `AAST_Knife`
8. Aggravated assault with another type of weapon
    - `V4529` in 11:12 and `V4049`=1 and `V4050`=1 and not firearm or knife
    - Variable: `AAST_Other`

```{r}
#| label: ncvs-vign-inc-inds
inc_ind <- inc_series %>%
  filter(V4022 != 1) %>%
  mutate(
    WeapCat = case_when(
      is.na(V4049) ~ NA_character_,
      V4049 == 2 ~ "NoWeap",
      V4049 == 3 ~ "UnkWeapUse",
      V4050 == 3 ~ "Other",
      V4051 == 1 | V4052 == 1 | V4050 == 7 ~ "Firearm",
      V4053 == 1 | V4054 == 1 ~ "Knife",
      TRUE ~ "Other"
    ),
    V4529_num = parse_number(as.character(V4529)),
    ReportPolice = V4399 == 1,
    Property = V4529_num >= 31,
    Violent = V4529_num <= 20,
    Property_ReportPolice = Property & ReportPolice,
    Violent_ReportPolice = Violent & ReportPolice,
    AAST = V4529_num %in% 11:13,
    AAST_NoWeap = AAST & WeapCat == "NoWeap",
    AAST_Firearm = AAST & WeapCat == "Firearm",
    AAST_Knife = AAST & WeapCat == "Knife",
    AAST_Other = AAST & WeapCat == "Other"
  )
```

This is a good point to pause to look at the output of crosswalks between an original variable and a derived one to check that the logic was programmed correctly and that everything ends up in the expected category.  

```{r}
#| label: ncvs-vign-inc-inds-check
inc_series %>% count(V4022)
inc_ind %>% count(V4022)
inc_ind %>%
  count(WeapCat, V4049, V4050, V4051, V4052, V4052, V4053, V4054)
inc_ind %>% count(V4529, Property, Violent, AAST) %>% print(n = 40)
inc_ind %>% count(ReportPolice, V4399)
inc_ind %>%
  count(AAST,
        WeapCat,
        AAST_NoWeap,
        AAST_Firearm,
        AAST_Knife,
        AAST_Other)
```

After creating indicators of victimization types and characteristics, the file is summarized, and crimes are summed across persons or households by `YEARQ.` Property crimes (i.e., crimes committed against households, such as household burglary or motor vehicle theft) are summed across households, and personal crimes (i.e., crimes committed against an individual, such as assault, robbery, and personal theft) are summed across persons. The indicators are summed using our created series weight variable (`serieswgt`). Additionally, the existing weight variable (`WGTVICCY`) needs to be retained for later analysis.

```{r}
#| label: ncvs-vign-inc-sum
inc_hh_sums <-
  inc_ind %>%
  filter(V4529_num > 23) %>% # restrict to household crimes
  group_by(YEARQ, IDHH) %>%
  summarize(WGTVICCY = WGTVICCY[1],
            across(starts_with("Property"), 
                   ~ sum(. * serieswgt),
                   .names = "{.col}"),
            .groups = "drop")

inc_pers_sums <-
  inc_ind %>%
  filter(V4529_num <= 23) %>% # restrict to person crimes
  group_by(YEARQ, IDHH, IDPER) %>%
  summarize(WGTVICCY = WGTVICCY[1],
            across(c(starts_with("Violent"), starts_with("AAST")),
                   ~ sum(. * serieswgt), 
                   .names = "{.col}"),
            .groups = "drop")
```

Now, we merge the victimization summary files into the appropriate files. For any record on the household or person file that is not on the victimization file, the victimization counts are set to 0 after merging. In this step, we also create the victimization adjustment factor. See Section 2.2.4 in the User's Guide for details of why this adjustment is created [@ncvs_user_guide]. It is calculated as follows:

$$ A_{ijk}=\frac{v_{ijk}}{w_{ijk}}$$

where $w_{ijk}$ is the person weight (`WGTPERCY`) for personal crimes or the household weight (`WGTHHCY`) for household crimes, and $v_{ijk}$ is the victimization weight (`WGTVICCY`) for household $i$, respondent $j$, in reporting period $k$. The adjustment factor is set to 0 if no incidents are reported.

```{r}
#| label: ncvs-vign-merge-inc-sum

hh_z_list <- rep(0, ncol(inc_hh_sums) - 3) %>% as.list() %>%
  setNames(names(inc_hh_sums)[-(1:3)])
pers_z_list <- rep(0, ncol(inc_pers_sums) - 4) %>% as.list() %>%
  setNames(names(inc_pers_sums)[-(1:4)])

hh_vsum <- ncvs_2021_household %>%
  full_join(inc_hh_sums, by = c("YEARQ", "IDHH")) %>%
  replace_na(hh_z_list) %>%
  mutate(ADJINC_WT = if_else(is.na(WGTVICCY), 0, WGTVICCY / WGTHHCY))

pers_vsum <- ncvs_2021_person %>%
  full_join(inc_pers_sums, by = c("YEARQ", "IDHH", "IDPER")) %>%
  replace_na(pers_z_list) %>%
  mutate(ADJINC_WT = if_else(is.na(WGTVICCY), 0, WGTVICCY / WGTPERCY))
```

### Derived demographic variables

A final step in file preparation for the household and person files is creating any derived variables on the household and person files, such as income categories or age categories, for subgroup analysis. We can do this step before or after merging the victimization counts.

#### Household variables

For the household file, we create categories for tenure (rental status), urbanicity, income, place size, and region. A codebook of the household variables is listed in Table \@ref(tab:cb-hh).

Table: (\#tab:cb-hh) Codebook for household variables

|Variable|Description|Value|Label|
|---|---|---|---|
|V2015|Tenure|1|Owned or being bought|
|||2|Rented for cash|
|||3|No cash rent|
|SC214A|Household Income|01|Less than $5,000|
|||02|$5,000--7,499|
|||03|$7,500--9,999|
|||04|$10,000--12,499|
|||05|$12,500--14,999|
|||06|$15,000--17,499|
|||07|$17,500--19,999|
|||08|$20,000--24,999|
|||09|$25,000--29,999|
|||10|$30,000--34,999|
|||11|$35,000--39,999|
|||12|$40,000--49,999|
|||13|$50,000--74,999|
|||15|$75,000--99,999|
|||16|$100,000--149,999|
|||17|$150,000--199,999|
|||18|$200,000 or more|
|V2126B|Place Size (Population) Code|00|Not in a place|
|||13|Population under 10,000|
|||16|10,000--49,999|
|||17|50,000--99,999|
|||18|100,000--249,999|
|||19|250,000--499,999|
|||20|500,000--999,999|
|||21|1,000,000--2,499,999|
|||22|2,500,000--4,999,999|
|||23|5,000,000 or more|
|V2127B|Region|1|Northeast|
|||2|Midwest|
|||3|South|
|||4|West|
|V2143|Urbanicity|1|Urban|
|||2|Suburban|
|||3|Rural|

```{r}
#| label: ncvs-vign-hh-der
hh_vsum_der <- hh_vsum %>%
  mutate(
    Tenure = factor(case_when(V2015 == 1 ~ "Owned", 
                              !is.na(V2015) ~ "Rented"),
                    levels = c("Owned", "Rented")),
    Urbanicity = factor(case_when(V2143 == 1 ~ "Urban",
                                  V2143 == 2 ~ "Suburban",
                                  V2143 == 3 ~ "Rural"),
                        levels = c("Urban", "Suburban", "Rural")),
    SC214A_num = as.numeric(as.character(SC214A)),
    Income = case_when(SC214A_num <= 8 ~ "Less than $25,000",
                       SC214A_num <= 12 ~ "$25,000--49,999",
                       SC214A_num <= 15 ~ "$50,000--99,999",
                       SC214A_num <= 17 ~ "$100,000--199,999",
                       SC214A_num <= 18 ~ "$200,000 or more"),
    Income = fct_reorder(Income, SC214A_num, .na_rm = FALSE),
    PlaceSize = case_match(as.numeric(as.character(V2126B)),
                           0 ~ "Not in a place",
                           13 ~ "Population under 10,000",
                           16 ~ "10,000--49,999",
                           17 ~ "50,000--99,999",
                           18 ~ "100,000--249,999",
                           19 ~ "250,000--499,999",
                           20 ~ "500,000--999,999",
                           c(21, 22, 23) ~ "1,000,000 or more"),
    PlaceSize = fct_reorder(PlaceSize, as.numeric(V2126B)),
    Region = case_match(as.numeric(V2127B),
                        1 ~ "Northeast",
                        2 ~ "Midwest",
                        3 ~ "South",
                        4 ~ "West"),
    Region = fct_reorder(Region, as.numeric(V2127B))
  )
```

As before, we want to check to make sure the recoded variables we create match the existing data as expected.

```{r}
#| label: ncvs-vign-hh-der-checks
hh_vsum_der %>% count(Tenure, V2015)
hh_vsum_der %>% count(Urbanicity, V2143)
hh_vsum_der %>% count(Income, SC214A)
hh_vsum_der %>% count(PlaceSize, V2126B)
hh_vsum_der %>% count(Region, V2127B)
```

#### Person variables

For the person file, we create categories for sex, race/Hispanic origin, age categories, and marital status. A codebook of the household variables is located in Table \@ref(tab:cb-pers). We also merge the household demographics to the person file as well as the design variables (`V2117` and `V2118`).

Table: (\#tab:cb-pers) Codebook for person variables

|Variable|Description|Value|Label| 
|---|---|---|---|
|V3014|Age||12--90
|V3015|Current Marital Status|1|Married|
|||2|Widowed|
|||3|Divorced|
|||4|Separated|
|||5|Never married|
|V3018|Sex|1|Male|
|||2|Female|
|V3023A|Race|01|White only|
|||02|Black only|
|||03|American Indian, Alaska native only|
|||04|Asian only|
|||05|Hawaiian/Pacific Islander only|
|||06|White-Black|
|||07|White-American Indian|
|||08|White-Asian|
|||09|White-Hawaiian|
|||10|Black-American Indian|
|||11|Black-Asian|
|||12|Black-Hawaiian/Pacific Islander|
|||13|American Indian-Asian|
|||14|Asian-Hawaiian/Pacific Islander|
|||15|White-Black-American Indian|
|||16|White-Black-Asian|
|||17|White-American Indian-Asian|
|||18|White-Asian-Hawaiian|
|||19|2 or 3 races|
|||20|4 or 5 races|
|V3024|Hispanic Origin|1|Yes|
|||2|No|

```{r}
#| label: ncvs-vign-pers-der
NHOPI <- "Native Hawaiian or Other Pacific Islander"

pers_vsum_der <- pers_vsum %>%
  mutate(
    Sex = factor(case_when(V3018 == 1 ~ "Male",
                           V3018 == 2 ~ "Female")),
    RaceHispOrigin = factor(case_when(V3024 == 1 ~ "Hispanic",
                                      V3023A == 1 ~ "White",
                                      V3023A == 2 ~ "Black",
                                      V3023A == 4 ~ "Asian",
                                      V3023A == 5 ~ NHOPI,
                                      TRUE ~ "Other"),
                            levels = c("White", "Black", "Hispanic", 
                                       "Asian", NHOPI, "Other")),
    V3014_num = as.numeric(as.character(V3014)),
    AgeGroup = case_when(V3014_num <= 17 ~ "12--17",
                         V3014_num <= 24 ~ "18--24",
                         V3014_num <= 34 ~ "25--34",
                         V3014_num <= 49 ~ "35--49",
                         V3014_num <= 64 ~ "50--64",
                         V3014_num <= 90 ~ "65 or older"),
    AgeGroup = fct_reorder(AgeGroup, V3014_num),
    MaritalStatus = factor(case_when(V3015 == 1 ~ "Married",
                                     V3015 == 2 ~ "Widowed",
                                     V3015 == 3 ~ "Divorced",
                                     V3015 == 4 ~ "Separated",
                                     V3015 == 5 ~ "Never married"),
                           levels = c("Never married", "Married", 
                                      "Widowed","Divorced", 
                                      "Separated"))
  ) %>% 
  left_join(hh_vsum_der %>% select(YEARQ, IDHH, 
                                   V2117, V2118, Tenure:Region),
            by = c("YEARQ", "IDHH"))
```

As before, we want to check to make sure the recoded variables we create match the existing data as expected.

```{r}
#| label: ncvs-vign-pers-der-checks
pers_vsum_der %>% count(Sex, V3018)
pers_vsum_der %>% count(RaceHispOrigin, V3024)
pers_vsum_der %>%
  filter(RaceHispOrigin != "Hispanic" | 
           is.na(RaceHispOrigin)) %>%
  count(RaceHispOrigin, V3023A)
pers_vsum_der %>% group_by(AgeGroup) %>%
  summarize(minAge = min(V3014),
            maxAge = max(V3014),
            .groups = "drop")
pers_vsum_der %>% count(MaritalStatus, V3015)
```

We then create tibbles that contain only the variables we need, which makes it easier to use them for analyses.

```{r}
#| label: ncvs-vign-hh-pers-slim
hh_vsum_slim <- hh_vsum_der %>%
  select(YEARQ:V2118,
         WGTVICCY:ADJINC_WT,
         Tenure,
         Urbanicity,
         Income,
         PlaceSize,
         Region)

pers_vsum_slim <- pers_vsum_der %>%
  select(YEARQ:WGTPERCY, WGTVICCY:ADJINC_WT, Sex:Region)
```

To calculate estimates about types of crime, such as what percentage of violent crimes are reported to the police, we must use the incident file. The incident file is not guaranteed to have every pseudo-stratum and half-sample code, so dummy records are created to append before estimation. Finally, we merge demographic variables onto the incident tibble.

```{r}
#| label: ncvs-vign-inc-analysis
dummy_records <- hh_vsum_slim %>%
  distinct(V2117, V2118) %>%
  mutate(Dummy = 1,
         WGTVICCY = 1,
         NEWWGT = 1)

inc_analysis <- inc_ind %>%
  mutate(Dummy = 0) %>%
  left_join(select(pers_vsum_slim, YEARQ, IDHH, IDPER, Sex:Region),
            by = c("YEARQ", "IDHH", "IDPER")) %>%
  bind_rows(dummy_records) %>%
  select(YEARQ:IDPER,
         WGTVICCY,
         NEWWGT,
         V4529,
         WeapCat,
         ReportPolice,
         Property:Region)
```

The tibbles `hh_vsum_slim`, `pers_vsum_slim`, and `inc_analysis` can now be used to create design objects and calculate crime rate estimates.

## Survey design objects

\index{Clustered sampling|(} \index{Stratified sampling|(} \index{Strata|(} \index{Primary sampling unit|(}
All the data preparation above is necessary to create the \index{Functions in srvyr!as\_survey\_design|(}design objects and finally begin analysis. We create three design objects for different types of analysis, depending on the estimate we are creating. For the incident data, the weight of analysis is `NEWWGT`, which we constructed previously. The household and person-level data use `WGTHHCY` and `WGTPERCY`, respectively. For all analyses, `V2117` is the strata variable, and `V2118` is the cluster/PSU variable for analysis. This information can be found in the User's Guide [@ncvs_user_guide].

```{r}
#| label: ncvs-vign-desobj

inc_des <- inc_analysis %>%
  as_survey_design(
    weight = NEWWGT,
    strata = V2117,
    ids = V2118,
    nest = TRUE
  )

hh_des <- hh_vsum_slim %>%
  as_survey_design(
    weight = WGTHHCY,
    strata = V2117,
    ids = V2118,
    nest = TRUE
  )

pers_des <- pers_vsum_slim %>%
  as_survey_design(
    weight = WGTPERCY,
    strata = V2117,
    ids = V2118,
    nest = TRUE
  )
```
\index{Functions in srvyr!as\_survey\_design|)} \index{Clustered sampling|)} \index{Stratified sampling|)} \index{Strata|)} \index{Primary sampling unit|)}

## Calculating estimates

Now that we have prepared our data and created the design objects, we can calculate our estimates. As a reminder, those are:

1. Victimization totals estimate the number of criminal victimizations with a given characteristic.

2. Victimization proportions estimate characteristics among victimizations or victims.

3. Victimization rates are estimates of the number of victimizations per 1,000 persons or households in the population.

4. Prevalence rates are estimates of the percentage of the population (persons or households) who are victims of a crime.

### Estimation 1: Victimization totals {#vic-tot}

There are two ways to calculate victimization totals. Using the incident design object (`inc_des`) is the most straightforward method, but the person (`pers_des`) and household (`hh_des`) design objects can be used as well if the adjustment factor (`ADJINC_WT`) is incorporated. In the example below, the total number of property and violent victimizations is first calculated using the incident file and then using the household and person design objects. The incident file is smaller, and thus, estimation is faster using that file, but the estimates are the same as illustrated in Table \@ref(tab:ncvs-vign-vt1), Table \@ref(tab:ncvs-vign-vt2a), and Table \@ref(tab:ncvs-vign-vt2b). \index{Functions in srvyr!survey\_total} \index{Functions in srvyr!summarize|(}

```{r}
#| label: ncvs-vign-victot-examp-calc
#| echo: false
#| warning: false
vt1df <- inc_des %>%
  summarize(
    Property_Vzn = survey_total(Property, na.rm = TRUE),
    Violent_Vzn = survey_total(Violent, na.rm = TRUE)
  )

vt2adf <- hh_des %>%
  summarize(Property_Vzn = survey_total(Property * ADJINC_WT,
    na.rm = TRUE
  ))

vt2bdf <- pers_des %>%
  summarize(Violent_Vzn = survey_total(Violent * ADJINC_WT,
    na.rm = TRUE
  ))
```



```{r}
#| label: ncvs-vign-victot-examp
vt1 <-
  inc_des %>%
  summarize(Property_Vzn = survey_total(Property, na.rm = TRUE),
            Violent_Vzn = survey_total(Violent, na.rm = TRUE)) %>%
  gt() %>%
  tab_spanner(
    label="Property Crime",
    columns=starts_with("Property")
  ) %>%
  tab_spanner(
    label="Violent Crime",
    columns=starts_with("Violent")
  ) %>%
  cols_label(
    ends_with("Vzn")~"Total",
    ends_with("se")~"S.E."
  ) %>%
  fmt_number(decimals=0)
  
vt2a <- hh_des %>%
  summarize(Property_Vzn = survey_total(Property * ADJINC_WT, 
                                        na.rm = TRUE)) %>%
  gt() %>%
    tab_spanner(
    label="Property Crime",
    columns=starts_with("Property")
  ) %>%
  cols_label(
    ends_with("Vzn")~"Total",
    ends_with("se")~"S.E."
  ) %>%
  fmt_number(decimals=0)

vt2b <- pers_des %>%
  summarize(Violent_Vzn = survey_total(Violent * ADJINC_WT, 
                                       na.rm = TRUE)) %>%
  gt() %>%
  tab_spanner(
    label="Violent Crime",
    columns=starts_with("Violent")
  ) %>%
  cols_label(
    ends_with("Vzn")~"Total",
    ends_with("se")~"S.E."
  ) %>%
  fmt_number(decimals=0)
```

(ref:ncvs-vign-vt1) Estimates of total property and violent victimizations with standard errors calculated using the incident design object, 2021 (vt1)

```{r}
#| label: ncvs-vign-vt1
#| echo: FALSE
#| warning: FALSE

vt1 %>%
    print_gt_book(knitr::opts_current$get()[["label"]])
```


(ref:ncvs-vign-vt2a) Estimates of total property victimizations with standard errors calculated using the household design object, 2021 (vt2a)

```{r}
#| label: ncvs-vign-vt2a
#| echo: FALSE
#| warning: FALSE

vt2a %>%
    print_gt_book(knitr::opts_current$get()[["label"]])
```


(ref:ncvs-vign-vt2b) Estimates of total violent victimizations with standard errors calculated using the person design object, 2021 (vt2b)

```{r}
#| label: ncvs-vign-vt2b
#| echo: FALSE
#| warning: FALSE

vt2b %>%
    print_gt_book(knitr::opts_current$get()[["label"]])
```
\index{Functions in srvyr!summarize|)}

The number of victimizations estimated using the incident file is equivalent to the person and household file method.  There were an estimated `r prettyNum(vt1df$Property_Vzn, big.mark=",")` property victimizations and `r prettyNum(vt1df$Violent_Vzn, big.mark=",")` violent victimizations in 2021.

### Estimation 2: Victimization proportions {#vic-prop}

Victimization proportions are proportions describing features of a victimization. The key here is that these are estimates among victimizations, not among the population. These types of estimates can only be calculated using the incident design object (`inc_des`). 

For example, we could be interested in the percentage of property victimizations reported to the police as shown in the following code with an estimate, the standard error, and 95% confidence interval: \index{Functions in srvyr!survey\_mean|(} \index{Functions in srvyr!filter|(} \index{Functions in srvyr!summarize|(}

```{r}
#| label: ncvs-vign-vic-prop-police
prop1 <- inc_des %>%
  filter(Property) %>%
  summarize(Pct = survey_mean(ReportPolice, 
                              na.rm = TRUE, 
                              proportion=TRUE, 
                              vartype=c("se", "ci")) * 100)

prop1
```

Or, the percentage of violent victimizations that are in urban areas: 

```{r}
#| label: ncvs-vign-vic-prop-urban
prop2 <- inc_des %>%
  filter(Violent) %>%
  summarize(Pct = survey_mean(Urbanicity=="Urban", 
                              na.rm = TRUE) * 100)

prop2
```
\index{Functions in srvyr!filter|)} \index{Functions in srvyr!survey\_mean|)}  

In 2021, we estimate that `r formatC(prop1$Pct, digits=1, format="f")`% of property crimes were reported to the police, and `r formatC(prop2$Pct, digits=1, format="f")`% of violent crimes occurred in urban areas.

### Estimation 3: Victimization rates {#vic-rate}

Victimization rates measure the number of victimizations per population. They are not an estimate of the proportion of households or persons who are victimized, which is the prevalence rate described in Section \@ref(prev-rate). Victimization rates are estimated using the household (`hh_des`) or person (`pers_des`) design objects depending on the type of crime, and the adjustment factor (`ADJINC_WT`) must be incorporated. We return to the example of property and violent victimizations used in the example for victimization totals (Section \@ref(vic-tot)). In the following example, the property victimization totals are calculated as above, as well as the property victimization rate (using `survey_mean()`) and the population size using `survey_total()`. 

Victimization rates use the incident weight in the numerator and the person or household weight in the denominator. This is accomplished by calculating the rates with the weight adjustment (`ADJINC_WT`) multiplied by the estimate of interest. Let's look at an example of property victimization. \index{Functions in srvyr!survey\_total} \index{Functions in srvyr!survey\_mean|(}

```{r}
#| label: ncvs-vign-vic-rate
vr_prop <- hh_des %>%
  summarize(
    Property_Vzn = survey_total(Property * ADJINC_WT, 
                                na.rm = TRUE),
    Property_Rate = survey_mean(Property * ADJINC_WT * 1000,
                                na.rm = TRUE),
    PopSize = survey_total(1, vartype = NULL)
  )

vr_prop
```
\index{Functions in srvyr!survey\_mean|)}  

In the output above, we see the estimate for property victimization rate in 2021 was `r formatC(vr_prop$Property_Rate, format="f", digits=1)` per 1,000 households. This is consistent with calculating the number of victimizations per 1,000 population, as demonstrated in the following code output.

```{r}
#| label: ncvs-vign-vic-rate-2

vr_prop %>%
  select(-ends_with("se")) %>%
  mutate(Property_Rate_manual=Property_Vzn/PopSize*1000)
```

Victimization rates can also be calculated based on particular characteristics of the victimization. In the following example, we calculate the rate of aggravated assault with no weapon, firearm, knife, and another weapon.
\index{Functions in srvyr!survey\_mean|(} 

```{r}
#| label: ncvs-vign-pers-rates-char
pers_des %>%
  summarize(across(
    starts_with("AAST_"),
    ~ survey_mean(. * ADJINC_WT * 1000, na.rm = TRUE)
  ))
```

A common desire is to calculate victimization rates by several characteristics. For example, we may want to calculate the violent victimization rate and aggravated assault rate by sex, race/Hispanic origin, age group, marital status, and household income. This requires a separate `group_by()` statement for each categorization. Thus, we make a function to do this and then use the `map_df()` function from the {purrr} package to loop through the variables [@R-purrr]. This function takes a demographic variable as its input (`byarvar`) and calculates the violent and aggravated assault victimization rate for each level. It then creates some columns with the variable, the level of each variable, and a numeric version of the variable (`LevelNum`) for sorting later. The function is run across multiple variables using `map()` and then stacks the results into a single output using `bind_rows()`.  \index{Functions in srvyr!filter|(} 

```{r}
#| label: ncvs-vign-rates-demo
pers_est_by <- function(byvar) {
  pers_des %>%
    rename(Level := {{byvar}}) %>%
    filter(!is.na(Level)) %>%
    group_by(Level) %>%
    summarize(
      Violent = survey_mean(Violent * ADJINC_WT * 1000, na.rm = TRUE),
      AAST = survey_mean(AAST * ADJINC_WT * 1000, na.rm = TRUE)
    ) %>%
    mutate(
      Variable = byvar,
      LevelNum = as.numeric(Level),
      Level = as.character(Level)
    ) %>%
    select(Variable, Level, LevelNum, everything())
}

pers_est_df <-
  c("Sex", "RaceHispOrigin", "AgeGroup", "MaritalStatus", "Income") %>%
  map(pers_est_by) %>%
  bind_rows()
```
\index{Functions in srvyr!filter|)} \index{Functions in srvyr!survey\_mean|)}

\index{gt package|(} 
The output from all the estimates is cleaned to create better labels, such as going from "RaceHispOrigin" to "Race/Hispanic Origin." Finally, the {gt} package is used to make a publishable table (Table \@ref(tab:ncvs-vign-rates-demo-tab)). Using the functions from the {gt} package, we add column labels and footnotes and present estimates rounded to the first decimal place [@R-gt].

```{r}
#| label: ncvs-vgn-rates-demo-gt-create

vr_gt<-pers_est_df %>%
  mutate(
    Variable = case_when(
      Variable == "RaceHispOrigin" ~ "Race/Hispanic Origin",
      Variable == "MaritalStatus" ~ "Marital Status",
      Variable == "AgeGroup" ~ "Age",
      TRUE ~ Variable
    )
  ) %>%
  select(-LevelNum) %>%
  group_by(Variable) %>%
  gt(rowname_col = "Level") %>%
  tab_spanner(
    label = "Violent Crime",
    id = "viol_span",
    columns = c("Violent", "Violent_se")
  ) %>%
  tab_spanner(label = "Aggravated Assault",
              columns = c("AAST", "AAST_se")) %>%
  cols_label(
    Violent = "Rate",
    Violent_se = "S.E.",
    AAST = "Rate",
    AAST_se = "S.E.",
  ) %>%
  fmt_number(
    columns = c("Violent", "Violent_se", "AAST", "AAST_se"),
    decimals = 1
  ) %>%
  tab_footnote(
    footnote = "Includes rape or sexual assault, robbery,
    aggravated assault, and simple assault.",
    locations = cells_column_spanners(spanners = "viol_span")
  ) %>%
  tab_footnote(
    footnote = "Excludes persons of Hispanic origin.",
    locations =
      cells_stub(rows = Level %in%
                   c("White", "Black", "Asian", NHOPI, "Other"))) %>%
  tab_footnote(
    footnote = "Includes persons who identified as
    Native Hawaiian or Other Pacific Islander only.",
    locations = cells_stub(rows = Level == NHOPI)
  ) %>%
  tab_footnote(
    footnote = "Includes persons who identified as American Indian or
    Alaska Native only or as two or more races.",
    locations = cells_stub(rows = Level == "Other")
  ) %>%
  tab_source_note(
    source_note = md("*Note*: Rates per 1,000 persons age 12 or older.")
  ) %>%
  tab_source_note(
    source_note = md("*Source*: Bureau of Justice Statistics,
                     National Crime Victimization Survey, 2021.")
  ) %>%
  tab_stubhead(label = "Victim Demographic") %>%
  tab_caption("Rate and standard error of violent victimization,
              by type of crime and demographic characteristics, 2021")
```



```{r}
#| label: ncvs-vign-rates-demo-noeval
#| eval: false
vr_gt
```

(ref:ncvs-vign-rates-demo-tab) Rate and standard error of violent victimization, by type of crime and demographic characteristics, 2021

```{r}
#| label: ncvs-vign-rates-demo-tab
#| echo: FALSE
#| warning: FALSE

vr_gt %>%
    print_gt_book(knitr::opts_current$get()[["label"]])
```

\index{gt package|)} 

### Estimation 4: Prevalence rates {#prev-rate}

Prevalence rates differ from victimization rates, as the numerator is the number of people or households victimized rather than the number of victimizations. To calculate the prevalence rates, we must run another summary of the data by calculating an indicator for whether a person or household is a victim of a particular crime at any point in the year. Below is an example of calculating the indicator and then the prevalence rate of violent crime and aggravated assault. \index{Functions in srvyr!survey\_mean|(} 

```{r}
#| label: ncvs-vign-prevexamp

pers_prev_des <-
  pers_vsum_slim %>%
  mutate(Year = floor(YEARQ)) %>%
  mutate(Violent_Ind = sum(Violent) > 0,
         AAST_Ind = sum(AAST) > 0,
         .by = c("Year", "IDHH", "IDPER")) %>%
  as_survey(
    weight = WGTPERCY,
    strata = V2117,
    ids = V2118,
    nest = TRUE
  )

pers_prev_ests <- pers_prev_des %>%
  summarize(Violent_Prev = survey_mean(Violent_Ind * 100),
            AAST_Prev = survey_mean(AAST_Ind * 100))

pers_prev_ests
```
\index{Functions in srvyr!survey\_mean|)} 

In the example above, the indicator is multiplied by 100 to return a percentage rather than a proportion. In 2021, we estimate that `r formatC(pers_prev_ests$Violent_Prev, digits=2, format="f")`% of people aged 12 and older were victims of violent crime in the United States, and `r formatC(pers_prev_ests$AAST_Prev, digits=2, format="f")`% were victims of aggravated assault.

## Statistical testing

\index{Statistical testing|(} \index{t-test|(}
For any of the types of estimates discussed, we can also perform statistical testing. For example, we could test whether property victimization rates are different between properties that are owned versus rented. First, we calculate the point estimates. \index{Functions in srvyr!survey\_mean|(} 

```{r}
#| label: ncvs-vgn-prop-pt-estimates
prop_tenure <- hh_des %>%
  group_by(Tenure) %>%
  summarize(
    Property_Rate = survey_mean(Property * ADJINC_WT * 1000,
                                na.rm = TRUE, vartype="ci"),
  )

prop_tenure  
```
\index{Functions in srvyr!summarize|)} \index{Functions in srvyr!survey\_mean|)} \index{t-test!two-sample t-test|(} \index{t-test!unpaired two-sample t-test|(}

The property victimization rate for rented households is `r prop_tenure %>% filter(Tenure=="Rented") %>% pull(Property_Rate) %>% round(1)` per 1,000 households, while the property victimization rate for owned households is `r prop_tenure %>% filter(Tenure=="Owned") %>% pull(Property_Rate) %>% round(1)`, which seem very different, especially given the non-overlapping confidence intervals. However, survey data are inherently non-independent, so statistical testing cannot be done by comparing confidence intervals. \index{Functions in survey!svyttest|(}To conduct the statistical test, we first need to create a variable that incorporates the adjusted incident weight (`ADJINC_WT`), and then the test can be conducted on this adjusted variable as discussed in Chapter \@ref(c06-statistical-testing). 

```{r}
#| label: ncvs-vign-prop-stat-test
prop_tenure_test <- hh_des %>%
  mutate(
    Prop_Adj=Property * ADJINC_WT * 1000
  ) %>%
  svyttest(
    formula = Prop_Adj ~ Tenure,
    design = .,
    na.rm = TRUE
  ) %>%
  broom::tidy()
```

```{r}
#| label: ncvs-vign-prop-stat-test-gt
#| eval: FALSE
prop_tenure_test %>%
  mutate(p.value = pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number()
```

(ref:ncvs-vign-prop-stat-test-gt-tab)  T-test output for estimates of property victimization rates between properties that are owned versus rented, NCVS 2021

```{r}
#| label: ncvs-vign-prop-stat-test-gt-tab
#| echo: FALSE
#| warning: FALSE

prop_tenure_test %>%
  mutate(p.value = pretty_p_value(p.value)) %>%
  gt() %>%
  fmt_number() %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

\index{p-value|(} 
The output of the statistical test shown in Table \@ref(tab:ncvs-vign-prop-stat-test-gt-tab) indicates a difference of `r prop_tenure_test$estimate %>% round(1)` between the property victimization rates of renters and owners, and the test is highly significant with the p-value of `r prettyunits::pretty_p_value(prop_tenure_test$p.value)`. \index{Functions in survey!svyttest|)} \index{Statistical testing|)} \index{p-value|)} \index{t-test|)} \index{t-test!two-sample t-test|)} \index{t-test!unpaired two-sample t-test|)}

## Exercises

1. What proportion of completed motor vehicle thefts are not reported to the police? Hint: Use the codebook to look at the definition of Type of Crime (V4529).

2. How many violent crimes occur in each region?

3. What is the property victimization rate among each income level?

4. What is the difference between the violent victimization rate between males and females? Is it statistically different?

\index{National Crime Victimization Survey (NCVS)|)}

<!--chapter:end:13-ncvs-vignette.Rmd-->

# AmericasBarometer vignette {#c14-ambarom-vignette}

\index{AmericasBarometer|(} \index{LAPOP|see {AmericasBarometer}}

```{r}
#| label: ambarom-styler
#| include: false
knitr::opts_chunk$set(tidy = 'styler')
```

::: {.prereqbox-header}
`r if (knitr:::is_html_output()) '### Prerequisites {- #prereq10}'`
:::

::: {.prereqbox data-latex="{Prerequisites}"}
For this chapter, load the following packages:

```{r}
#| label: ambarom-setup
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(survey)
library(srvyr)
library(sf)
library(rnaturalearth)
library(rnaturalearthdata)
library(gt)
library(ggpattern)
```

This vignette uses a subset of data from the 2021 AmericasBarometer survey. Download the raw files, available on the [LAPOP website](http://datasets.americasbarometer.org/database/index.php). We work with version 1.2 of the data, and there are separate files for each of the 22 countries. To import all files into R while ignoring the Stata labels, we recommend running the following code using the `read_stata()` function from the {haven} package [@R-haven]:

```r
stata_files <- list.files(here("RawData", "LAPOP_2021"), "*.dta")

read_stata_unlabeled <- function(file) {
  read_stata(file) %>%
    zap_labels() %>%
    zap_label()
}

ambarom_in <- here("RawData", "LAPOP_2021", stata_files) %>%
  map_df(read_stata_unlabeled) %>%
  select(pais, strata, upm, weight1500, strata, core_a_core_b,
         q2, q1tb, covid2at, a4, idio2, idio2cov, it1, jc13,
         m1, mil10a, mil10e, ccch1, ccch3, ccus1, ccus3,
         edr, ocup4a, q14, q11n, q12c, q12bn,
         starts_with("covidedu1"), gi0n,
         r15, r18n, r18) 
```

The code above reads all the `.dta` files and combines them into one tibble.
:::

## Introduction

The AmericasBarometer surveys, conducted by the LAPOP Lab [@lapop], are public opinion surveys of the Americas focused on democracy. The study was launched in 2004/2005 with 11 countries. Though the participating countries change over time, AmericasBarometer maintains a consistent methodology across many of them. In 2021, the study included 22 countries ranging from Canada in the north to Chile and Argentina in the south [@lapop-about].

Historically, surveys were administered through in-person household interviews, but the COVID-19 pandemic changed the study significantly. Now, random-digit dialing (RDD) of mobile phones is used in all countries except the United States and Canada [@lapop-tech]. In Canada, LAPOP collaborated with the Environics Institute to collect data from a panel of Canadians using a web survey [@lapop-can]. In the United States, YouGov conducted a web survey on behalf of LAPOP among its panelists [@lapop-usa].

The survey includes a core set of questions for all countries, but not every question is asked in each country. Additionally, some questions are only posed to half of the respondents in a country, with different randomized sections [@lapop-svy]. 

## Data structure

Each country and year has its own file available in Stata format (`.dta`). In this vignette, we download and combine all the data from the 22 participating countries in 2021. We subset the data to a smaller set of columns, as noted in the Prerequisites box. We recommend reviewing the core questionnaire to understand the common variables across the countries [@lapop-svy]. 

## Preparing files

Many of the variables are coded as numeric and do not have intuitive variable names, so the next step is to create derived variables and wrangle the data for analysis. \index{Codebook|(} Using the core questionnaire as a codebook, we reference the factor descriptions to create derived variables with informative names:\index{Codebook|)} 

```{r}
#| label: ambarom-read-secret
#| include: FALSE
#| cache: TRUE
#| message: FALSE
library(osfr)
osf_auth(Sys.getenv("OSF_PAT"))

lapop_rds_files <- osf_retrieve_node("https://osf.io/z5c3m/") %>%
  osf_ls_files(path = "LAPOP_2021",
               n_max = 40,
               pattern = ".rds")

filedet <- lapop_rds_files %>%
  osf_download(conflicts = "overwrite")

ambarom_in <- filedet %>%
  pull(local_path) %>%
  read_rds()

unlink(pull(filedet, "local_path"))
```

```{r}
#| label: ambarom-derive
ambarom <- ambarom_in %>%
  mutate(
    Country = factor(
      case_match(pais,
                 1 ~ "Mexico",
                 2 ~ "Guatemala",
                 3 ~ "El Salvador",
                 4 ~ "Honduras",
                 5 ~ "Nicaragua",
                 6 ~ "Costa Rica",
                 7 ~ "Panama",
                 8 ~ "Colombia",
                 9 ~ "Ecuador",
                 10 ~ "Bolivia",
                 11 ~ "Peru",
                 12 ~ "Paraguay",
                 13 ~ "Chile",
                 14 ~ "Uruguay",
                 15 ~ "Brazil",
                 17 ~ "Argentina",
                 21 ~ "Dominican Republic",
                 22 ~ "Haiti",
                 23 ~ "Jamaica",
                 24 ~ "Guyana",
                 40 ~ "United States",
                 41 ~ "Canada")),
    CovidWorry = fct_reorder(
      case_match(covid2at,
                 1 ~ "Very worried",
                 2 ~ "Somewhat worried",
                 3 ~ "A little worried",
                 4 ~ "Not worried at all"),
      covid2at,
      .na_rm = FALSE)
  ) %>%
  rename(Educ_NotInSchool = covidedu1_1,
         Educ_NormalSchool = covidedu1_2,
         Educ_VirtualSchool = covidedu1_3,
         Educ_Hybrid = covidedu1_4,
         Educ_NoSchool = covidedu1_5,
         BroadbandInternet = r18n,
         Internet = r18)
```

At this point, it is a good time to check the cross-tabs between the original and newly derived variables. These tables help us confirm that we have correctly matched the numeric data from the original dataset to the renamed factor data in the new dataset. For instance, let's check the original variable `pais` and the derived variable `Country`. \index{Codebook|(} We can consult the questionnaire or codebook to confirm that Argentina is coded as `17`, Bolivia as `10`, etc. Similarly, for `CovidWorry` and `covid2at`, we can verify that `Very worried` is coded as `1`, and so on for the other variables.\index{Codebook|)} 

```{r}
#| label: ambarom-derive-check
ambarom %>%
  count(Country, pais) %>%
  print(n = 22)

ambarom %>%
  count(CovidWorry, covid2at)
```

## Survey design objects

\index{Clustered sampling|(} \index{Stratified sampling|(} \index{Strata|(} \index{Primary sampling unit|(}
The technical report is the best reference for understanding how to specify the sampling design in R [@lapop-tech]. The data include two weights: `wt` and `weight1500`. The first weight variable is specific to each country and sums to the sample size, but it is calibrated to reflect each country's demographics. The second weight variable sums to 1500 for each country and is recommended for multi-country analyses. \index{Functions in srvyr!as\_survey\_design|(} Although not explicitly stated in the documentation, the Stata syntax example (`svyset upm [pw=weight1500], strata(strata)`) indicates the variable `upm` is a clustering variable, and `strata` is the strata variable. Therefore, the design object for multi-country analysis is created in R as follows: 

```{r}
#| label: ambarom-design
ambarom_des <- ambarom %>%
  as_survey_design(ids = upm,
                   strata = strata,
                   weight = weight1500)
```
\index{Functions in srvyr!as\_survey\_design|)} \index{Clustered sampling|)} \index{Stratified sampling|)} \index{Strata|)} \index{Primary sampling unit|)}

One interesting thing to note is that these weight variables can provide estimates for comparing countries but not for multi-country estimates. This is due to the fact that the weights do not account for the different sizes of countries. For example, Canada has about 10% of the population of the United States, but an estimate that uses records from both countries would weigh them equally.

## Calculating estimates {#ambarom-estimates}

When calculating estimates from the data, we use the survey design object `ambarom_des` and then apply the \index{Functions in srvyr!survey\_mean} `survey_mean()` function. The next sections walk through a few examples. 

### Example: Worry about COVID-19

This survey was administered between March and August 2021, with the specific timing varying by country^[See table 2 in @lapop-tech for dates by country]. Given the state of the pandemic at that time, several questions about COVID-19 were included. According to the core questionnaire [@lapop-svy], the first question asked about COVID-19 was:

> How worried are you about the possibility that you or someone in your household will get sick from coronavirus in the next 3 months?
>
> |   - Very worried
> |   - Somewhat worried
> |   - A little worried
> |   - Not worried at all

If we are interested in those who are very worried or somewhat worried, we can create a new variable (`CovidWorry_bin`) that groups levels of the original question using the `fct_collapse()` function from the {forcats} package [@R-forcats]. We then use the `survey_count()` function to understand how responses are distributed across each category of the original variable (`CovidWorry`) and the new variable (`CovidWorry_bin`). \index{Functions in srvyr!survey\_count|(}

```{r}
#| label: ambarom-worry-est1
covid_worry_collapse <- ambarom_des %>%
  mutate(CovidWorry_bin = fct_collapse(
    CovidWorry,
    WorriedHi = c("Very worried", "Somewhat worried"),
    WorriedLo = c("A little worried", "Not worried at all")
  ))

covid_worry_collapse %>%
  survey_count(CovidWorry_bin, CovidWorry)
```
\index{Functions in srvyr!survey\_count|)}

With this new variable, we can now use `survey_mean()` to calculate the percentage of people in each country who are either very or somewhat worried about COVID-19. \index{Missing data|(}There are missing data, as indicated in the `survey_count()` output above, so we need to use `na.rm = TRUE` in the `survey_mean()` function to handle the missing values. \index{Functions in srvyr!survey\_mean|(} \index{Functions in srvyr!summarize|(} \index{Missing data|)}

```{r}
#| label: ambarom-worry-est2
covid_worry_country_ests <- covid_worry_collapse %>%
  group_by(Country) %>%
  summarize(p = survey_mean(CovidWorry_bin == "WorriedHi",
                            na.rm = TRUE) * 100)

covid_worry_country_ests
```
\index{Functions in srvyr!summarize|)} \index{Functions in srvyr!survey\_mean|)}

To view the results for all countries, we can use the {gt} package to create Table \@ref(tab:ambarom-worry-tab) [@R-gt].

```{r}
#| label: ambarom-worry-gt
covid_worry_country_ests_gt <- covid_worry_country_ests %>%
  gt(rowname_col = "Country") %>%
  cols_label(p = "%",
             p_se = "S.E.") %>%
  fmt_number(decimals = 1) %>%
  tab_source_note(md("*Source*: AmericasBarometer Surveys, 2021"))
```

```{r}
#| label: ambarom-worry-noeval
#| eval: false
covid_worry_country_ests_gt
```

(ref:ambarom-worry-tab) Percentage worried about the possibility that they or someone in their household will get sick from coronavirus in the next 3 months

```{r}
#| label: ambarom-worry-tab
#| echo: FALSE
#| warning: FALSE

covid_worry_country_ests_gt %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

### Example: Education affected by COVID-19

In the core questionnaire [@lapop-svy], respondents were also asked a question about how the pandemic affected education. This question was asked to households with children under the age of 13, and respondents could select more than one option, as follows:

> Did any of these children have their school education affected due to the pandemic?
> 
> |   - No, because they are not yet school age or because they do not attend school for another reason
> |   - No, their classes continued normally
> |   - Yes, they went to virtual or remote classes
> |   - Yes, they switched to a combination of virtual and in-person classes
> |   - Yes, they cut all ties with the school

Working with multiple-choice questions can be both challenging and interesting. Let's walk through how to analyze this question. If we are interested in the impact on education, we should focus on the data of those whose children are attending school. This means we need to exclude those who selected the first response option: "No, because they are not yet school age or because they do not attend school for another reason." To do this, we use the `Educ_NotInSchool` variable in the dataset, which has values of `0` and `1`. A value of `1` indicates that the respondent chose the first response option (none of the children are in school), and a value of `0` means that at least one of their children is in school. By filtering the data to those with a value of `0` (they have at least one child in school), we can consider only respondents with at least one child attending school.

Now, let's review the data for those who selected one of the next three response options: 

- No, their classes continued normally: `Educ_NormalSchool`
- Yes, they went to virtual or remote classes: `Educ_VirtualSchool`
- Yes, they switched to a combination of virtual and in-person classes: `Educ_Hybrid`
 
The unweighted cross-tab for these responses is included below. It reveals a wide range of impacts, where many combinations of effects on education are possible.

```{r}
#| label: ambarom-covid-ed-skip
ambarom %>%
  filter(Educ_NotInSchool == 0) %>%
  count(Educ_NormalSchool,
        Educ_VirtualSchool,
        Educ_Hybrid)
```

In reviewing the survey question, we might be interested in knowing the answers to the following:

- What percentage of households indicated that school continued as normal with no virtual or hybrid option?
- What percentage of households indicated that the education medium was changed to either virtual or hybrid?
- What percentage of households indicated that they cut ties with their school?

To find the answers, we create indicators for the first two questions, make national estimates for all three questions, and then construct a summary table for easy viewing. First, we create and inspect the indicators and their distributions using `survey_count()`. \index{Functions in srvyr!survey\_count|(}

```{r}
#| label: ambarom-covid-ed-inds
ambarom_des_educ <- ambarom_des %>%
  filter(Educ_NotInSchool == 0) %>%
  mutate(
    Educ_OnlyNormal = (Educ_NormalSchool == 1 &
                         Educ_VirtualSchool == 0 &
                         Educ_Hybrid == 0),
    Educ_MediumChange = (Educ_VirtualSchool == 1 |
                           Educ_Hybrid == 1)
  )

ambarom_des_educ %>%
  survey_count(Educ_OnlyNormal,
               Educ_NormalSchool,
               Educ_VirtualSchool,
               Educ_Hybrid)

ambarom_des_educ %>%
  survey_count(Educ_MediumChange,
               Educ_VirtualSchool,
               Educ_Hybrid)
```
\index{Functions in srvyr!survey\_count|)}

Next, we group the data by country and calculate the population estimates for our three questions. \index{Functions in srvyr!survey\_mean|(} \index{Functions in srvyr!summarize|(}

```{r}
#| label: ambarom-covid-ed-ests
covid_educ_ests <-
  ambarom_des_educ %>%
  group_by(Country) %>%
  summarize(
    p_onlynormal = survey_mean(Educ_OnlyNormal, na.rm = TRUE) * 100,
    p_mediumchange = survey_mean(Educ_MediumChange, na.rm = TRUE) * 100,
    p_noschool = survey_mean(Educ_NoSchool, na.rm = TRUE) * 100,
  ) 

covid_educ_ests
```
\index{Functions in srvyr!summarize|)} \index{Functions in srvyr!survey\_mean|)}

Finally, to view the results for all countries, we can use the {gt} package to construct Table \@ref(tab:ambarom-covid-ed-der-tab).

```{r}
#| label: ambarom-covid-ed-gt
covid_educ_ests_gt <- covid_educ_ests %>%
  gt(rowname_col = "Country") %>%
  cols_label(
    p_onlynormal = "%",
    p_onlynormal_se = "S.E.",
    p_mediumchange = "%",
    p_mediumchange_se = "S.E.",
    p_noschool = "%",
    p_noschool_se = "S.E."
  ) %>%
  tab_spanner(label = "Normal School Only",
              columns = c("p_onlynormal", "p_onlynormal_se")) %>%
  tab_spanner(label = "Medium Change",
              columns = c("p_mediumchange", "p_mediumchange_se")) %>%
  tab_spanner(label = "Cut Ties with School",
              columns = c("p_noschool", "p_noschool_se")) %>%
  fmt_number(decimals = 1) %>%
  tab_source_note(md("*Source*: AmericasBarometer Surveys, 2021"))
```

```{r}
#| label: ambarom-covid-ed-der-noeval
#| eval: false
covid_educ_ests_gt
```

(ref:ambarom-covid-ed-der-tab) Impact on education in households with children under the age of 13 who generally attend school

```{r}
#| label: ambarom-covid-ed-der-tab
#| echo: FALSE
#| warning: FALSE

covid_educ_ests_gt %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

In the countries that were asked this question, many households experienced a change in their child's education medium. However, in Haiti, only `r covid_educ_ests %>% filter(Country=="Haiti") %>% pull(p_mediumchange) %>% signif(.,2)`% of households with children switched to virtual or hybrid learning.

## Mapping survey data {#ambarom-maps}

While the table effectively presents the data, a map could also be insightful. To create a map of the countries, we can use the package {rnaturalearth} and subset North and South America with the `ne_countries()` function [@R-rnaturalearth]. The function returns a simple features (sf) object with many columns [@sf2023man], but most importantly, `soverignt` (sovereignty), `geounit` (country or territory), and `geometry` (the shape). For an example of the difference between sovereignty and country/territory, the United States, Puerto Rico, and the U.S. Virgin Islands are all separate units with the same sovereignty. A map without data is plotted in Figure \@ref(fig:ambarom-americas-map) using `geom_sf()` from the {ggplot2} package, which plots sf objects [@ggplot2wickham].

```{r}
#| label: ambarom-americas-map
#| fig.cap: "Map of North and South America"
#| fig.alt: "A blank map of the world, showing only the outlines of the countries in Western Hemisphere."

country_shape <-
  ne_countries(
    scale = "medium",
    returnclass = "sf",
    continent = c("North America", "South America")
  )

country_shape %>%
  ggplot() +
  geom_sf()
```

The map in Figure \@ref(fig:ambarom-americas-map) appears very wide due to the Aleutian Islands in Alaska extending into the Eastern Hemisphere. We can crop the shapefile to include only the Western Hemisphere using `st_crop()` from the {sf} package, which removes some of the trailing islands of Alaska.

```{r}
#| label: ambarom-update-map
#| warning: false
country_shape_crop <- country_shape %>%
  st_crop(c(xmin = -180,
            xmax = 0,
            ymin = -90,
            ymax = 90)) 
```

Now that we have the necessary shape files, our next step is to match our survey data to the map. Countries can be named differently (e.g., "U.S.", "U.S.A.", "United States"). To make sure we can visualize our survey data on the map, we need to match the country names in both the survey data and the map data. To do this, we can use the `anti_join()` function from the {dplyr} package to identify the countries in the survey data that are not in the map data. Table \@ref(tab:ambarom-map-merge-check-1-tab) shows the countries in the survey data but not the map data, and Table \@ref(tab:ambarom-map-merge-check-2-tab) shows the countries in the map data but not the survey data. As shown below, the United States is referred to as "United States" in the survey data but "United States of America" in the map data.

```{r}
#| label: ambarom-map-merge-check-1-gt
survey_country_list <- ambarom %>% distinct(Country)

survey_country_list_gt <- survey_country_list %>%
  anti_join(country_shape_crop, by = c("Country" = "geounit")) %>%
  gt()
```

```{r}
#| label: ambarom-map-merge-check-1-noeval
#| eval: false
survey_country_list_gt
```

(ref:ambarom-map-merge-check-1-tab) Countries in the survey data but not the map data

```{r}
#| label: ambarom-map-merge-check-1-tab
#| echo: FALSE
#| warning: FALSE

survey_country_list_gt %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

```{r}
#| label: ambarom-map-merge-check-2-gt
map_country_list_gt<-country_shape_crop %>% as_tibble() %>% 
  select(geounit, sovereignt) %>%
  anti_join(survey_country_list, by = c("geounit" = "Country")) %>%
  arrange(geounit) %>%
  gt()
```

```{r}
#| label: ambarom-map-merge-check-2-noeval
#| eval: false
map_country_list_gt
```

(ref:ambarom-map-merge-check-2-tab) Countries in the map data but not the survey data

```{r}
#| label: ambarom-map-merge-check-2-tab
#| echo: FALSE
#| warning: FALSE

map_country_list_gt %>%
  print_gt_book(knitr::opts_current$get()[["label"]])
```

There are several ways to fix the mismatched names for a successful join. The simplest solution is to rename the data in the shape object before merging. Since only one country name in the survey data differs from the map data, we rename the map data accordingly.

```{r}
#| label: ambarom-update-map-usa
country_shape_upd <- country_shape_crop %>%
  mutate(geounit = if_else(geounit == "United States of America", 
                           "United States", geounit))
```

Now that the country names match, we can merge the survey and map data and then plot the resulting dataset. We begin with the map file and merge it with the survey estimates generated in Section \@ref(ambarom-estimates) (`covid_worry_country_ests` and `covid_educ_ests`). We use the {dplyr} function of `full_join()`, which joins the rows in the map data and the survey estimates based on the columns `geounit` and `Country`. A full join keeps all the rows from both datasets, matching rows when possible. For any rows without matches, the function fills in an `NA` for the missing value [@sf2023man].

```{r}
#| label: ambarom-join-maps-ests
covid_sf <- country_shape_upd %>%
  full_join(covid_worry_country_ests, 
            by = c("geounit" = "Country")) %>%
  full_join(covid_educ_ests,
            by = c("geounit" = "Country"))
```

After the merge, we create two figures that display the population estimates for the percentage of people worried about COVID-19 (Figure \@ref(fig:ambarom-make-maps-covid)) and the percentage of households with at least one child participating in virtual or hybrid learning (Figure \@ref(fig:ambarom-make-maps-covid-ed)). We also add a crosshatch pattern to the countries without any data using the `geom_sf_pattern()` function from the {ggpattern} package [@R-ggpattern].

```{r}
#| label: ambarom-make-maps-covid
#| fig.cap: "Percentage of households by country worried someone in their household will get COVID-19 in the next 3 months"
#| fig.alt: "A choropleth map of the Western Hemisphere where the color scale filling in each country corresponds to the percentage of households worried someone in their household will get COVID-19 in the next 3 months. The bottom of the range is 30% and the top of the range is 80%. Brazil and Chile look like the countries with the highest percentage of worry, with North America showing a lower percentage of worry. Countries without data, such as Venezuela, are displayed with a hash pattern."


ggplot() +
  geom_sf(data = covid_sf,
          aes(fill = p, geometry = geometry),
          color = "darkgray") +
  scale_fill_gradientn(
    guide = "colorbar",
    name = "Percent",
    labels = scales::comma,
    colors = c("#BFD7EA", "#087e8b", "#0B3954"),
    na.value = NA
  ) +
  geom_sf_pattern(
    data = filter(covid_sf, is.na(p)),
    pattern = "crosshatch",
    pattern_fill = "lightgray",
    pattern_color = "lightgray",
    fill = NA,
    color = "darkgray"
  ) +
  theme_minimal()
```

```{r}
#| label: ambarom-make-maps-covid-ed
#| fig.cap: "Percentage of households by country who had at least one child participate in virtual or hybrid learning"
#| fig.alt: "A choropleth map of the Western Hemisphere where the color scale filling in each country corresponds to the percentage of households who had at least one child participate in virtual or hybrid learning. The bottom of the range is 20% and the top of the range is 100%. Most of North America is missing data and are filled in with a hash pattern. The countries with data show a high percentage of households who had at least one child participate in virtual or hybrid learning."

ggplot() +
  geom_sf(
    data = covid_sf,
    aes(fill = p_mediumchange, geometry = geometry),
    color = "darkgray"
  ) +
  scale_fill_gradientn(
    guide = "colorbar",
    name = "Percent",
    labels = scales::comma,
    colors = c("#BFD7EA", "#087e8b", "#0B3954"),
    na.value = NA
  ) +
  geom_sf_pattern(
    data = filter(covid_sf, is.na(p_mediumchange)),
    pattern = "crosshatch",
    pattern_fill = "lightgray",
    pattern_color = "lightgray",
    fill = NA,
    color = "darkgray"
  ) +
  theme_minimal()
```

\index{Missing data|(}
In Figure \@ref(fig:ambarom-make-maps-covid-ed), we observe missing data (represented by the crosshatch pattern) for Canada, Mexico, and the United States. The questionnaires indicate that these three countries did not include the education question in the survey. To focus on countries with available data, we can remove North America from the map and show only Central and South America. We do this below by restricting the shape files to Latin America and the Caribbean, as depicted in Figure \@ref(fig:ambarom-make-maps-covid-ed-c-s). \index{Missing data|)}

```{r}
#| label: ambarom-make-maps-covid-ed-c-s
#| fig.cap: "Percentage of  households who had at least one child participate in virtual or hybrid learning, in Central and South America"
#| fig.alt: "A choropleth map of Central and South America where the color scale filling in each country corresponds to the percentage of households who had at least one child participate in virtual or hybrid learning. The bottom of the range is 20% and the top of the range is 100%. Most of North America is missing data and are filled in with a hash pattern. The countries with data show a high percentage of households who had at least one child participate in virtual or hybrid learning."


covid_c_s <- covid_sf %>%
  filter(region_wb == "Latin America & Caribbean")

ggplot() +
  geom_sf(
    data = covid_c_s,
    aes(fill = p_mediumchange, geometry = geometry),
    color = "darkgray"
  ) +
  scale_fill_gradientn(
    guide = "colorbar",
    name = "Percent",
    labels = scales::comma,
    colors = c("#BFD7EA", "#087e8b", "#0B3954"),
    na.value = NA
  ) +
  geom_sf_pattern(
    data = filter(covid_c_s, is.na(p_mediumchange)),
    pattern = "crosshatch",
    pattern_fill = "lightgray",
    pattern_color = "lightgray",
    fill = NA,
    color = "darkgray"
  ) +
  theme_minimal()
```

In Figure \@ref(fig:ambarom-make-maps-covid-ed-c-s), we can see that most countries with available data have similar percentages (reflected in their similar shades). However, Haiti stands out with a lighter shade, indicating a considerably lower percentage of households with at least one child participating in virtual or hybrid learning.

## Exercises

1. Calculate the percentage of households with broadband internet and those with any internet at home, including from a phone or tablet in Latin America and the Caribbean. Hint: if there are countries with 0% internet usage, try filtering by something first.

2. Create a faceted map showing both broadband internet and any internet usage.

\index{AmericasBarometer|)}

<!--chapter:end:14-ambarom-vignette.Rmd-->

\cleardoublepage 

# (APPENDIX) Appendices {-}

# Importing survey data into R {#importing-survey-data-into-r}

```{r}
#| label: readr-styler
#| include: false
knitr::opts_chunk$set(tidy = 'styler')
```

To analyze a survey, we need to bring the survey data into R. This process is often referred to as importing, loading, or reading in data. Survey files come in different formats depending on the software used to create them. One of the many advantages of R is its flexibility in handling various data formats, regardless of their file extensions. Here are examples of common public-use survey file formats we may encounter:

* Delimiter-separated text files
* Excel spreadsheets in `.xls` or `.xlsx` format
* R native `.rda` files
* Stata datasets in `.dta` format
* SAS datasets in `.sas` format
* SPSS datasets in `.sav` format
* Application Programming Interfaces (APIs), often in JavaScript Object Notation (JSON) format
* Data stored in databases

This appendix guides analysts through the process of importing these various types of survey data into R.

## Importing delimiter-separated files into R

Delimiter-separated files use specific characters, known as delimiters, to separate values within the file. For example, CSV (comma-separated values) files use commas as delimiters, while TSV (tab-separated values) files use tabs. These file formats are widely used because of their simplicity and compatibility with various software applications.

The {readr} package, part of the tidyverse ecosystem, offers efficient ways to import delimiter-separated files into R [@R-readr]. It offers several advantages, including automatic data type detection and flexible handling of missing values, depending on one's survey analysis needs. The {readr} package includes functions for:

* `read_csv()`: This function is specifically designed to read CSV files.
* `read_tsv()`: Use this function for TSV files.
* `read_delim()`: This function can handle a broader range of delimiter-separated files, including CSV and TSV. Specify the delimiter using the `delim` argument.
* `read_fwf()`: This function is useful for importing fixed-width files (FWF), where columns have predetermined widths, and values are aligned in specific positions.
* `read_table()`: Use this function when dealing with whitespace-separated files, such as those with spaces or multiple spaces as delimiters.
* `read_log()`: This function can read and parse web log files.

The syntax for `read_csv()` is:

```
read_csv(
  file,
  col_names = TRUE,
  col_types = NULL,
  col_select = NULL,
  id = NULL,
  locale = default_locale(),
  na = c("", "NA"),
  comment = "",
  trim_ws = TRUE,
  skip = 0,
  n_max = Inf,
  guess_max = min(1000, n_max),
  name_repair = "unique",
  num_threads = readr_threads(),
  progress = show_progress(),
  show_col_types = should_show_types(),
  skip_empty_rows = TRUE,
  lazy = should_read_lazy()
)
```

The arguments are:

* `file`:  the path to the CSV file to import
* `col_names`: a value of `TRUE` imports the first row of the `file` as column names and not included in the data frame. A value of `FALSE` creates automated column names. Alternatively, we can provide a vector of column names.
* `col_types`: by default, R infers the column variable types. We can also provide a column specification using `list()` or `cols()`; for example, use `col_types = cols(.default = "c")` to read all the columns as characters. Alternatively, we can use a string to specify the variable types for each column.
* `col_select`: the columns to include in the results
* `id`: a column for storing the file path. This is useful for keeping track of the input file when importing multiple CSVs at a time.
* `locale`: the location-specific defaults for the file
* `na`: a character vector of values to interpret as missing
* `comment`:  a character vector of values to interpret as comments
* `trim_ws`: a value of `TRUE` trims leading and trailing white space
* `skip`: number of lines to skip before importing the data
* `n_max`: maximum number of lines to read
* `guess_max`: maximum number of lines used for guessing column types
* `name_repair`: whether to check column names. By default, the column names are unique.
* `num_threads`: the number of processing threads to use for initial parsing and lazy reading of data
* `progress`: a value of `TRUE` displays a progress bar
* `show_col_types`: a value of `TRUE` displays the column types
* `skip_empty_rows`: a value of `TRUE` ignores blank rows
* `lazy`: a value of `TRUE` reads values lazily

The other functions share a similar syntax to `read_csv()`. To find more details, run `??` followed by the function name. For example, run `??read_tsv` in the Console for additional information on importing TSV files.

In the example below, we use {readr} to import a CSV file named 'anes_timeseries_2020_csv_20220210.csv' into an R object called `anes_csv`. The `read_csv()` imports the file and stores the data in the `anes_csv` object. We can then use this object for further analysis.

```r
library(readr)

anes_csv <-
  read_csv(file = "data/anes_timeseries_2020_csv_20220210.csv")
```

## Importing Excel files into R

Excel, a widely used spreadsheet software program created by Microsoft, is a common file format in survey research. We can import Excel spreadsheets into the R environment using the {readxl} package. The package supports both the legacy `.xls` files and the modern `.xlsx` format. 

To import Excel data into R, we can use the `read_excel()` function from the {readxl} package. This function offers a range of  options for the import process. Let's explore the syntax:

```
read_excel(
  path,
  sheet = NULL,
  range = NULL,
  col_names = TRUE,
  col_types = NULL,
  na = "",
  trim_ws = TRUE,
  skip = 0,
  n_max = Inf,
  guess_max = min(1000, n_max),
  progress = readxl_progress(),
  .name_repair = "unique"
)
```

The arguments are:

* `path`: the path to the Excel file to import
* `sheet`: the name or index of the sheet (sometimes called tabs) within the Excel file
* `range`: the range of cells to import (for example, `P15:T87`)
* `col_names`: indicates whether the first row of the dataset contains column names
* `col_types`: specifies the data types of columns
* `na`: defines the representation of missing values (for example, `NULL`)
* `trim_ws`: controls whether leading and trailing whitespaces should be trimmed
* `skip` and `n_max`: enable skipping rows and limit the number of rows imported
* `guess_max`: sets the maximum number of rows used for data type guessing
* `progress`: specifies a progress bar for large imports
* `.name_repair`: determines how column names are repaired if they are not valid

In the code example below, we import an Excel spreadsheet named 'anes_timeseries_2020_csv_20220210.xlsx' into R. The resulting data is saved as a tibble in the `anes_excel` object, ready for further analysis.

```r
library(readxl)

anes_excel <-
  read_excel(path = "data/anes_timeseries_2020_csv_20220210.xlsx")
```

## Importing Stata, SAS, and SPSS files into R

The {haven} package, also from the tidyverse ecosystem, imports various proprietary data formats: Stata `.dta` files,  SPSS `.sav` files, and SAS `.sas7bdat` and `.sas7bcat` files [@R-haven]. One of the notable strengths of the {haven} package is its ability to handle multiple proprietary formats within a unified framework. It offers dedicated functions for each supported proprietary format, making it straightforward to import data regardless of the program. Here, we introduce `read_dat()` for Stata files, `read_sav()` for SPSS files, and `read_sas()` for SAS files.

### Syntax

Let's explore the syntax for importing Stata files `.dat` files using `haven::read_dat()`:

```r
read_dta(
  file,
  encoding = NULL,
  col_select = NULL,
  skip = 0,
  n_max = Inf,
  .name_repair = "unique"
)
```

The arguments are:

* `file`: the path to the proprietary data file to import
* `encoding`: specifies the character encoding of the data file
* `col_select`: selects specific columns for import
* `skip` and `n_max`: control the number of rows skipped and the maximum number of rows imported
* `.name_repair`: determines how column names are repaired if they are not valid

The syntax for `read_sav()` is similar to `read_dat()`:

```
read_sav(
  file,
  encoding = NULL,
  user_na = FALSE,
  col_select = NULL,
  skip = 0,
  n_max = Inf,
  .name_repair = "unique"
)
```

The arguments are:

* `file`: the path to the proprietary data file to import
* `encoding`: specifies the character encoding of the data file
* `col_select`: selects specific columns for import
* `user_na`: a value of `TRUE` reads variables with user-defined missing labels into `labelled_spss()` objects
* `skip` and `n_max`: control the number of rows skipped and the maximum number of rows imported
* `.name_repair`: determines how column names are repaired if they are not valid

The syntax for importing SAS files with `read_sas()` is as follows:

```r
read_sas(
  data_file,
  catalog_file = NULL,
  encoding = NULL,
  catalog_encoding = encoding,
  col_select = NULL,
  skip = 0L,
  n_max = Inf,
  .name_repair = "unique"
)
```

The arguments are:

* `data_file`: the path to the proprietary data file to import
* `catalog_file`: the path to the catalog file to import
* `encoding`: specifies the character encoding of the data file
* `catalog_encoding`: specifies the character encoding of the catalog file
* `col_select`: selects specific columns for import
* `skip` and `n_max`: control the number of rows skipped and the maximum number of rows imported
* `.name_repair`: determines how column names are repaired if they are not valid

In the code examples below, we demonstrate how to import Stata, SPSS, and SAS files into R using the respective {haven} functions. The resulting data are stored in `anes_dta`, `anes_sav`, and `anes_sas`  objects as tibbles, ready for use in R. For the Stata example, we show how to import the data from the {srvyrexploR} package to use in examples.

Stata: \index{American National Election Studies (ANES)|(}

```{r}
#| label: readr-stata
library(haven)

anes_dta <-
  read_dta(file = system.file("extdata",
                              "anes_2020_stata_example.dta",
                              package = "srvyrexploR"))
```

\index{American National Election Studies (ANES)|)}

SPSS:

```r
library(haven)

anes_sav <-
  read_sav(file = "data/anes_timeseries_2020_spss_20220210.sav")
```

SAS:

```r
library(haven)

anes_sas <-
  read_sas(
    data_file = "data/anes_timeseries_2020_sas_20220210.sas7bdat"
  )
```

### Working with labeled data

\index{American National Election Studies (ANES)|(} \index{Categorical data|(}
Stata, SPSS, and SAS files can contain labeled variables and values. These labels provide descriptive information about categorical data, making them easier to understand and analyze. When importing data from Stata, SPSS, or SAS, we want to preserve these labels to maintain data fidelity.

Consider a variable like 'Education Level' with coded values (e.g., 1, 2, 3). Without labels, these codes can be cryptic. However, with labels ('High School Graduate,' 'Bachelor's Degree,' 'Master's Degree'), the data become more informative and easier to work with.

With the {haven} package, we have the capability to import and work with labeled data from Stata, SPSS, and SAS files. The package uses a special class of data called `haven_labelled` to store labeled variables. When a dataset label is defined in Stata, it is stored in the 'label' attribute of the tibble when imported, ensuring that the information is not lost.

We can use functions like `select()`, `glimpse()`, and `is.labelled()` to inspect the imported data and verify if the variables are labeled. Take a look at the ANES Stata file. Notice that categorical variables `V200002` and `V201006` are marked with a type of `<dbl+lbl>`. This notation indicates that these variables are labeled.

```{r}
#| label: readr-glimpse
#| message: false
library(dplyr)

anes_dta %>%
  select(1:6) %>%
  glimpse()
```

We can confirm their label status using the `haven::is.labelled()` function.

```{r}
#| label: readr-islabelled
haven::is.labelled(anes_dta$V200002)
```

To explore the labels further, we can use the `attributes()` function. This function provides insights into both the variable labels (`$label`) and the associated value labels (`$labels`).

```{r}
#| label: readr-attributes
attributes(anes_dta$V200002)
```

When we import a labeled dataset using {haven}, it results in a tibble containing both the data and label information. However, this is meant to be an intermediary data structure and not intended to be the final data format for analysis. Instead, we should convert it into a regular R data frame before continuing our data workflow. There are two primary methods to achieve this conversion: (1) convert to factors or (2) remove the labels.

#### Option 1: Convert the vector into a factor {-}

\index{Factor|(}
Factors are native R data types for working with categorical data. They consist of integer values that correspond to character values, known as levels. Below is a dummy example of factors. The `factors` show the four different levels in the data: `strongly agree`, `agree`, `disagree`, and `strongly disagree`.

```{r}
#| label: readr-factor
response <- 
  c("strongly agree", "agree", "agree", "disagree", "strongly disagree")

response_levels <-
  c("strongly agree", "agree", "disagree", "strongly disagree")

factors <- factor(response, levels = response_levels)

factors
```

Factors are integer vectors, though they may look like character strings. We can confirm by looking at the vector's structure:

```{r}
#| label: readr-factor-view
glimpse(factors)
```

R's factors differ from Stata, SPSS, or SAS labeled vectors. However, we can convert labeled variables into factors using the `as_factor()` function.

```{r}
#| label: readr-factor-create
anes_dta %>%
  transmute(V200002 = as_factor(V200002))
```

The `as_factor()` function can be applied to all columns in a data frame or individual ones. Below, we convert all `<dbl+lbl>` columns into factors.

```{r}
#| label: readr-factor-glimpse
anes_dta_factor <-
  anes_dta %>%
  as_factor()

anes_dta_factor %>%
  select(1:6) %>%
  glimpse()
```

\index{Factor|)}

#### Option 2: Strip the labels {-}

The second option is to remove the labels altogether, converting the labeled data into a regular R data frame. To remove, or 'zap,' the labels from our tibble, we can use the {haven} package's `zap_label()` and `zap_labels()` functions. This approach removes the labels but retains the data values in their original form.

The ANES Stata file columns contain variable labels. Using the `map()` function from {purrr}, we can review the labels using `attr`. In the example below, we list the first two variables and their labels. For instance, the label for `V200002` is "Mode of interview: pre-election interview."

```{r}
#| label: readr-label-show
purrr::map(anes_dta, ~ attr(.x, "label")) %>%
  head(2)
```

Use `zap_label()` to remove the variable labels but retain the value labels. Notice that the labels return as `NULL`.

```{r}
#| label: readr-zaplabel
zap_label(anes_dta) %>%
  purrr::map( ~ attr(.x, "label")) %>%
  head(2)
```

To remove the value labels, use `zap_labels()`. Notice the previous `<dbl+lbl>` columns are now `<dbl>`.

```{r}
#| label: readr-zaplabels
zap_labels(anes_dta) %>%
  select(1:6) %>%
  glimpse()
```

While it is important to convert labeled datasets into regular R data frames for working in R, the labels themselves often contain valuable information that provides context and meaning to the survey variables. To aid with interpretability and documentation, we can create a data dictionary from the labeled dataset. A data dictionary is a reference document that provides detailed information about the variables and values of a survey.
\index{Categorical data|)}

The {labelled} package offers a convenient function, `generate_dictionary()`, that creates data dictionaries directly from a labeled dataset [@R-labelled]. This function extracts variable labels, value labels, and other metadata and organizes them into a structured document that we can browse and reference throughout our analysis. 

Let's create a data dictionary from the ANES Stata dataset as an example:

```{r}
#| label: readr-dictionary-create
library(labelled)

dictionary <- generate_dictionary(anes_dta)
```

Once we've generated the data dictionary, we can take a look at the `V200002` variable and see the label, column type, number of missing entries, and associated values.

```{r}
#| label: readr-dictionary-view
dictionary %>%
  filter(variable == "V200002")
```

\index{American National Election Studies (ANES)|)}

### Labeled missing data values

\index{Missing data|(}
In survey data analysis, dealing with missing values is a crucial aspect of data preparation. Stata, SPSS, and SAS files each have their own method for handling missing values.

* Stata has "extended" missing values, `.A` through `.Z`.
* SAS has "special" missing values, `.A` through `.Z` and `._`.
* SPSS has per-column "user" missing values. Each column can declare up to three distinct values or a range of values (plus one distinct value) that should be treated as missing.

SAS and Stata use a concept known as 'tagged' missing values, which extend R's regular `NA`. A 'tagged' missing value is essentially an `NA` with an additional single-character label. These values behave identically to regular `NA` in standard R operations while preserving the informative tag associated with the missing value.

Here is an example from the NORC at the University of Chicago’s 2018 General Society Survey, where Don't Know (`DK`) responses are tagged as `NA(d)`, Inapplicable (`IAP`) responses are tagged as `NA(i)`, and `No Answer` responses are tagged as `NA(n)` [@gss-codebook].

```r
head(gss_dta$HEALTH)
#> <labelled<double>[6]>: condition of health
#> [1]     2     1 NA(i) NA(i)     1     2
#> 
#> Labels:
#>  value     label
#>      1 excellent
#>      2      good
#>      3      fair
#>      4      poor
#>  NA(d)        DK
#>  NA(i)       IAP
#>  NA(n)        NA
```

In contrast, SPSS uses a different approach called 'user-defined values' to denote missing values. Each column in an SPSS dataset can have up to three distinct values designated as missing or a specified range of missing values. To model these additional user-defined missing values, {haven} provides the `labeled_spss()` subclass of `labeled()`. When importing SPSS data using {haven}, it ensures that user-defined missing values are correctly handled. We can work with these data in R while preserving the unique missing value conventions from SPSS.

Here is what the GSS SPSS dataset looks like when loaded with {haven}.

```
head(gss_sps$HEALTH)
#> <labelled_spss<double>[6]>: Condition of health
#> [1] 2 1 0 0 1 2
#> Missing values: 0, 8, 9
#> 
#> Labels:
#>  value     label
#>      0       IAP
#>      1 EXCELLENT
#>      2      GOOD
#>      3      FAIR
#>      4      POOR
#>      8        DK
#>      9        NA
```

\index{Missing data|)}

## Importing data from APIs into R

In addition to working with data saved as files, we may also need to retrieve data through Application Programming Interfaces (APIs). APIs provide a structured way to access data hosted on external servers and import them directly into R for analysis.

To access these data, we need to understand how to construct API requests. Each API has unique endpoints, parameters, and authentication requirements. Pay attention to:

* Endpoints: These are URLs that point to specific data or services
* Parameters: Information passed to the API to customize the request (e.g., date ranges, filters)
* Authentication: APIs may require API keys or tokens for access
* Rate Limits: APIs may have usage limits, so be aware of any rate limits or quotas

Typically, we begin by making a GET request to an API endpoint. The {httr2} package allows us to generate and process HTTP requests [@R-httr2]. We can make the GET request by pointing to the URL that contains the data we would like:

```r
library(httr2)

api_url <- "https://api.example.com/survey-data"
response <- GET(url = api_url)
```

Once we make the request, we obtain the data as the `response`. The data often come in JSON format. We can extract and parse the data using the {jsonlite} package, allowing us to work with them in R [@jsonliteooms]. The `fromJSON()` function, shown below, converts JSON data to an R object.

```r
survey_data <- fromJSON(content(response, "text"))
```

Note that these are dummy examples. Please review the documentation to understand how to make requests from a specific API.

R offers several packages that simplify API access by providing ready-to-use functions for popular APIs. These packages are called "wrappers," as they "wrap" the API in R to make it easier to use. For example, the {tidycensus} package used in this book simplifies access to U.S. Census data, allowing us to retrieve data with R commands instead of writing API requests from scratch [@R-tidycensus]. Behind the scenes, `get_pums()` is making a GET request from the Census API, and the {tidycensus} functions are converting the response into an R-friendly format. For example, if we are interested in the age, sex, race, and Hispanicity of those in the American Community Survey sample of Durham County, North Carolina^[The public use microdata areas (PUMA) for Durham County were identified using the 2020 PUMA Names File: https://www2.census.gov/geo/pdfs/reference/puma2020/2020_PUMA_Names.pdf], we can use the `get_pums()` function to extract the microdata as shown in the code below. We can then use the replicate weights to create a survey object and calculate estimates for Durham County.

```{r}
#| label: readr-pumsin
#| results: false
library(tidycensus)

durh_pums <- get_pums(
  variables = c("PUMA", "SEX", "AGEP", "RAC1P", "HISP"),
  state = "NC",
  puma = c("01301", "01302"),
  survey = "acs1",
  year = 2022,
  rep_weights = "person"
)
```

```{r}
#| label: readr-pumsprint

durh_pums
```

In Chapter \@ref(c04-getting-started), we used the {censusapi} package to get data from the Census data API for the Current Population Survey. To discover if there is an R package that directly interfaces with a specific survey or data source, search for "[survey] R wrapper" or "[data source] R package" online.

## Importing data from databases in R

Databases provide a secure and organized solution as the volume and complexity of data grow. We can access, manage, and update data stored in databases in a systematic way. Because of how the data are organized, teams can draw from the same source and obtain any metadata that would be helpful for analysis.

There are various ways of using R to work with databases. If using RStudio, we can connect to different databases through the Connections Pane in the top right of the IDE. We can also use packages like {DBI} and {odbc} to access database tables in R files. Here is an example script connecting to a database:

```r
con <- 
  DBI::dbConnect(
    odbc::odbc(),
    Driver    = "[driver name]",
    Server    = "[server path]",
    UID       = rstudioapi::askForPassword("Database user"),
    PWD       = rstudioapi::askForPassword("Database password"),
    Database  = "[database name]",
    Warehouse = "[warehouse name]",
    Schema    = "[schema name]"
  )
```

The {dbplyr} and {dplyr} packages allow us to make queries and run data analysis entirely using {dplyr} syntax. All of the code can be written in R, so we do not have to switch between R and SQL to explore the data. Here is some sample code:

```r
q1 <- tbl(con, "bank") %>%
  group_by(month_idx, year, month) %>%
  summarize(subscribe = sum(ifelse(term_deposit == "yes", 1, 0)),
            total = n())

show_query(q1)
```

Be sure to check the documentation to configure a database connection.

## Importing data from other formats

R also offers dedicated packages such as {googlesheets4} for Google Sheets or {qualtRics} for Qualtrics. With less common or proprietary file formats, the broader data science community can often provide guidance. Online resources like [Stack Overflow](https://stackoverflow.com/) and dedicated forums like [Posit Community](https://forum.posit.co/) are valuable sources of information for importing data into R.

<!--chapter:end:90-AppendixA-DataImport.Rmd-->

# ANES derived variable codebook {#anes-cb}

```{r}
#| label: anes-cb-setup
#| echo: FALSE
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(janitor)
library(kableExtra)
library(knitr)

data(anes_2020)

attrlist <- map(anes_2020, attributes)

NULL_to_NA <- function(x){
  if (is.null(x)){
    NA
  }else{
    x
  }
}

anes_var_info <- tibble(
  Vars=names(attrlist),
  Section=map_chr(attrlist, "Section") %>% unname(),
  Question=map(attrlist, "Question") %>% map(NULL_to_NA) %>% unlist(use.names = FALSE),
  Description=map_chr(attrlist, "label") %>% unname(),
  VarType=map(anes_2020, class) ,) %>%
  rowwise() %>%
  mutate(
    VarClass=str_c(VarType, collapse=", "),
    VarType=case_when(
      Vars%in%c("V200001", "CaseID")~ list("ID"),
      Vars=="V201507x"~list("numeric2"),
      TRUE~list(VarType)
    )
  ) %>%
  ungroup()

cb_count <- function(dat, var){
  t <- dat %>%
    count(.data[[var]]) %>%
    mutate(`Unweighted Freq` = n / sum(n)) %>%
    janitor::adorn_totals(where="row", fill="-", na.rm=TRUE, name="Total", n, `Unweighted Freq`) %>%
    mutate(`Unweighted Freq`= round(`Unweighted Freq`, 3)) %>%
    kbl(position="H")
  
   if (knitr:::is_html_output()){
    t %>% kable_minimal() %>% print()
  } else{
    t %>% print()
  }
}

cb_count_labelled <- function(dat, var){
  dat2 <- dat %>%
    mutate(
      Label=as_factor(.data[[var]], levels="labels"),
    ) %>% haven::zap_labels()
  
  
  t <- dat2 %>%
    count(.data[[var]], Label) %>%
    mutate(`Unweighted Freq` = n / sum(n)) %>%
    janitor::adorn_totals(where="row", fill="-", na.rm=TRUE, name="Total", n, `Unweighted Freq`) %>%
    mutate(`Unweighted Freq`= round(`Unweighted Freq`, 3)) %>%
    kbl(position="H")
  
   if (knitr:::is_html_output()){
    t %>% kable_minimal() %>% print()
  } else{
    t %>% print()
  }
}


cb_continuous <- function(dat, var){
  t <- dat %>%
    summarize(
      `N Missing`=sum(is.na(.data[[var]])),
      Minimum = min(.data[[var]], na.rm = TRUE),
      Median = median(.data[[var]], na.rm = TRUE),
      Maximum = max(.data[[var]], na.rm = TRUE)) %>%
    kbl(position="H") 
  
  if (knitr:::is_html_output()){
    t %>% kable_minimal() %>% print()
  } else{
    t %>% print()
  }
  
}

cb_continuous_spec <- function(dat, var){
  dat2 <- dat %>%
    haven::zap_labels()
  
  t_valid <- dat2 %>%
    filter(.data[[var]]>0) %>%
    summarize(
      `N Missing`=sum(is.na(.data[[var]])),
      Minimum = min(.data[[var]], na.rm = TRUE),
      Median = median(.data[[var]], na.rm = TRUE),
      Maximum = max(.data[[var]], na.rm = TRUE)) 
  
  t_ref <- dat2 %>%
    filter(.data[[var]]==-9)  %>%
    count(name="N Refused (-9)")
  
  t <- t_ref %>% bind_cols(t_valid) %>%
    select(`N Missing`, `N Refused (-9)`, everything()) %>%
    kbl(position="H") 
    
  if (knitr:::is_html_output()){
    t %>% kable_minimal() %>% print()
  } else{
    t %>% print()
  }
  
}


make_section <- function(sec){
  cat(str_c("## ", sec, "\n\n"))
  
  make_sum <- function(var){
    cat(str_c("#### ", var, " {-} \n\n"))
    vi <- anes_var_info %>% filter(Vars==var)
    de <- vi %>% pull(Description)
    cat(str_c("Description: ", de, "\n\n"))
    qt <- vi %>% pull(Question)
    if (!is.na(qt)) cat(str_c("Question text: ", qt, "\n\n"))
    vc <- vi %>% pull(VarClass)
    cat(str_c("Variable class: ", vc, "\n\n"))
    vt <- vi %>% pull(VarType) %>% unlist()
    
    if (any(c("factor", "character", "logical") %in% vt)){
      anes_2020 %>% cb_count(var)
      cat("\n")
    } else if ("haven_labelled" %in% vt){
      anes_2020 %>% cb_count_labelled(var)
      cat("\n")
    } else if ("numeric" %in% vt){
      anes_2020 %>% cb_continuous(var)
      cat("\n")
    } else if ("numeric2" %in% vt){
      anes_2020 %>% cb_continuous_spec(var)
      cat("\n")
    }
    
  }
  
  anes_var_info %>% filter(Section==sec) %>% pull(Vars) %>%
    walk(make_sum)
}

```

The full codebook with the original variables is available at @anes-cb.

This is a codebook for the ANES data used in this book (`anes_2020`) from the {srvyrexploR} package.


```{r}
#| label: anes-cb-write
#| echo: FALSE
#| results: asis
anes_var_info %>%
  distinct(Section) %>%
  pull(Section) %>%
  walk(make_section)
```

<!--chapter:end:91-AppendixB-ANES-CB.Rmd-->

# RECS derived variable codebook {#recs-cb}

```{r}
#| label: recs-cb-cb-setup
#| echo: FALSE
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(janitor)
library(kableExtra)
library(knitr)

data(recs_2020)
```

The full codebook with the original variables is available at [https://www.eia.gov/consumption/residential/data/2020/index.php?view=microdata](https://www.eia.gov/consumption/residential/data/2020/index.php?view=microdata) - "Variable and Response Codebook."

This is a codebook for the RECS data used in this book (`recs_2020`) from the {srvyrexploR} package.


```{r}
#| label: recs-cb-prep
#| echo: FALSE

attrlist <- map(recs_2020, attributes)

recs_var_info <- tibble(
  Vars=names(attrlist),
  Section=map_chr(attrlist, "Section") %>% unname(),
  Question=map(attrlist, "Question") %>% map(NULL_to_NA) %>% unlist(use.names = FALSE),
  Description=map_chr(attrlist, "label") %>% unname(),
  VarType=map(recs_2020, class) ,
) %>%
  mutate(
    VarType=if_else(Vars=="DOEID", list("ID"), VarType)
  )


cb_count <- function(dat, var){
  t <- dat %>%
    count(.data[[var]]) %>%
    mutate(`Unweighted Freq` = n / sum(n)) %>%
    janitor::adorn_totals(where="row", fill="-", na.rm=TRUE, name="Total", n, `Unweighted Freq`) %>%
    mutate(`Unweighted Freq`= round(`Unweighted Freq`, 3)) %>%
    kbl(position="H")
  
   if (knitr:::is_html_output()){
    t %>% kable_minimal() %>% print()
  } else{
    t %>% print()
  }
}

cb_continuous <- function(dat, var){
  t <- dat %>%
    summarize(
      `N Missing`=sum(is.na(.data[[var]])),
      Minimum = min(.data[[var]], na.rm = TRUE),
      Median = median(.data[[var]], na.rm = TRUE),
      Maximum = max(.data[[var]], na.rm = TRUE)) %>%
    kbl(position="H") 
  
  if (knitr:::is_html_output()){
    t %>% kable_minimal() %>% print()
  } else{
    t %>% print()
  }
  
}


make_section <- function(sec){
  cat(str_c("## ", sec, "\n\n"))
  
  make_sum <- function(var){
    cat(str_c("#### ", var, " {-} \n\n"))
    vi <- recs_var_info %>% filter(Vars==var)
    de <- vi %>% pull(Description)
    cat(str_c("Description: ", de, "\n\n"))
    qt <- vi %>% pull(Question)
    if (!is.na(qt)) cat(str_c("Question text: ", qt, "\n\n"))
    vt <- vi %>% pull(VarType) %>% unlist()
    
    if (any(c("factor", "character", "logical") %in% vt)){
      recs_2020 %>% cb_count(var)
      cat("\n")
    } else if ("numeric" %in% vt){
      recs_2020 %>% cb_continuous(var)
      cat("\n")
    }
    
  }
  
  recs_var_info %>% filter(Section==sec) %>% pull(Vars) %>%
    walk(make_sum)
}




```


```{r}
#| label: recs-cb-write
#| echo: FALSE
#| results: asis
recs_var_info %>%
  distinct(Section) %>%
  pull(Section) %>%
  walk(make_section)
```


<!--chapter:end:92-AppendixC-RECS-CB.Rmd-->

# Exercise solutions {#exercise-solutions}

```{r}
#| label: exercise-sol-styler
#| include: false
knitr::opts_chunk$set(tidy = 'styler')
```

The chapter exercises use the survey design objects and packages provided in the Prerequisites box in the beginning of the chapter. Please ensure they are loaded in the environment before running the exercise solutions. Code chunks to load these are also included below.


```r
library(tidyverse)
library(survey)
library(srvyr)
library(srvyrexploR)
library(broom)
library(prettyunits)
library(gt)
```

```r
targetpop <- 231592693

anes_adjwgt <- anes_2020 %>%
  mutate(Weight = Weight / sum(Weight) * targetpop)

anes_des <- anes_adjwgt %>%
  as_survey_design(
    weights = Weight,
    strata = Stratum,
    ids = VarUnit,
    nest = TRUE
  )
```

```r
recs_des <- recs_2020 %>%
  as_survey_rep(
    weights = NWEIGHT,
    repweights = NWEIGHT1:NWEIGHT60,
    type = "JK1",
    scale = 59/60,
    mse = TRUE
  )
```

```r
inc_series <- ncvs_2021_incident %>%
  mutate(
    series = case_when(V4017 %in% c(1, 8) ~ 1,
                       V4018 %in% c(2, 8) ~ 1,
                       V4019 %in% c(1, 8) ~ 1,
                       TRUE ~ 2
    ),
    n10v4016 = case_when(V4016 %in% c(997, 998) ~ NA_real_,
                         V4016 > 10 ~ 10,
                         TRUE ~ V4016),
    serieswgt = case_when(series == 2 & is.na(n10v4016) ~ 6,
                          series == 2 ~ n10v4016,
                          TRUE ~ 1),
    NEWWGT = WGTVICCY * serieswgt
  )

inc_ind <- inc_series %>%
  filter(V4022 != 1) %>%
  mutate(
    WeapCat = case_when(
      is.na(V4049) ~ NA_character_,
      V4049 == 2 ~ "NoWeap",
      V4049 == 3 ~ "UnkWeapUse",
      V4050 == 3 ~ "Other",
      V4051 == 1 | V4052 == 1 | V4050 == 7 ~ "Firearm",
      V4053 == 1 | V4054 == 1 ~ "Knife",
      TRUE ~ "Other"
    ),
    V4529_num = parse_number(as.character(V4529)),
    ReportPolice = V4399 == 1,
    Property = V4529_num >= 31,
    Violent = V4529_num <= 20,
    Property_ReportPolice = Property & ReportPolice,
    Violent_ReportPolice = Violent & ReportPolice,
    AAST = V4529_num %in% 11:13,
    AAST_NoWeap = AAST & WeapCat == "NoWeap",
    AAST_Firearm = AAST & WeapCat == "Firearm",
    AAST_Knife = AAST & WeapCat == "Knife",
    AAST_Other = AAST & WeapCat == "Other"
  )
inc_hh_sums <-
  inc_ind %>%
  filter(V4529_num > 23) %>% # restrict to household crimes
  group_by(YEARQ, IDHH) %>%
  summarize(WGTVICCY = WGTVICCY[1],
            across(starts_with("Property"), 
                   ~ sum(. * serieswgt),
                   .names = "{.col}"),
            .groups = "drop")

inc_pers_sums <-
  inc_ind %>%
  filter(V4529_num <= 23) %>% # restrict to person crimes
  group_by(YEARQ, IDHH, IDPER) %>%
  summarize(WGTVICCY = WGTVICCY[1],
            across(c(starts_with("Violent"), starts_with("AAST")),
                   ~ sum(. * serieswgt), 
                   .names = "{.col}"),
            .groups = "drop")

hh_z_list <- rep(0, ncol(inc_hh_sums) - 3) %>% as.list() %>%
  setNames(names(inc_hh_sums)[-(1:3)])
pers_z_list <- rep(0, ncol(inc_pers_sums) - 4) %>% as.list() %>%
  setNames(names(inc_pers_sums)[-(1:4)])

hh_vsum <- ncvs_2021_household %>%
  full_join(inc_hh_sums, by = c("YEARQ", "IDHH")) %>%
  replace_na(hh_z_list) %>%
  mutate(ADJINC_WT = if_else(is.na(WGTVICCY), 0, WGTVICCY / WGTHHCY))

pers_vsum <- ncvs_2021_person %>%
  full_join(inc_pers_sums, by = c("YEARQ", "IDHH", "IDPER")) %>%
  replace_na(pers_z_list) %>%
  mutate(ADJINC_WT = if_else(is.na(WGTVICCY), 0, WGTVICCY / WGTPERCY))

hh_vsum_der <- hh_vsum %>%
  mutate(
    Tenure = factor(case_when(V2015 == 1 ~ "Owned", 
                              !is.na(V2015) ~ "Rented"),
                    levels = c("Owned", "Rented")),
    Urbanicity = factor(case_when(V2143 == 1 ~ "Urban",
                                  V2143 == 2 ~ "Suburban",
                                  V2143 == 3 ~ "Rural"),
                        levels = c("Urban", "Suburban", "Rural")),
    SC214A_num = as.numeric(as.character(SC214A)),
    Income = case_when(SC214A_num <= 8 ~ "Less than $25,000",
                       SC214A_num <= 12 ~ "$25,000--49,999",
                       SC214A_num <= 15 ~ "$50,000--99,999",
                       SC214A_num <= 17 ~ "$100,000--199,999",
                       SC214A_num <= 18 ~ "$200,000 or more"),
    Income = fct_reorder(Income, SC214A_num, .na_rm = FALSE),
    PlaceSize = case_match(as.numeric(as.character(V2126B)),
                           0 ~ "Not in a place",
                           13 ~ "Population under 10,000",
                           16 ~ "10,000--49,999",
                           17 ~ "50,000--99,999",
                           18 ~ "100,000--249,999",
                           19 ~ "250,000--499,999",
                           20 ~ "500,000--999,999",
                           c(21, 22, 23) ~ "1,000,000 or more"),
    PlaceSize = fct_reorder(PlaceSize, as.numeric(V2126B)),
    Region = case_match(as.numeric(V2127B),
                        1 ~ "Northeast",
                        2 ~ "Midwest",
                        3 ~ "South",
                        4 ~ "West"),
    Region = fct_reorder(Region, as.numeric(V2127B))
  )
NHOPI <- "Native Hawaiian or Other Pacific Islander"

pers_vsum_der <- pers_vsum %>%
  mutate(
    Sex = factor(case_when(V3018 == 1 ~ "Male",
                           V3018 == 2 ~ "Female")),
    RaceHispOrigin = factor(case_when(V3024 == 1 ~ "Hispanic",
                                      V3023A == 1 ~ "White",
                                      V3023A == 2 ~ "Black",
                                      V3023A == 4 ~ "Asian",
                                      V3023A == 5 ~ NHOPI,
                                      TRUE ~ "Other"),
                            levels = c("White", "Black", "Hispanic", 
                                       "Asian", NHOPI, "Other")),
    V3014_num = as.numeric(as.character(V3014)),
    AgeGroup = case_when(V3014_num <= 17 ~ "12--17",
                         V3014_num <= 24 ~ "18--24",
                         V3014_num <= 34 ~ "25--34",
                         V3014_num <= 49 ~ "35--49",
                         V3014_num <= 64 ~ "50--64",
                         V3014_num <= 90 ~ "65 or older"),
    AgeGroup = fct_reorder(AgeGroup, V3014_num),
    MaritalStatus = factor(case_when(V3015 == 1 ~ "Married",
                                     V3015 == 2 ~ "Widowed",
                                     V3015 == 3 ~ "Divorced",
                                     V3015 == 4 ~ "Separated",
                                     V3015 == 5 ~ "Never married"),
                           levels = c("Never married", "Married", 
                                      "Widowed","Divorced", 
                                      "Separated"))
  ) %>% 
  left_join(hh_vsum_der %>% select(YEARQ, IDHH, 
                                   V2117, V2118, Tenure:Region),
            by = c("YEARQ", "IDHH"))
hh_vsum_slim <- hh_vsum_der %>%
  select(YEARQ:V2118,
         WGTVICCY:ADJINC_WT,
         Tenure,
         Urbanicity,
         Income,
         PlaceSize,
         Region)

pers_vsum_slim <- pers_vsum_der %>%
  select(YEARQ:WGTPERCY, WGTVICCY:ADJINC_WT, Sex:Region)

dummy_records <- hh_vsum_slim %>%
  distinct(V2117, V2118) %>%
  mutate(Dummy = 1,
         WGTVICCY = 1,
         NEWWGT = 1)

inc_analysis <- inc_ind %>%
  mutate(Dummy = 0) %>%
  left_join(select(pers_vsum_slim, YEARQ, IDHH, IDPER, Sex:Region),
            by = c("YEARQ", "IDHH", "IDPER")) %>%
  bind_rows(dummy_records) %>%
  select(YEARQ:IDPER,
         WGTVICCY,
         NEWWGT,
         V4529,
         WeapCat,
         ReportPolice,
         Property:Region)

inc_des <- inc_analysis %>%
  as_survey_design(
    weight = NEWWGT,
    strata = V2117,
    ids = V2118,
    nest = TRUE
  )

hh_des <- hh_vsum_slim %>%
  as_survey_design(
    weight = WGTHHCY,
    strata = V2117,
    ids = V2118,
    nest = TRUE
  )

pers_des <- pers_vsum_slim %>%
  as_survey_design(
    weight = WGTPERCY,
    strata = V2117,
    ids = V2118,
    nest = TRUE
  )
```
The chapter exercises use the survey design objects and packages provided in the Prerequisites box in the beginning of the chapter. Please ensure they are loaded in the environment before running the exercise solutions.

## 5 - Descriptive analysis {-}

1. How many females have a graduate degree? Hint: The variables `Gender` and `Education` will be useful.

```{r}
#| label: desc-ex-solution1
# Option 1:
femgd_option1 <- anes_des %>%
  filter(Gender == "Female", Education == "Graduate") %>%
  survey_count(name = "n")

femgd_option1

# Option 2:
femgd_option2 <- anes_des %>%
  filter(Gender == "Female", Education == "Graduate") %>%
  summarize(N = survey_total(), .groups = "drop")

femgd_option2
```

Answer: `r femgd_option2$N %>% prettyNum(big.mark = ",")`

2. What percentage of people identify as "Strong Democrat"? Hint: The variable `PartyID` indicates someone's party affiliation.

```{r}
#| label: desc-ex-solution2
psd <- anes_des %>%
  group_by(PartyID) %>%
  summarize(p = survey_mean()) %>%
  filter(PartyID == "Strong democrat")

psd
```

Answer: `r round(psd$p*100, 1)`%

3. What percentage of people who voted in the 2020 election identify as "Strong Republican"? Hint: The variable `VotedPres2020` indicates whether someone voted in 2020.

```{r}
#| label: desc-ex-solution3
psr <- anes_des %>%
  filter(VotedPres2020 == "Yes") %>%
  group_by(PartyID) %>%
  summarize(p = survey_mean()) %>%
  filter(PartyID == "Strong republican")

psr
```

Answer: `r round(psr$p*100, 1)`%

4. What percentage of people voted in both the 2016 election and the 2020 election?  Include the logit confidence interval. Hint: The variable `VotedPres2016` indicates whether someone voted in 2016.

```{r}
#| label: desc-ex-solution4
#| message: false
pvb <- anes_des %>%
  filter(!is.na(VotedPres2016),!is.na(VotedPres2020)) %>%
  group_by(interact(VotedPres2016, VotedPres2020)) %>%
  summarize(p = survey_prop(var = "ci", method = "logit"),) %>%
  filter(VotedPres2016 == "Yes", VotedPres2020 == "Yes")

pvb
```

Answer: `r round(pvb$p*100, 1)` with confidence interval: (`r round(pvb$p_low*100, 1)`, `r round(pvb$p_upp*100, 1)`)

5. What is the design effect for the proportion of people who voted early? Hint: The variable `EarlyVote2020` indicates whether someone voted early in 2020.

```{r}
#| label: desc-ex-solution5
pdeff <- anes_des %>%
  filter(!is.na(EarlyVote2020)) %>%
  group_by(EarlyVote2020) %>%
  summarize(p = survey_mean(deff = TRUE)) %>%
  filter(EarlyVote2020 == "Yes")

pdeff
```

Answer: `r round(pdeff$p_deff,2)`

6. What is the median temperature people set their thermostats to at night during the winter? Hint: The variable `WinterTempNight` indicates the temperature that people set their thermostat to in the winter at night.

```{r}
#| label: desc-ex-solution6
med_wintertempnight <- recs_des %>%
  summarize(wtn_med = survey_median(x = WinterTempNight,
                                   na.rm = TRUE))

med_wintertempnight
```

Answer: `r round(med_wintertempnight$wtn_med,1)`


7. People sometimes set their temperature differently over different seasons and during the day. What median temperatures do people set their thermostat to in the summer and winter, both during the day and at night? Include confidence intervals. Hint: Use the variables `WinterTempDay`, `WinterTempNight`, `SummerTempDay`, and `SummerTempNight`.

```{r}
#| label: desc-ex-solution7
# Option 1

med_temps <- recs_des %>%
  summarize(
    across(c(WinterTempDay, WinterTempNight, SummerTempDay, SummerTempNight), ~survey_median(.x, na.rm=TRUE))
  )

med_temps

# Alternatively, could use `survey_quantile()` as shown below for WinterTempNight:

quant_temps <- recs_des %>%
  summarize(
    across(c(WinterTempDay, WinterTempNight, SummerTempDay, SummerTempNight), ~survey_quantile(.x, quantiles=0.5, na.rm=TRUE))
  )

quant_temps
```

Answer: 
- Winter during the day: `r round(med_temps$WinterTempDay, 1)`
- Winter during the night: `r round(med_temps$WinterTempNight, 1)`
- Summer during the day: `r round(med_temps$SummerTempDay, 1)`
- Summer during the night: `r round(med_temps$SummerTempNight, 1)`

8. What is the correlation between the temperature that people set their temperature at during the night and during the day in the summer?

```{r}
#| label: desc-ex-solution8
#| warning: false
corr_summer_temp <- recs_des %>%
  summarize(summer_corr = survey_corr(SummerTempNight, SummerTempDay,
                                      na.rm = TRUE))

corr_summer_temp
```

Answer: `r round(corr_summer_temp$summer_corr, 3)`

9. What is the 1st, 2nd, and 3rd quartile of the amount of money spent on energy by Building America (BA) climate zone? Hint: `TOTALDOL` indicates the total amount spent on all fuel, and `ClimateRegion_BA` indicates the BA climate zones.

```{r}
#| label: desc-ex-solution9
quant_baenergyexp <- recs_des %>%
  group_by(ClimateRegion_BA) %>%
  summarize(dol_quant = survey_quantile(
    TOTALDOL,
    quantiles = c(0.25, 0.5, 0.75),
    vartype = "se",
    na.rm = TRUE
  ))

quant_baenergyexp
```

Answer:

```{r}
#| label: desc-ex-solution9-print
#| echo: FALSE
quant_baenergyexp %>%
  select(-ends_with("se")) %>%
  gt(rowname_col="ClimateRegion_BA") %>%
  fmt_currency(decimals=0) %>%
  cols_label(
    dol_quant_q25 ="Q1",
    dol_quant_q50 ="Q2",
    dol_quant_q75 ="Q3"
  ) %>%
  tab_header("Quartile summary of energy expenditure by BA Climate Zone")
```

## 6 - Statistical testing {-}

1. Using the RECS data, do more than 50% of U.S. households use A/C (`ACUsed`)?

```{r}
#| label: stattest-ex-solution1
ttest_solution1 <- recs_des %>%
  svyttest(design = .,
           formula = ((ACUsed == TRUE) - 0.5) ~ 0,
           na.rm = TRUE,
           alternative="greater") %>%
  tidy()

ttest_solution1
```

Answer: `r round((ttest_solution1$estimate+.5)*100, 1)`% of households use air conditioning which is significantly different from 50% (p`r pretty_p_value(ttest_solution1$p.value)`) so there is strong evidence that more than 50% of households use air-conditioning.


2. Using the RECS data, does the average temperature that U.S. households set their thermostats to differ between the day and night in the winter (`WinterTempDay` and `WinterTempNight`)?

```{r}
#| label: stattest-ex-solution2
ttest_solution2 <- recs_des %>%
  svyttest(
    design = .,
    formula = WinterTempDay - WinterTempNight ~ 0,
    na.rm = TRUE
  ) %>%
  tidy()

ttest_solution2
```

Answer: The average temperature difference between night and day during the winter for thermostat settings is `r ttest_solution2$estimate %>% round(2)` which is significantly different from 0 (p`r pretty_p_value(ttest_solution2$p.value)`) so there is strong evidence that the temperature setting is different between night and daytime during the winter.

3. Using the ANES data, does the average age (`Age`) of those who voted for Joseph Biden in 2020 (`VotedPres2020_selection`) differ from those who voted for another candidate?

```{r}
#| label: stattest-ex-solution3
ttest_solution3 <- anes_des %>%
  filter(!is.na(VotedPres2020_selection)) %>%
  svyttest(
    design = .,
    formula = Age ~ VotedPres2020_selection == "Biden",
    na.rm = TRUE
  ) %>%
  tidy()

ttest_solution3
```

On average, those who voted for Joseph Biden in 2020 were `r ttest_solution3$estimate %>% round(1)` years younger than voters for other candidates and this is significantly different (p `r ttest_solution3$p.value %>% pretty_p_value()`).

4. If we wanted to determine if the political party affiliation differed for males and females, what test would we use?

  a. Goodness-of-fit test (`svygofchisq()`)
  b. Test of independence (`svychisq()`)
  c. Test of homogeneity (`svychisq()`)
  

Answer: c. Test of homogeneity (`svychisq()`)

5. In the RECS data, is there a relationship between the type of housing unit (`HousingUnitType`) and the year the house was built (`YearMade`)?

```{r}
#| label: stattest-ex-solution5
chisq_solution2 <- recs_des %>%
  svychisq(
    formula =  ~ HousingUnitType + YearMade,
    design = .,
    statistic = "Wald",
    na.rm = TRUE
  )

chisq_solution2 %>% tidy()
```

Answer: There is strong evidence (p`r chisq_solution2$p.value %>% pretty_p_value()`) that there is a relationship between type of housing unit and the year the house was built.

6. In the ANES data, is there a difference in the distribution of gender (`Gender`) across early voting status in 2020 (`EarlyVote2020`)?

```{r}
#| label: stattest-ex-solution6
chisq_solution3 <- anes_des %>%
  svychisq(
    formula =  ~ Gender + EarlyVote2020,
    design = .,
    statistic = "F",
    na.rm = TRUE
  ) %>%
  tidy()

chisq_solution3
```

Answer: There is strong evidence that there is a difference in the gender distribution of gender by early voting status (p=`r chisq_solution3$p.value %>% pretty_p_value()`).

## 7 - Modeling {-}

1.  The type of housing unit may have an impact on energy expenses. Is there any relationship between housing unit type (`HousingUnitType`) and total energy expenditure (`TOTALDOL`)? First, find the average energy expenditure by housing unit type as a descriptive analysis and then do the test. The reference level in the comparison should be the housing unit type that is most common.

```{r}
#| label: model-ex-solution1
expense_by_hut <- recs_des %>%
  group_by(HousingUnitType) %>%
  summarize(Expense = survey_mean(TOTALDOL, na.rm = TRUE),
            HUs = survey_total()) %>%
  arrange(desc(HUs))

expense_by_hut

exp_unit_out <- recs_des %>%
  mutate(HousingUnitType = fct_infreq(HousingUnitType, NWEIGHT)) %>%
  svyglm(
    design = .,
    formula = TOTALDOL ~ HousingUnitType,
    na.action = na.omit
  )

tidy(exp_unit_out)
```

Answer: The reference level should be `r expense_by_hut %>% slice(1) %>% pull(HousingUnitType) %>% as.character()`. All p-values are very small indicating there is a significant relationship between housing unit type and total energy expenditure.


2.  Does temperature play a role in electricity expenditure? Cooling degree days are a measure of how hot a place is. CDD65 for a given day indicates the number of degrees Fahrenheit warmer than 65°F (18.3°C) it is in a location. On a day that averages 65°F and below, CDD65=0. While a day that averages 85°F (29.4°C) would have CDD65=20 because it is 20 degrees Fahrenheit warmer [@eia-cdd]. For each day in the year, this is summed to give an indicator of how hot the place is throughout the year. Similarly, HDD65 indicates the days colder than 65°F. Can energy expenditure be predicted using these temperature indicators along with square footage? Is there a significant relationship? Include main effects and two-way interactions.

```{r}
#| label: model-ex-solution2
temps_sqft_exp <- recs_des %>%
  svyglm(
    design = .,
    formula = DOLLAREL ~ (TOTSQFT_EN + CDD65 + HDD65) ^ 2,
    na.action = na.omit
  )

tidy(temps_sqft_exp) %>%
  mutate(p.value=pretty_p_value(p.value) %>% str_pad(7))
```

Answer: There is a significant interaction between square footage and cooling degree days in the model and the square footage is a significant predictor of eletricity expenditure. 

3.  Continuing with our results from Exercise 2, create a plot between the actual and predicted expenditures and a residual plot for the predicted expenditures.

Answer: 
```{r}
#| label: model-ex-solution3
temps_sqft_exp_fit <- temps_sqft_exp %>%
  augment() %>%
  mutate(.se.fit = sqrt(attr(.fitted, "var")), 
         # extract the variance of the fitted value
         .fitted = as.numeric(.fitted)) 
```

```{r}
#| label: model-ex-solution4
#| fig.cap: "Actual and predicted electricity expenditures"
temps_sqft_exp_fit %>%
  ggplot(aes(x = DOLLAREL, y = .fitted)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red") +
  xlab("Actual expenditures") +
  ylab("Predicted expenditures") +
  theme_minimal()
```

```{r}
#| label: model-ex-solution5
#| fig.cap: "Residual plot of electric cost model with covariates TOTSQFT_EN, CDD65, and HDD65"
temps_sqft_exp_fit %>%
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red") +
  xlab("Predicted expenditure") +
  ylab("Residual value of expenditure") +
  theme_minimal()
```

4.  Early voting expanded in 2020 [@npr-voting-trend]. Build a logistic model predicting early voting in 2020 (`EarlyVote2020`) using age (`Age`), education (`Education`), and party identification (`PartyID`). Include two-way interactions.

Answer: 
```{r}
#| label: model-ex-solution6
earlyvote_mod <- anes_des %>%
  filter(!is.na(EarlyVote2020)) %>%
  svyglm(
    design = .,
    formula = EarlyVote2020 ~ (Age + Education + PartyID) ^ 2 ,
    family = quasibinomial
  )

tidy(earlyvote_mod) %>% print(n=50)
```

5.  Continuing from Exercise 4, predict the probability of early voting for two people. Both are 28 years old and have a graduate degree; however, one person is a strong Democrat, and the other is a strong Republican.

```{r}
#| label: model-ex-solution7
add_vote_dat <- anes_2020 %>%
  select(EarlyVote2020, Age, Education, PartyID) %>%
  rbind(tibble(
    EarlyVote2020 = NA,
    Age = 28,
    Education = "Graduate",
    PartyID = c("Strong democrat", "Strong republican")
  )) %>%
  tail(2)

log_ex_2_out <- earlyvote_mod %>%
  augment(newdata = add_vote_dat, type.predict = "response") %>%
  mutate(.se.fit = sqrt(attr(.fitted, "var")),
         # extract the variance of the fitted value
         .fitted = as.numeric(.fitted))

log_ex_2_out
```

Answer: We predict that the 28 year old with a graduate degree who identifies as a strong democrat will vote early `r round(log_ex_2_out$.fitted[1]*100, 1)`% of the time while a person who is otherwise similar but is a strong replican will vote early `r round(log_ex_2_out$.fitted[2]*100, 1)`% of the time

## 10 - Specifying sample designs and replicate weights in {srvyr}  {-}


1. The National Health Interview Survey (NHIS) is an annual household survey conducted by the National Center for Health Statistics (NCHS). The NHIS includes a wide variety of health topics for adults including health status and conditions, functioning and disability, health care access and health service utilization, health-related behaviors, health promotion, mental health, barriers to receiving care, and community engagement. Like many national in-person surveys, the sampling design is a stratified clustered design with details included in the Survey Description [@nhis-svy-des]. The Survey Description provides information on setting up syntax in SUDAAN, Stata, SPSS, SAS, and R ({survey} package implementation). We have imported the data and the variable containing the data as: `nhis_adult_data`. How would we specify the design using either `as_survey_design()` or `as_survey_rep()`?

Answer: 

```{r}
#| label: samp-ex-solution1
#| eval: false
nhis_adult_des <- nhis_adult_data %>%
  as_survey_design(
    ids = PPSU,
    strata = PSTRAT,
    nest = TRUE,
    weights = WTFA_A
  )
```

2. The General Social Survey (GSS) is a survey that has been administered since 1972 on social, behavioral, and attitudinal topics. The 2016-2020 GSS Panel codebook provides examples of setting up syntax in SAS and Stata but not R [@gss-codebook]. We have imported the data and the variable containing the data as: `gss_data`. How would we specify the design in R using either `as_survey_design()` or `as_survey_rep()`?

Answer: 

```{r}
#| label: samp-ex-solution2
#| eval: false
gss_des <- gss_data %>%
  as_survey_design(ids = VPSU_2,
                   strata = VSTRAT_2,
                   weights = WTSSNR_2)
```

## 13 - National Crime Victimization Survey Vignette {-}

1. What proportion of completed motor vehicle thefts are not reported to the police? Hint: Use the codebook to look at the definition of Type of Crime (V4529).

```{r}
#| label: ncvs-vign-ex-solution1
ans1 <- inc_des %>%
  filter(str_detect(V4529, "40|41")) %>%
  summarize(Pct = survey_mean(!ReportPolice, na.rm = TRUE) * 100)
```

Answer: It is estimated that `r round(ans1$Pct, 1)`% of motor vehicle thefts are not reported to the police.

2. How many violent crimes occur in each region?

Answer:

```{r}
#| label: ncvs-vign-ex-solution2
inc_des %>%
  filter(Violent) %>%
  survey_count(Region) %>%
  select(-n_se) %>%
  gt(rowname_col="Region") %>%
  fmt_integer() %>%
  cols_label(
    n ="Violent victimizations",
  ) %>%
  tab_header("Estimated number of violent crimes by region")
```

3. What is the property victimization rate among each income level?

Answer: 

```{r}
#| label: ncvs-vign-ex-solution3
hh_des %>%
  filter(!is.na(Income)) %>%
  group_by(Income) %>%
  summarize(Property_Rate = survey_mean(Property * ADJINC_WT * 1000, 
                                        na.rm = TRUE)) %>%
  gt(rowname_col="Income") %>%
  cols_label(
    Property_Rate="Rate",
    Property_Rate_se="Standard Error"
  ) %>%
  fmt_number(decimals=1) %>%
  tab_header("Estimated property victimization rate by income level")
```

4. What is the difference between the violent victimization rate between males and females? Is it statistically different?

```{r}
#| label: ncvs-vign-ex-solution4
vr_gender <- pers_des %>%
  group_by(Sex) %>%
  summarize(
    Violent_rate=survey_mean(Violent * ADJINC_WT * 1000, na.rm=TRUE)
  )

vr_gender_test <- pers_des %>%
  mutate(
    Violent_Adj=Violent * ADJINC_WT * 1000
  ) %>%
  svyttest(
    formula = Violent_Adj ~ Sex,
    design = .,
    na.rm = TRUE
  ) %>%
  broom::tidy()
```

Answer: The difference between male and female victimization rate is estimated as `r vr_gender_test$estimate %>% round(1)` victimizations/1,000 people and is not significantly different (p-value=`r vr_gender_test$p.value %>% pretty_p_value()`)

## 14 - AmericasBarometer Vignette {-}

1. Calculate the percentage of households with broadband internet and those with any internet at home, including from a phone or tablet in Latin America and the Caribbean. Hint: if there are countries with 0% internet usage, try filtering by something first.
Answer: 

```{r}
#| label: ambarom-ex-solution1
int_ests <-
  ambarom_des %>%
  filter(!is.na(Internet) | !is.na(BroadbandInternet)) %>%
  group_by(Country) %>%
  summarize(
    p_broadband = survey_mean(BroadbandInternet, na.rm = TRUE) * 100,
    p_internet = survey_mean(Internet, na.rm = TRUE) * 100
  ) 

int_ests %>%
  gt(rowname_col = "Country") %>%
  fmt_number(decimals=1) %>%
  tab_spanner(
    label="Broadband at home",
    columns=c(p_broadband, p_broadband_se)
  ) %>%
  tab_spanner(
    label="Internet at home",
    columns=c(p_internet, p_internet_se)
  ) %>%
  cols_label(
    p_broadband="Percent",
    p_internet="Percent",
    p_broadband_se="S.E.",
    p_internet_se="S.E.",
  )
```

2. Create a faceted map showing both broadband internet and any internet usage.

Answer:

```{r}
#| label: ambarom-ex-solution2
#| error: true
#| fig.cap: "Percent of broadband internet and any internet usage, Central and South America"
library(sf)
library(rnaturalearth)
library(ggpattern)
internet_sf <- country_shape_upd %>%
  full_join(select(int_ests, p = p_internet, geounit = Country), by = "geounit") %>%
  mutate(Type = "Internet")
broadband_sf <- country_shape_upd %>%
  full_join(select(int_ests, p = p_broadband, geounit = Country), by = "geounit") %>%
  mutate(Type = "Broadband")
b_int_sf <- internet_sf %>%
  bind_rows(broadband_sf) %>%
  filter(region_wb == "Latin America & Caribbean")

b_int_sf %>%
  ggplot(aes(fill = p),
         color="darkgray") +
  geom_sf() +
  facet_wrap( ~ Type) +
  scale_fill_gradientn(
    guide = "colorbar",
    name = "Percent",
    labels = scales::comma,
    colors = c("#BFD7EA", "#087E8B", "#0B3954"),
    na.value = NA
  ) +
  geom_sf_pattern(
    data = filter(b_int_sf, is.na(p)),
    pattern = "crosshatch",
    pattern_fill = "lightgray",
    pattern_color = "lightgray",
    fill = NA,
    color = "darkgray"
  ) +
  theme_minimal()
```

<!--chapter:end:93-AppendixD-Solutions.Rmd-->

`r if (knitr:::is_html_output()) '# References {-}'`

```{r}
#| label: refs-package-list
#| include: FALSE
# generate a BibTeX database automatically for some R packages
library(targets)
library(visdat)
library(readxl)
library(httr2)
library(jsonlite)
library(tidycensus)
library(pak)


our_write_bib <- function (x = .packages(), file = "", tweak = TRUE, width = NULL, 
          prefix = getOption("knitr.bib.prefix", "R-"), lib.loc = NULL) 
{
  .this.year = sprintf('  year = {%s},', format(Sys.Date(), '%Y'))
  system.file = function(...) base::system.file(..., lib.loc = lib.loc)
  citation = function(...) utils::citation(..., lib.loc = lib.loc)
  x = x[nzchar(x)]
  idx = mapply(system.file, package = x) == ""
  if (any(idx)) {
    warning("package(s) ", paste(x[idx], collapse = ", "), 
            " not found")
    x = x[!idx]
  }
  x = setdiff(x, setdiff(xfun::base_pkgs(), "base"))
  x = sort(x)
  bib = sapply(x, function(pkg) {
    cite = citation(pkg, auto = if (pkg != "base") {
      meta = packageDescription(pkg, lib.loc = lib.loc)
      if (identical(meta$Repository, "CRAN") && !is.null(meta$URL)) {
        if (!grepl("[, ]", meta$URL)) 
          meta$Repository = NULL
      }
      meta
    })
    
    
    if (tweak) {
      cite$title = gsub(sprintf("^(%s: )(\\1)", pkg), 
                        "\\1", cite$title)
      cite$title = gsub(pkg, paste0("{", pkg, "}"), cite$title)
      cite$title = gsub("\\b(R)\\b", "{R}", cite$title)
      cite$title = gsub("\\b(ggplot2)\\b", "{ggplot2}", cite$title)
      cite$title = gsub("\\b(dplyr)\\b", "{dplyr}", cite$title)
      cite$title = gsub("\\b(tidyverse)\\b", "{tidyverse}", cite$title)
      cite$title = gsub("\\b(sf)\\b", "{sf}", cite$title)
      cite$title = gsub(" & ", " \\\\& ", cite$title)
    }
    entry = toBibtex(cite)
    entry[1] = sub("\\{,$", sprintf("{%s%s,", prefix, pkg), 
                   entry[1])
    entry
  }, simplify = FALSE)
  if (tweak) {
    for (i in intersect(names(knitr:::.tweak.bib), x)) {
      message("tweaking ", i)
      bib[[i]] = merge_list(bib[[i]], knitr:::.tweak.bib[[i]])
    }
    bib = lapply(bib, function(b) {
      b["author"] = sub("Duncan Temple Lang", "Duncan {Temple Lang}", 
                        b["author"])
      # b["title"] = gsub("(^|\\W)'([^']+)'(\\W|$)", "\\1\\2\\3", 
      #                   b["title"])
      if (!is.na(b["note"])) 
        b["note"] = gsub("(^.*?https?://.*?),\\s+https?://.*?(},\\s*)$", 
                         "\\1\\2", b["note"])
      if (!("year" %in% names(b))) 
        b["year"] = .this.year
      b
    })
  }
  bib2 = lapply(x, function(pkg) {
    if (pkg == "base") 
      return()
    if (system.file("CITATION", package = pkg) == "") 
      return()
    cites = citation(pkg, auto = FALSE)
    cites = Filter(x = cites, function(cite) {
      !isTRUE(grepl("R package version", cite$note))
    })
    s = knitr:::make_unique(unlist(lapply(cites, function(cite) {
      if (is.null(cite$year)) 
        format(Sys.Date(), "%Y")
      else cite$year
    })))
    mapply(cites, s, FUN = function(cite, suffix) {
      if (isTRUE(grepl("R package version", cite$note))) 
        return()
      entry = toBibtex(cite)
      entry[1] = sub("\\{,$", sprintf("{%s%s,", pkg, suffix), 
                     entry[1])
      entry
    }, SIMPLIFY = FALSE)
  })
  bib = c(bib, unlist(bib2, recursive = FALSE))
  bib = lapply(bib, function(b) {
    idx = which(names(b) == "")
    if (!is.null(width)) 
      b[-idx] = str_wrap(b[-idx], width, 2, 4)
    structure(c(b[idx[1L]], b[-idx], b[idx[2L]], ""), class = "Bibtex")
  })
  if (!is.null(file) && length(x)) 
    xfun::write_utf8(unlist(bib), file)
  invisible(bib)
}



our_write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown', 'renv', 'here'
), 'packages.bib')
```

<!--chapter:end:99-references.Rmd-->

