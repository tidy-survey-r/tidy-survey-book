# Modeling {#c07-modeling}

::: {.prereqbox-header}
`r if (knitr:::is_html_output()) '### Prerequisites {- #prereq7}'`
:::

::: {.prereqbox data-latex="{Prerequisites}"}
For this chapter, load the following packages and the helper function:
```{r}
#| label: model-setup
#| error: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(survey) 
library(srvyr) 
library(osfr)
source("helper-fun/helper-function.R")
library(broom)
```

We will be using data from ANES and RECS. Here is the code to create the design objects for each to use throughout this chapter. For ANES, we need to adjust the weight so it sums to the population instead of the sample (see the ANES documentation and Chapter \@ref(c04-understanding-survey-data-documentation) for more information).
```{r}
#| label: model-anes-des
#| eval: FALSE
anes_in <- read_osf("anes_2020.rds") 
targetpop <- 231592693

anes_adjwgt <- anes_in %>%
  mutate(Weight = Weight / sum(Weight) * targetpop)

anes_des <- anes_adjwgt %>%
  as_survey_design(
    weights = Weight,
    strata = Stratum,
    ids = VarUnit,
    nest = TRUE)
```

For RECS, details are included in the RECS documentation and Chapter \@ref(c03-specifying-sample-designs).
```{r}
#| label: model-recs-des
#| eval: FALSE
recs_in <-read_osf("recs_2015.rds")

recs_des <- recs_in %>%
  as_survey_rep(weights = NWEIGHT,
                repweights = starts_with("BRRWT"),
                type = "Fay",
                rho = 0.5,
                mse = TRUE)
```
:::

## Introduction

Modeling data is a way for researchers to investigate the relationship between a single dependent variable and one or more independent variables. This builds upon the analyses conducted in Chapter \@ref(c06-statistical-testing), which looked at the relationships between just two variables.  For example, in Example 3 in Section \@ref(stattest-ttest-examples), we investigated if there is a relationship between the electrical bill cost and whether or not the household used air-conditioning.  However, there are potentially other elements that could go into what the cost of electrical bill is in a household (e.g., outside temperature, desired internal temperature, types and number of appliances, etc.). T-tests only allow us to investigate the relationship of one independent variable at a time, but using models we can look into multiple variables and even explore interactions between these variables.  There are several types of models, but in this chapter we will cover Analysis of Variance (ANOVA) and linear regression models following common Gaussian and logit distributions. Jonas Kristoffer Lindeløv has an interesting [discussion](https://lindeloev.github.io/tests-as-linear/) of many statistical tests and models being equivalent to a linear model. For example, a one-way ANOVA is a linear model with one categorical independent variable, and a two-sample t-test is an ANOVA where the independent variable has exactly two levels.

When modeling data, it is helpful to first create an equation that provides an overview as to what it is that we are modeling.  The main structure of these models is as follows:

$$y_i=\beta_0 +\sum_{i=1}^p \beta_i x_i + \epsilon_i$$

where $y_i$ is the outcome, $\beta_0$ is an intercept, $x_1, \cdots, x_p$ are the predictors with $\beta_1, \cdots, \beta_p$ as the associated coefficients, and $\epsilon_i$ is the error.  Different models may not include an intercept, have interactions between different independent variables ($x_i$), or may have different underlying structures for the dependent variable ($y_i$).  However, all linear models have the independent variables related to the dependent variable in a linear form.

To specify these models in R, the formulas are the same with both survey data and other data. The left side of the formula is the response/dependent variable, and the right side of the formula has the predictor/independent variable(s). There are many symbols used in R to specify the formula.

For example, a linear formula mathematically specified as
<!-- <JM> Why switch to captial Y and X? -->
$$Y_i=\beta_0+\beta_1 X_i+\epsilon_i$$ would be specified in R as `y~x` where the intercept is not explicitly included. To fit a model with no intercept, that is,

$$Y_i=\beta_1 X_i+\epsilon_i$$
it can be specified as `y~x-1`. Formula notation details in R can be found in the help file for formula^[Use `help(formula)` or `?formula` in R or find the documentation online at <https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html>]. A quick overview of the common formula notation is in the following table:

Table: Common symbols in formula notation

| Symbol |   Example   | Meaning                                                                  |
|:-----------:|:---------------:|---------------------------------------------------------------------------|
|   \+   |    `+X`     | include this variable                                                    |
|   \-   |    `-X`     | delete this variable                                                     |
|   :    |    `X:Z`    | include the interaction between these variables                          |
|   \*   |    `X*Z`    | include these variables and the interactions between them                |
|  `^n`  | `(X+Z+Y)^3` | include these variables and all interactions up to n-way                 |
|   I    |  `I(X-Z)`   | as-as: include a new variable which is the difference of these variables |

There are often multiple ways to specify the same formula. For example, consider the following equation using the mtcars data

$$mpg_i=\beta_0+\beta_1cyl_{i}+\beta_2disp_{i}+\beta_3hp_{i}+\beta_4cyl_{i}disp_{i}+\beta_5cyl_{i}hp_{i}+\beta_6disp_{i}hp_{i}+\epsilon_i$$

This could be specified as any of the following:
<!-- <JM> Spaces in the code would help with readability -->
  - `mpg~(cyl+disp+hp)^2`
  - `mpg~cyl+disp+hp+cyl:disp+cyl:hp+disp:hp`
  - `mpg~cyl*disp+cyl*hp+disp*hp`

Note that the following two specifications are not the same:
  
  - `mpg~cyl:disp:hp` this only has the interactions and not the main effect
  - `mpg~cyl*disp*hp` this also has the 3-way interaction in addition to the main effects and 2-way interactions

<!-- <JM> Link to Appendix E? -->
When using non-survey data such as experimental or observational data, researchers will use the `glm()` function for linear models.  With survey data, however, we use `svyglm()` from the {survey} package to ensure that we account for the survey design and weights in modeling^[There is some debate about whether weights should be used in regression [@gelman2007weights; @bollen2016weightsreg]. However, for the purposes of providing complete information on how to analyze complex survey data, this chapter will include weights.].  This allows us to generalize a model to the target population and accounts for the fact that the observations in the survey data may not be independent.  As discussed in Chapter \@ref(c06-statistical-testing), modeling survey data cannot be directly done in {srvyr}, but can be done in the {survey} [@lumley2010complex; @R-survey] package. In this chapter, we will provide syntax and examples for linear models, including ANOVA, Gaussian linear regression, and logistic regression. For details on other types of regression, including ordinal regression, log-linear models, and survival analysis, refer to @lumley2010complex. @lumley2010complex also discusses custom models such as a negative binomial or Poisson model in Appendix E of his book.

## Analysis of Variance (ANOVA)

In ANOVA, we are testing whether the mean of an outcome is the same across two or more groups. Statistically, we set up this as follows:

  - $H_0: \mu_1 = \mu_2= \dots = \mu_k$ where $\mu_i$ is the mean outcome for group $i$
  - $H_A: \text{At least one mean is different}$

Some assumptions when using ANOVA on survey data include:

  - The outcome variable is normally distributed within each group
  - The variances of the outcome variable between each group are approximately equal
  - We do NOT assume independence between the groups as with general ANOVA. The covariance is accounted for in the survey design

### Syntax

To perform this type of analysis in R, the general syntax is as follows:

``` r
des_obj %>%
  svyglm(
    formula = outcome ~ group,
    design = .,
    na.action = na.omit,
    df.resid = NULL
  )
```

The arguments are:

* `formula`: Formula in the form of `outcome~group`. The group variable must be a factor or character.
* `design`: a `tbl_svy` object created by `as_survey`
* `na.action`: handling of missing data
* `df.resid`: degrees of freedom for Wald tests (optional) - defaults to using `degf(design)-(g-1)` where $g$ is the number of groups

The function `svyglm()` does not have the design as the first argument so the dot (`.`) notation is used to pass it with a pipe (see Chapter \@ref(c06-statistical-testing) for more details). The default for missing data is `na.omit`, this means that we are removing all records with any missing data in either predictors or outcomes from analyses.  There are other options for handling missing data and we recommend looking at the help documentation for `na.omit` (run `help(na.omit)` or `?na.omit`) for more information on options to use for `na.action`.  For a discussion of how to handle missing data see Chapter \@ref(c04-understanding-survey-data-documentation). 

### Example

Looking at an example will help us discuss the output and how to interpret the results. In RECS, respondents are asked what temperature they set their thermostat to during the day and evening when using the air-conditioning during the summer. To analyze this data, we filter the respondents to only those using AC (`ACUsed`). Then if we want to see if there are differences by region, we can use `group_by()`. A descriptive analysis of the temperature at night (`SummerTempNight`) set by region and the sample sizes is displayed below.

```{r}
#| label: model-anova-prep
recs_des %>%
  filter(ACUsed) %>%
  group_by(Region) %>%
  summarize(
    SMN = survey_mean(SummerTempNight, na.rm = TRUE),
    n = unweighted(n()),
    n_na = unweighted(sum(is.na(SummerTempNight)))
  )
```

<!-- <JM> Isn't na.action = na.omit the default? -->
In the following code, we test whether this temperature varies by region by first using `svyglm()` to run the test and then using `broom::tidy()` to display the output. Note that the temperature setting is set to NA when the household does not use air-conditioning, and thus `na.action=na.omit` is specified to ignore these cases.

```{r}
#| label: model-anova-ex
anova_out <- recs_des %>%
  svyglm(design = .,
         formula = SummerTempNight ~ Region,
         na.action = na.omit)

tidy(anova_out)
```

In the output above, we can see the estimated coefficients (`estimate`), estimated standard errors of the coefficients (`std.error`), the t-statistic (`statistic`), and the p-value for each coefficient. In this output, the intercept represents the reference value of the Northeast region^[To change the reference level, reorder the factor before modeling using the function `relevel()` from {stats} or using one of many factor ordering functions in {forcats} such as `fct_relevel()` or `fct_infreq()`]. The other coefficients indicate the difference in temperature relative to the Northeast region. For example, in the Midwest, temperatures are set, on average, `r tidy(anova_out) %>% filter(term=="RegionMidwest") %>% pull(estimate) %>% signif(3)` degrees higher than in the Northeast during summer nights.

## Gaussian Linear Regression

Gaussian linear regression is a more generalized method than ANOVA where we fit a model of a continuous outcome with any number of categorical or continuous predictors, such that

$$y_i=\beta_0 +\sum_{i=1}^p \beta_i x_i + \epsilon_i$$

where $y_i$ is the outcome, $\beta_0$ is an intercept, $x_1, \cdots, x_n$ are the predictors with $\beta_1, \cdots, \beta_p$ as the associated coefficients, and $\epsilon_i$ is the error.

Assumptions in Gaussian linear regression using survey data include:

  - The residuals ($\epsilon_i$) are normally distributed, but there is not an assumption of independence, and the correlation structure is captured in the survey design object
  - There is a linear relationship between the outcome variable and the independent variables
  - The residuals are homoscedastic, that is, the error term is the same across all values of independent variables

### Syntax

The syntax for this regression uses the same function as ANOVA, but can have more than one variable listed on the right-hand side of the formula:

``` r
des_obj %>%
  svyglm(
    formula = outcomevar ~ x1 + x2 + x3,
    design = .,
    na.action = na.omit,
    df.resid = NULL
  )
```

The arguments are:

* `formula`: Formula in the form of `y~x`
* `design`: a `tbl_svy` object created by `as_survey`
* `na.action`: handling of missing data
* `df.resid`: degrees of freedom for Wald tests (optional) - defaults to using `degf(design)-p` where $p$ is the rank of the design matrix

As discussed at the beginning of the chapter, the formula on the right-hand side can be specified in many ways, whether interactions are desired or not, for example. 

### Examples

#### Example 1: Linear Regression with Single Variable {.unnumbered}
On RECS, we can obtain information on the square footage of homes and the electric bills. We assume that square footage is related to the amount of money spent on electricity and examine a model for this. Before any modeling, we first plot the data to determine whether it is reasonable to assume a linear relationship. In Figure \@ref(fig:model-plot-sf-elbill), each hexagon represents the weighted count of households in the bin and we can see a general positive linear trend (as the square footage increases so does the amount of money spent on electricity).

```{r}
#| label: model-plot-sf-elbill
#| fig.cap: Relationship between square footage and dollars spent on electricity, RECS 2015
#| fig.alt: Hex chart where each hexagon represents a number of housing units at a point. x-axis is 'Total square footage' ranging from 0 to 7,500 and y-axis is 'Amount spent on electricity' ranging from $0 to 8,000. The trend is relatively linear and positve. A high concentration of points have square footage between 0 and 2,500 square feet as well as between electricity expenditure between $0 and 2,000
#| echo: FALSE
#| warning: FALSE
recs_in %>%
  ggplot(aes(
    x = TOTSQFT_EN,
    y = DOLLAREL,
    weight = NWEIGHT / 1000000
  )) +
  geom_hex() +
  scale_fill_gradientn(
    guide = "colorbar",
    name = "Housing Units\n(Millions)",
    labels = scales::comma,
    colors = rev(c("#0B3954", "#087E8B", "#BFD7EA"))
  ) +
  xlab("Total square footage") + ylab("Amount spent on electricity") +
  scale_y_continuous(labels = scales::dollar_format()) +
  scale_x_continuous(labels = scales::comma_format()) +
  theme_minimal() 
```

Given that the plot shows a potential relationship, fitting a model will allow us to determine if the relationship is statistically significant.  The model is fit below with electricity expenditure as the outcome. 

```{r}
#| label: model-slr-examp
m_electric_sqft <- recs_des %>%
  svyglm(design = .,
         formula = DOLLAREL ~ TOTSQFT_EN,
         na.action = na.omit)
tidy(m_electric_sqft)
```

In the output above, we can see the estimated coefficients (`estimate`), estimated standard errors of the coefficients (`std.error`), the t-statistic (`statistic`), and the p-value for each coefficient. In these results, we can say that, on average, for every additional square foot of house size, the electricity bill increases by `r (tidy(m_electric_sqft) %>% filter(term=="TOTSQFT_EN") %>% pull(estimate) %>% signif(3))*100` cents and that square footage is significantly associated with electricity expenditure. 

This is a very simple model, and there are likely many more factors in electricity expenditure, including the type of cooling, number of appliances, location, and more. However, often starting with one variable models can help researchers understand what potential relationships there are between variables before fitting more complex models.  Often researchers start with known relationships before building models to determine what impact additional variables have on the model.

#### Example 2: Linear Regression with Additional Variables and Interactions {.unnumbered}
<!-- <JM> Why no intercept here? -->
In the following example, a model is fit to predict electricity expenditure, including Census region (factor/categorical), urbanicity (factor/categorical), square footage (double/numeric), and whether air-conditioning is used (logical/categorical) with all two-way interactions also included.  As a reminder, using `-1` means that we are fitting this model without an intercept.

```{r}
#| label: model-lmr-examp
m_electric_multi <- recs_des %>%
  svyglm(
    design = .,
    formula = DOLLAREL ~ (Region + Urbanicity + TOTSQFT_EN + ACUsed)^2 - 1, 
    na.action = na.omit
  )

tidy(m_electric_multi) %>% print(n = 50) 
```

As shown above, there are many terms in this model. To test whether coefficients for a term are different from zero, the function `regTermTest()` can be used. For example, in the above regression, we can test whether the interaction of region and urbanicity is significant as follows:

```{r}
#| label: model-lmr-test-term

urb_reg_test <- regTermTest(m_electric_multi, ~Urbanicity:Region)
urb_reg_test
```

This output indicates there is a significant interaction between urbanicity and region (p-value=$`r signif(urb_reg_test[["p"]], 3)`$).

To examine the predictions, residuals and more from the model, the function `augment()` from {broom} can be used. The `augment()` function will return a tibble with the independent and dependent variables and other fit statistics. The `augment()` function has not been specifically written for objects of class `svyglm`, and as such, a warning will be displayed indicating this at this time. As it was not written exactly for this class of objects, a little tweaking needs to be done after using augment to get the predicted (`.fitted`) and standard error (`.se.fit`) values. To obtain the standard error of the fitted values we need to use the `attr()` function on the `.fitted` values created by `augment()`. 

```{r}
#| label: model-aug-examp-se
#| warning: false
fitstats <-
  augment(m_electric_multi) %>%
  mutate(.se.fit = sqrt(attr(.fitted, "var")), 
         .fitted = as.numeric(.fitted)) 

fitstats
```

These results can then be used in a variety of ways, including examining residual plots as illustrated in the code below and Figure \@ref(fig:model-aug-examp-plot).
<!-- <JM> You show the plot but don't interpret it. -->
```{r}
#| label: model-aug-examp-plot
#| fig.cap: Residual plot of electric cost model with covariates Region, Urbanicity, TOTSQFT\_EN, and ACUsed
#| fig.alt: Residual scatter plot with a x-axis of 'Fitted value of electricity cost' ranging between approximately $0 and $4,000 and a y-axis with the 'Residual of model' ranging from approximatley -$3,000 to $5,000. The points create a slight megaphone shape with largest residuals in the middle of the x-range. A red line is drawn horizontally at y=0.
fitstats %>%
  ggplot(aes(x = .fitted, .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red") +
  theme_minimal() +
  xlab("Fitted value of electricity cost") +
  ylab("Residual of model") +
  scale_y_continuous(labels = scales::dollar_format()) +
  scale_x_continuous(labels = scales::dollar_format()) 

```

Additionally, `augment()` can be used to predict outcomes for data not used in modeling. Perhaps, we would like to predict the energy expenditure for a home in an urban area in the south that uses air-conditioning and is 2,500 square feet. To do this, we first make a tibble including that additional data and then use the `newdata` argument in the `augment()` function.  As before, to obtain the standard error of the predicted values we need to use the `attr()` function.

```{r}
#| label: model-predict-new-dat
add_data <-
  recs_in %>% select(DOEID,
                     Region,
                     Urbanicity,
                     TOTSQFT_EN,
                     ACUsed,
                     DOLLAREL) %>%
  rbind(
    tibble(
      DOEID = NA,
      Region = "South",
      Urbanicity = "Urban Area",
      TOTSQFT_EN = 2500,
      ACUsed = TRUE,
      DOLLAREL = NA
    )
  ) %>% 
  tail(1)

pred_data <- augment(m_electric_multi, newdata = add_data) %>%
  mutate(.se.fit = sqrt(attr(.fitted, "var")), 
         .fitted = as.numeric(.fitted)) 

pred_data 
```

<!-- <JM> I only see 1775. in the output, not 1775.48-->
In the above example, it is predicted that the energy expenditure would be \$`r pred_data %>% slice_tail(n=1) %>% pull(.fitted) %>% round(2)`.


## Logistic Regression

Logistic regression is used to model a binary outcome and is a specific case of the generalized linear model (GLM). A GLM uses a link function to link the response variable to the linear model. In logistic regression, the link model is the logit function. Specifically, the model is specified as follows:

$$ y_i \sim \text{Bernoulli}(\pi_i)$$

```{=tex}
\begin{equation}
\log \left(\frac{\pi_i}{1-\pi_i} \right)=\beta_0 +\sum_{i=1}^p \beta_i x_i
(\#eq:logoddlin)
\end{equation}
```
which can be re-expressed as

<!-- <JM> What's p?-->
$$ \pi_i=\frac{\exp \left(\beta_0 +\sum_{i=1}^p \beta_i x_i \right)}{1+\exp \left(\beta_0 +\sum_{i=1}^p \beta_i x_i \right)}.$$ where $y_i$ is the outcome, $\beta_0$ is an intercept, and $x_1, \cdots, x_n$ are the predictors with $\beta_1, \cdots, \beta_n$ as the associated coefficients.

Assumptions in logistic regression using survey data include:

  - The outcome variable has two levels
  - There is a linear relationship between the independent variables and the log odds (Equation \@ref(eq:logoddlin))
  - The residuals are homoscedastic, that is, the error term is the same across all values of independent variables

### Syntax

The syntax for logistic regression is as follows:

``` r
des_obj %>%
  svyglm(
    formula = outcomevar ~ x1 + x2 + x3,
    design = .,
    na.action = na.omit,
    df.resid = NULL,
    family = quasibinomial
  )
```

The arguments are:

* `formula`: Formula in the form of `y~x`
* `design`: a `tbl_svy` object created by `as_survey`
* `na.action`: handling of missing data
* `df.resid`: degrees of freedom for Wald tests (optional) - defaults to using `degf(design)-p` where $p$ is the rank of the design matrix
* `family`: the error distribution/link function to be used in the model

<!-- <JM> Suggest adding a refrence for quasibinomial for further learning -->
Note `svyglm()` is the same function used in both ANOVA and linear regression. However, we've added the link function quasibinomial. While we can use the binomial link function, it is recommended to use the quasibinomial as our weights may not be integers, and the quasibinomial also allows for overdispersion. The quasibinomial family has a default logit link which is what is specified in the equations above. When specifying the outcome variable, it will likely be specified in one of three ways with survey data:

  - A two level factor variable where the first level of the factor indicates a "failure" and the second level indicates a "success"
  - A numeric variable which is 1 or 0 where 1 indicates a success
  - A logical variable where TRUE indicates a success

### Examples

#### Example 1: Logistic Regression with Single Variable {.unnumbered}
In the following example, the ANES data are used, and we are modeling whether someone usually has trust in the government^[Question: How often can you trust the federal government in Washington to do what is right?] by who someone voted for president in 2020. As a reminder, the leading candidates were Biden and Trump though people could vote for someone else not in the Democratic or Republican parties. Those votes are all grouped into an "Other" category. We first create a binary outcome for trusting in the government and plot the data. A scatter plot of the raw data is not useful as it is all 0 and 1 outcomes, so instead, we plot a summary of the data.

<!-- <JM> Should xlab be 2020 instead of 2022? Also, are the bars supposed to add to 100? -->
```{r}
#| label: model-logisticexamp-plot
#| fig.cap: Relationship between candidate selection and trust in government, ANES 2020
#| fig.alt: Bar chart with x-axis of election choice with levels of Biden, Trump, and Other and y-axis of 'Usually trust the government' ranging from 0% to 20%. Bar 1 is centered at 1, and length is from 0 to 0.12 with fill color dark blue which maps to VotedPres2020_selection = Biden. Bar 2 is centered at 2, and length is from 0 to 0.17 with fill color very pale blue which maps to VotedPres2020_selection = Trump. Bar 3 is centered at 3, and length is from 0 to 0.06 with fill color moderate purple which maps to VotedPres2020_selection = Other. Error bars are drawn as well with the width of the Biden and Trump error bars being similar and the error bar for Other being significantly wider.
#| warning: false
anes_des_der <- anes_des %>%
  mutate(TrustGovernmentUsually = case_when(
    is.na(TrustGovernment) ~ NA,
    TRUE ~ TrustGovernment %in% c("Always", "Most of the time")
  ))

anes_des_der %>%
  group_by(VotedPres2020_selection) %>%
  summarize(pct_trust = survey_mean(TrustGovernmentUsually,
                                    na.rm = TRUE,
                                    proportion = TRUE,
                                    vartype = "ci"),
    .groups = "drop") %>%
  filter(complete.cases(.)) %>%
  ggplot(aes(x = VotedPres2020_selection, y = pct_trust, 
             fill = VotedPres2020_selection)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = pct_trust_low, ymax = pct_trust_upp), 
                width = .2) +
  scale_fill_manual(values = c("#0b3954", "#bfd7ea", "#8d6b94")) +
  xlab("Election choice (2022)") +
  ylab("Usually trust the government") +
  scale_y_continuous(labels = scales::percent) +
  guides(fill = "none") +
  theme_minimal()
```

By looking at Figure \@ref(fig:model-logisticexamp-plot) it appears that people who voted for Trump are more likely to say that they usually have trust in the government compared to those who voted for Biden and Other candidates. To determine if this insight is accurate, we next we fit the model.

```{r}
#| label: model-logisticexamp-model
logistic_trust_vote <- anes_des_der %>%
  svyglm(design = .,
         formula = TrustGovernmentUsually ~ VotedPres2020_selection,
         family = quasibinomial) 

tidy(logistic_trust_vote)
```
In the output above, we can see the estimated coefficients (`estimate`), estimated standard errors of the coefficients (`std.error`), the t-statistic (`statistic`), and the p-value for each coefficient.  This output indicates that respondents who voted for Trump are `r signif(logistic_trust_vote$coefficients[2],3)` times more likely to usually have trust in the government compared to those who voted for Biden (the reference level). 

Sometimes it is easier to talk about the odds instead of the likelihood.  In this case, we can also see the exponentiated coefficients which illustrates the odds:
```{r}
#| label: model-logisticexamp-model-odds
tidy(logistic_trust_vote, exponentiate = TRUE) %>% 
  select(term, estimate)
```

```{r}
#| label: model-logisticcalc
#| echo: false
or_trump <-
  tidy(logistic_trust_vote, exponentiate = TRUE) %>% 
  filter(str_detect(term, "Trump")) %>% 
  pull(estimate)
or_other <-
  tidy(logistic_trust_vote, exponentiate = TRUE) %>% 
  filter(str_detect(term, "Other")) %>% 
  pull(estimate)
```

We can interpret this as saying that the odds of usually trusting the government for someone who voted for Trump is `r signif(or_trump*100, 3)`% as likely to trust the government compared to a person who voted for Biden (the reference level). In comparison, a person who voted for neither Biden nor Trump is `r signif(or_other*100, 3)`% as likely to trust the government as someone who voted for Biden.

As with linear regression, the `augment()` can be used to predict values. By default, the prediction is the link function and not the probability. To predict the probability, add an argument of `type.predict="response"` as demonstrated below:

```{r}
#| label: model-logistic-aug
#| warning: false
logistic_trust_vote %>%
  augment(type.predict = "response") %>%
  mutate(.se.fit = sqrt(attr(.fitted, "var")), 
         .fitted = as.numeric(.fitted)) %>%
  select(TrustGovernmentUsually,
         VotedPres2020_selection,
         .fitted,
         .se.fit) 
```


#### Example 2: Interaction Effects {.unnumbered}
Let's look at another example with interaction effects.  If we're interested in understanding the demographics of people who voted for Biden, we could include `Gender` and `Education` in our model.  

First we need to create an indicator for voted for Biden. Note that this indicator places anyone who did not vote at all into `VoteBiden = 0`.
<!-- <JM> Sure you want to include non-voters in here? Seems to be conflating preference for Biden and voting.-->
```{r}
#| label: model-logisticexamp-biden-ind
anes_des_ind <- anes_des %>% 
  mutate(VoteBiden = case_when(VotedPres2020_selection == "Biden"~1,
                               TRUE ~ 0))
```

Let's first look at the main effects of gender and education.
```{r}
#| label: model-logisticexamp-biden-main
log_biden_main <- anes_des_ind %>%
  svyglm(design = .,
         formula = VoteBiden ~ Gender + Education,
         family = quasibinomial) 

tidy(log_biden_main)
```

This main effect model indicates that respondents with a graduate degree are `r signif(log_biden_main$coefficients[6],3)` times more likely to vote for Biden compared to respondents with less than a high school degree. However, we see that gender is not significant. 

It is possible that there is an interaction between gender and education. To determine this we can create a model that includes the interaction effects:
```{r}
#| label: model-logisticexamp-biden-int
log_biden_int <- anes_des_ind %>%
  svyglm(design = .,
         formula = VoteBiden ~ (Gender + Education)^2,
         family = quasibinomial) 

tidy(log_biden_int)
```

<!-- <JM> Tell reader which interaction effect is significant from the output. -->
The results from the interaction model show a single interaction effect that is significant.  To better understand what this interaction means, we will want to plot the predicted probabilities.  Let's first obtain the predicted probabilities for each possible combination of variables using the `augment()` function.

```{r}
#| label: model-logisticexamp-biden-aug
#| warning: false
log_biden_pred <- log_biden_int %>%
  augment(type.predict = "response") %>%
  mutate(.se.fit = sqrt(attr(.fitted, "var")), 
         .fitted = as.numeric(.fitted)) %>%
  select(VoteBiden, Gender, Education, .fitted, .se.fit) 
```

We can then use this information to plot the predicted probabilities to better understand the interaction effects. To create an interaction plot, the y-axis will be the predicted probabilities, and one of our x-variables will be on the x-axis and the other will be represented by multiple lines. Figure \@ref(fig:model-logisticexamp-biden-plot) shows the interaction plot with the gender variable on the x-axis and education represented by the lines.

```{r}
#| label: model-logisticexamp-biden-plot
#| fig.cap: Interaction Plot of Gender and Education Predicting the Probability of Voting for Biden
#| fig.alt: "Line plot with x-axis as Male and Female (left to right) and y-axis as 'Predicted Probability of Voting for Biden'. There are five lines for education levels with lines being from top to bottom: Graduate, Bachelor's, Post HS, High school, and Less than HS. Most lines are roughly parallel with similar predicted probabilities between males and females of the same education except for those with graduate and less than HS educations. For those with a graduate degree, females have higher predicted probability of voting for Biden than males. For those with less than a HS degree, females have lower predicted probability of voting for Biden than males with HS education."

biden_int_plot <- log_biden_pred %>% 
  filter(VoteBiden==1) %>% 
  distinct() %>% 
  arrange(Gender, Education) %>% 
  mutate(Education = fct_reorder2(Education, Gender, .fitted)) %>%
  ggplot(aes(x = Gender, y = .fitted, group = Education,
             color = Education, linetype = Education)) +
  geom_line(linewidth = 1.1) +
  scale_color_manual(values = book_colors) +
  ylab("Predicted Probability of Voting for Biden") +
  guides(fill = "none") +
  theme_minimal()
  
biden_int_plot
```

From this plot we can see that respondents who indicated a male gender and had less than a high school education were more likely to vote for Biden than females among those with less than a high school education.  Additionally, females with a graduate degree were more likely to vote for Biden than males with a graduate degree.

Interactions in models can be difficult to understand from the coefficients alone. Using these interaction plots can help others understand the nuances of the results.


## Exercises

1.  The type of housing unit may have an impact on energy expenses. Is there any relationship between housing unit type (`HousingUnitType`) and total energy expenditure (`TOTALDOL`)? First, find the average energy expenditure by housing unit type as a descriptive analysis and then do the test. The reference level in the comparison should be the housing unit type that is most common.

```{r}
#| label: model-lin-sol-1
recs_des %>%
  group_by(HousingUnitType) %>%
  summarize(Expense = survey_mean(TOTALDOL, na.rm = TRUE),
            HUs = survey_total()) %>%
  arrange(desc(HUs))

exp_unit_out <- recs_des %>%
  mutate(HousingUnitType = fct_infreq(HousingUnitType, NWEIGHT)) %>%
  svyglm(
    design = .,
    formula = TOTALDOL ~ HousingUnitType,
    na.action = na.omit
  )

tidy(exp_unit_out)

# Single-family detached units are most common
# There is a significant relationship between energy expenditure and housing unit type
```
<!-- <JM> Do you plan to add an answer to the initial question, "Does temperature...?" beyond just the output? -->
2.  Does temperature play a role in energy expenditure? Cooling degree days is a measure of how hot a place is. CDD65 for a given day indicates the number of degrees Fahrenheit warmer than 65°F (18.3°C) it is in a location. On a day that averages 65°F and below, CDD65=0. While a day that averages 85°F would have CDD80=20 because it is 20 degrees warmer. For each day in the year, this is summed to give an indicator of how hot the place is throughout the year. Similarly, HDD65 indicates the days colder than 65°F (18.3°C)^[<https://www.eia.gov/energyexplained/units-and-calculators/degree-days.php>]. Can energy expenditure be predicted using these temperature indicators along with square footage? Is there a significant relationship? Include main effects and two-way interactions.

```{r}
#| label: model-lin-sol-2
temps_sqft_exp <- recs_des %>%
  svyglm(
    design = .,
    formula = DOLLAREL ~ (TOTSQFT_EN + CDD65 + HDD65) ^ 2,
    na.action = na.omit
  )

tidy(temps_sqft_exp)
```

3.  Continuing with our results from question 2, create a plot between the actual and predicted expenditures and a residual plot for the predicted expenditures.

```{r}
#| label: model-lin-sol-3
temps_sqft_exp_fit <- temps_sqft_exp %>%
  augment() %>%
  mutate(.se.fit = sqrt(attr(.fitted, "var")), 
         # extract the variance of the fitted value
         .fitted = as.numeric(.fitted)) 
```

```{r}
#| label: model-lin-sol-3-p1
#| fig.cap: "Actual and predicted electricity expenditures"
temps_sqft_exp_fit %>%
  ggplot(aes(x = DOLLAREL, y = .fitted)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red") +
  xlab("Actual expenditures") +
  ylab("Predicted expenditures") +
  theme_minimal()
```

```{r}
#| label: model-lin-sol-3-p2
#| fig.cap: "Residual plot of electric cost model with covariates TOTSQFT_EN, CDD65, and HDD65"
temps_sqft_exp_fit %>%
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red") +
  xlab("Predicted expenditure") +
  ylab("Residual value of expenditure") +
  theme_minimal()
```

4.  Early voting expanded in 2020^[<https://www.npr.org/2020/10/26/927803214/62-million-and-counting-americans-are-breaking-early-voting-records>]. Build a logistic model predicting early voting in 2020 (`EarlyVote2020`) using age (`Age`), education (`Education`), and party identification (`PartyID`). Include two-way interactions.

```{r}
#| label: model-ex-logistic-1
earlyvote_mod <- anes_des %>%
  filter(!is.na(EarlyVote2020)) %>%
  svyglm(
    design = .,
    formula = EarlyVote2020 ~ (Age + Education + PartyID) ^ 2 ,
    family = quasibinomial
  )

tidy(earlyvote_mod) %>% arrange(p.value)
```

5.  Continuing from Exercise 1, predict the probability of early voting for two people. Both are 28 years old and have a graduate degree, but one person is a strong Democrat, and the other is a strong Republican.

```{r}
#| label: model-ex-logistic-2
add_vote_dat <- anes_in %>%
  select(EarlyVote2020, Age, Education, PartyID) %>%
  rbind(tibble(
    EarlyVote2020 = NA,
    Age = 28,
    Education = "Graduate",
    PartyID = c("Strong democrat", "Strong republican")
  )) %>%
  tail(2)

log_ex_2_out <- earlyvote_mod %>%
  augment(newdata = add_vote_dat, type.predict = "response") %>%
  mutate(.se.fit = sqrt(attr(.fitted, "var")), 
         # extract the variance of the fitted value
         .fitted = as.numeric(.fitted))
```
